[
  {
    "objectID": "docs/tasks.html",
    "href": "docs/tasks.html",
    "title": "Task Scripts",
    "section": "",
    "text": "spotforecast2 provides command-line task scripts for common forecasting workflows. These scripts are registered as console entry points and can be invoked directly via uv run or after package installation.\n\n\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\nspotforecast2-entsoe\nENTSO-E energy forecasting pipeline (download, train, predict)\n\n\nspotforecast-demo\nDemonstration task comparing baseline, covariate, and custom models\n\n\nspotforecast-n2o1\nN-to-1 forecasting with weighted aggregation\n\n\nspotforecast-n2o1-df\nN-to-1 forecasting using a DataFrame input\n\n\nspotforecast-n2o1-cov\nN-to-1 forecasting with exogenous covariates\n\n\nspotforecast-n2o1-cov-df\nN-to-1 forecasting with covariates and DataFrame input\n\n\n\n\n\n\n\nThe spotforecast2-entsoe command provides a unified CLI for the ENTSO-E energy forecasting pipeline.\n\n\n# Download data from ENTSO-E\nuv run spotforecast2-entsoe download --api-key YOUR_API_KEY 202301010000\n\n# Train a model (lgbm or xgb)\nuv run spotforecast2-entsoe train lgbm --force\n\n# Generate predictions and plot (defaults to lgbm)\nuv run spotforecast2-entsoe predict --plot\n\n# Generate predictions with explicit model selection\nuv run spotforecast2-entsoe predict lgbm --plot\nuv run spotforecast2-entsoe predict xgb --plot\n\n# Merge raw data files\nuv run spotforecast2-entsoe merge\n\n\n\nThe positional argument 202301010000 is a UTC timestamp in the format YYYYMMDDHHMM. It represents the start of the download window. You can provide either one timestamp (start only) or two timestamps (start and end).\n# Start only (end defaults to now, UTC)\nuv run spotforecast2-entsoe download 202301010000\n\n# Start and end (UTC)\nuv run spotforecast2-entsoe download 202301010000 202312312300\nHidden arguments and defaults for download:\n\n–api-key or ENTSOE_API_KEY environment variable\n–force to re-download even if files already exist\ndata home controlled by SPOTFORECAST2_DATA (default is ~/spotforecast2_data)\n\n\n\n\nThe ENTSO-E task uses a configuration class that can be customized programmatically. All configuration parameters have sensible defaults but can be overridden when needed.\n\n\nfrom spotforecast2 import Config\n\n# Create default configuration instance\nconfig = Config()\n\n# Access configuration values\nprint(config.API_COUNTRY_CODE)  # 'DE'\nprint(config.predict_size)      # 24\nprint(config.train_size)        # Timedelta(days=1095)\n\n\n\nfrom spotforecast2 import Config\nimport pandas as pd\n\n# Create custom configuration\ncustom_config = Config(\n    api_country_code='DE',\n    predict_size=48,\n    refit_size=14,\n    train_size=pd.Timedelta(days=365),\n    random_state=42\n)\n\n# Use in your code\nprint(custom_config.API_COUNTRY_CODE)  # 'DE'\nprint(custom_config.predict_size)      # 48\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nType\nDefault\nDescription\n\n\n\n\napi_country_code\nstr\n“DE”\nISO country code for ENTSO-E API\n\n\npredict_size\nint\n24\nNumber of hours to predict ahead\n\n\nrefit_size\nint\n7\nNumber of days between model refits\n\n\ntrain_size\nTimedelta\n3 years\nTraining data window\n\n\nend_train_default\nstr\n“2025-12-31 00:00+00:00”\nDefault training end date\n\n\ndelta_val\nTimedelta\n10 weeks\nValidation window size\n\n\nrandom_state\nint\n314159\nRandom seed for reproducibility\n\n\nn_hyperparameters_trials\nint\n20\nHyperparameter tuning trials\n\n\nlags_consider\nList[int]\n[1..23]\nLag values for features\n\n\nperiods\nList[Period]\n5 periods\nCyclical feature encodings\n\n\n\nFor more details, see the ConfigEntsoe API documentation.\n\n\n\n\nDownload interval is defined by the start/end timestamps passed to the download command.\nTraining, prediction, validation, and testing intervals are configured via the Config class. The CLI uses default configuration values which can be modified programmatically:\n\ntraining end time: config.end_train_default (defaults to “2025-12-31 00:00+00:00”)\ntraining window size: config.train_size (defaults to 3 years)\nprediction window: config.predict_size * config.refit_size hours\n\nValidation and testing are derived from the prediction window:\n\nvalidation metrics use the first 24 hours of the prediction window\ntesting metrics use the full prediction window\n\n!!! note “Customizing Configuration” To use custom configuration values, you’ll need to modify the task script to create a Config instance with your desired parameters. See the Configuration section above for examples.\n!!! tip “API Key Management” Store your ENTSO-E API key in the ENTSOE_API_KEY environment variable to avoid passing it on every command: bash     export ENTSOE_API_KEY=\"your-api-key-here\"     echo $ENTSOE_API_KEY     uv run spotforecast2-entsoe download 202301010000\n\n\n\nThe prediction plot shows the following graphs:\n\nTotal system load (actual): The real-time electricity demand (consumption) within the bidding zone. This includes network losses but excludes consumption for pumped storage and generating auxiliaries.\nTotal system load (model prediction): The demand forecast generated by the spotforecast2 machine learning model (e.g., LightGBM or XGBoost) based on historical data and exogenous features.\nBenchmark Forecast (e.g. ENTSOE): The reference forecast provided by the Transmission System Operators (TSOs) via the ENTSO-E Transparency Platform.\nActual (last week): The actual system load from exactly one week ago at the same time, which serves as a seasonal baseline comparison.\n\nThe prediction plot is saved as an HTML file named index.html in the data home directory. By default this is ~/spotforecast2_data/index.html or the path defined by SPOTFORECAST2_DATA.\n# Default location on macOS/Linux\nopen ~/spotforecast2_data/index.html\n\n# If you use a custom data home\nopen \"$SPOTFORECAST2_DATA/index.html\"\nCheck the CLI logs for the exact path (look for “Plot saved to …”).\n\n\n\n\n\nThe spotforecast-demo command runs a comparison of three forecasting approaches:\n\nBaseline: Standard N-to-1 recursive forecaster\nCovariate-enhanced: Includes weather, holidays, and cyclical features\nCustom LightGBM: Optimized hyperparameters\n\n# Run with default settings\nuv run spotforecast-demo\n\n# Force retraining and save plot\nuv run spotforecast-demo --force_train true --html task_demo_plot.html\n\n\n\n\nThese tasks implement multi-output time series forecasting with weighted aggregation.\n\n\nuv run spotforecast-n2o1\n\n\n\nuv run spotforecast-n2o1-df\n\n\n\nIncludes weather data, holiday indicators, and cyclical time features.\nuv run spotforecast-n2o1-cov\n\n\n\nuv run spotforecast-n2o1-cov-df\n\n\n\n\n\nAll tasks use sensible defaults but can be customized via:\n\nEnvironment variables (e.g., ENTSOE_API_KEY)\nCommand-line arguments (use --help for details)\nConfiguration files stored in ~/spotforecast2_models/\n\n# View available options for any command\nuv run spotforecast-demo --help\nuv run spotforecast2-entsoe predict --help\n\n\n\n\nTrained models are saved to ~/spotforecast2_models/&lt;task_name&gt;/ by default. This allows:\n\nIncremental retraining: Only retrain when models are stale\nReproducibility: Models are versioned by task and timestamp\nAuditability: Full training logs are stored alongside models\n\n!!! warning “Safety-Critical Consideration” In production environments, always verify model checksums and training timestamps before deployment.\n\n\n\n\nThe task scripts are covered by comprehensive safety-critical tests to ensure reliability in production environments.\n\n\nRun all ENTSO-E task tests:\nuv run pytest tests/test_tasks_entsoe.py -v\nRun specific test categories:\n# Run only safety-critical tests\nuv run pytest tests/test_tasks_entsoe.py::TestSafetyCriticalEntsoe -v\n\n# Run parameter validation tests\nuv run pytest tests/test_tasks_entsoe.py::TestSafetyCriticalEntsoe::test_train_lgbm_model_parameter_correctness -v\n\n# Run with coverage\nuv run pytest tests/test_tasks_entsoe.py --cov=spotforecast2.tasks.task_entsoe --cov-report=html\n\n\n\nThe test suite includes:\n\nParameter Validation: Ensures correct parameter passing between CLI and internal functions\nError Handling: Validates graceful degradation and meaningful error messages\nData Validation: Tests boundary conditions and edge cases\nIntegration Tests: Verifies end-to-end functionality\nRegression Tests: Protects against known historical bugs\nModel Selection Safety: Prevents model mismatch in production pipelines\n\n!!! tip “Continuous Testing” Run tests before deployment in production environments: bash     uv run pytest tests/ -v --tb=short",
    "crumbs": [
      "Tasks Guide",
      "Overview"
    ]
  },
  {
    "objectID": "docs/tasks.html#available-commands",
    "href": "docs/tasks.html#available-commands",
    "title": "Task Scripts",
    "section": "",
    "text": "Command\nDescription\n\n\n\n\nspotforecast2-entsoe\nENTSO-E energy forecasting pipeline (download, train, predict)\n\n\nspotforecast-demo\nDemonstration task comparing baseline, covariate, and custom models\n\n\nspotforecast-n2o1\nN-to-1 forecasting with weighted aggregation\n\n\nspotforecast-n2o1-df\nN-to-1 forecasting using a DataFrame input\n\n\nspotforecast-n2o1-cov\nN-to-1 forecasting with exogenous covariates\n\n\nspotforecast-n2o1-cov-df\nN-to-1 forecasting with covariates and DataFrame input",
    "crumbs": [
      "Tasks Guide",
      "Overview"
    ]
  },
  {
    "objectID": "docs/tasks.html#entso-e-task",
    "href": "docs/tasks.html#entso-e-task",
    "title": "Task Scripts",
    "section": "",
    "text": "The spotforecast2-entsoe command provides a unified CLI for the ENTSO-E energy forecasting pipeline.\n\n\n# Download data from ENTSO-E\nuv run spotforecast2-entsoe download --api-key YOUR_API_KEY 202301010000\n\n# Train a model (lgbm or xgb)\nuv run spotforecast2-entsoe train lgbm --force\n\n# Generate predictions and plot (defaults to lgbm)\nuv run spotforecast2-entsoe predict --plot\n\n# Generate predictions with explicit model selection\nuv run spotforecast2-entsoe predict lgbm --plot\nuv run spotforecast2-entsoe predict xgb --plot\n\n# Merge raw data files\nuv run spotforecast2-entsoe merge\n\n\n\nThe positional argument 202301010000 is a UTC timestamp in the format YYYYMMDDHHMM. It represents the start of the download window. You can provide either one timestamp (start only) or two timestamps (start and end).\n# Start only (end defaults to now, UTC)\nuv run spotforecast2-entsoe download 202301010000\n\n# Start and end (UTC)\nuv run spotforecast2-entsoe download 202301010000 202312312300\nHidden arguments and defaults for download:\n\n–api-key or ENTSOE_API_KEY environment variable\n–force to re-download even if files already exist\ndata home controlled by SPOTFORECAST2_DATA (default is ~/spotforecast2_data)\n\n\n\n\nThe ENTSO-E task uses a configuration class that can be customized programmatically. All configuration parameters have sensible defaults but can be overridden when needed.\n\n\nfrom spotforecast2 import Config\n\n# Create default configuration instance\nconfig = Config()\n\n# Access configuration values\nprint(config.API_COUNTRY_CODE)  # 'DE'\nprint(config.predict_size)      # 24\nprint(config.train_size)        # Timedelta(days=1095)\n\n\n\nfrom spotforecast2 import Config\nimport pandas as pd\n\n# Create custom configuration\ncustom_config = Config(\n    api_country_code='DE',\n    predict_size=48,\n    refit_size=14,\n    train_size=pd.Timedelta(days=365),\n    random_state=42\n)\n\n# Use in your code\nprint(custom_config.API_COUNTRY_CODE)  # 'DE'\nprint(custom_config.predict_size)      # 48\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nType\nDefault\nDescription\n\n\n\n\napi_country_code\nstr\n“DE”\nISO country code for ENTSO-E API\n\n\npredict_size\nint\n24\nNumber of hours to predict ahead\n\n\nrefit_size\nint\n7\nNumber of days between model refits\n\n\ntrain_size\nTimedelta\n3 years\nTraining data window\n\n\nend_train_default\nstr\n“2025-12-31 00:00+00:00”\nDefault training end date\n\n\ndelta_val\nTimedelta\n10 weeks\nValidation window size\n\n\nrandom_state\nint\n314159\nRandom seed for reproducibility\n\n\nn_hyperparameters_trials\nint\n20\nHyperparameter tuning trials\n\n\nlags_consider\nList[int]\n[1..23]\nLag values for features\n\n\nperiods\nList[Period]\n5 periods\nCyclical feature encodings\n\n\n\nFor more details, see the ConfigEntsoe API documentation.\n\n\n\n\nDownload interval is defined by the start/end timestamps passed to the download command.\nTraining, prediction, validation, and testing intervals are configured via the Config class. The CLI uses default configuration values which can be modified programmatically:\n\ntraining end time: config.end_train_default (defaults to “2025-12-31 00:00+00:00”)\ntraining window size: config.train_size (defaults to 3 years)\nprediction window: config.predict_size * config.refit_size hours\n\nValidation and testing are derived from the prediction window:\n\nvalidation metrics use the first 24 hours of the prediction window\ntesting metrics use the full prediction window\n\n!!! note “Customizing Configuration” To use custom configuration values, you’ll need to modify the task script to create a Config instance with your desired parameters. See the Configuration section above for examples.\n!!! tip “API Key Management” Store your ENTSO-E API key in the ENTSOE_API_KEY environment variable to avoid passing it on every command: bash     export ENTSOE_API_KEY=\"your-api-key-here\"     echo $ENTSOE_API_KEY     uv run spotforecast2-entsoe download 202301010000\n\n\n\nThe prediction plot shows the following graphs:\n\nTotal system load (actual): The real-time electricity demand (consumption) within the bidding zone. This includes network losses but excludes consumption for pumped storage and generating auxiliaries.\nTotal system load (model prediction): The demand forecast generated by the spotforecast2 machine learning model (e.g., LightGBM or XGBoost) based on historical data and exogenous features.\nBenchmark Forecast (e.g. ENTSOE): The reference forecast provided by the Transmission System Operators (TSOs) via the ENTSO-E Transparency Platform.\nActual (last week): The actual system load from exactly one week ago at the same time, which serves as a seasonal baseline comparison.\n\nThe prediction plot is saved as an HTML file named index.html in the data home directory. By default this is ~/spotforecast2_data/index.html or the path defined by SPOTFORECAST2_DATA.\n# Default location on macOS/Linux\nopen ~/spotforecast2_data/index.html\n\n# If you use a custom data home\nopen \"$SPOTFORECAST2_DATA/index.html\"\nCheck the CLI logs for the exact path (look for “Plot saved to …”).",
    "crumbs": [
      "Tasks Guide",
      "Overview"
    ]
  },
  {
    "objectID": "docs/tasks.html#demo-task",
    "href": "docs/tasks.html#demo-task",
    "title": "Task Scripts",
    "section": "",
    "text": "The spotforecast-demo command runs a comparison of three forecasting approaches:\n\nBaseline: Standard N-to-1 recursive forecaster\nCovariate-enhanced: Includes weather, holidays, and cyclical features\nCustom LightGBM: Optimized hyperparameters\n\n# Run with default settings\nuv run spotforecast-demo\n\n# Force retraining and save plot\nuv run spotforecast-demo --force_train true --html task_demo_plot.html",
    "crumbs": [
      "Tasks Guide",
      "Overview"
    ]
  },
  {
    "objectID": "docs/tasks.html#n-to-1-forecasting-tasks",
    "href": "docs/tasks.html#n-to-1-forecasting-tasks",
    "title": "Task Scripts",
    "section": "",
    "text": "These tasks implement multi-output time series forecasting with weighted aggregation.\n\n\nuv run spotforecast-n2o1\n\n\n\nuv run spotforecast-n2o1-df\n\n\n\nIncludes weather data, holiday indicators, and cyclical time features.\nuv run spotforecast-n2o1-cov\n\n\n\nuv run spotforecast-n2o1-cov-df",
    "crumbs": [
      "Tasks Guide",
      "Overview"
    ]
  },
  {
    "objectID": "docs/tasks.html#configuration-1",
    "href": "docs/tasks.html#configuration-1",
    "title": "Task Scripts",
    "section": "",
    "text": "All tasks use sensible defaults but can be customized via:\n\nEnvironment variables (e.g., ENTSOE_API_KEY)\nCommand-line arguments (use --help for details)\nConfiguration files stored in ~/spotforecast2_models/\n\n# View available options for any command\nuv run spotforecast-demo --help\nuv run spotforecast2-entsoe predict --help",
    "crumbs": [
      "Tasks Guide",
      "Overview"
    ]
  },
  {
    "objectID": "docs/tasks.html#model-persistence",
    "href": "docs/tasks.html#model-persistence",
    "title": "Task Scripts",
    "section": "",
    "text": "Trained models are saved to ~/spotforecast2_models/&lt;task_name&gt;/ by default. This allows:\n\nIncremental retraining: Only retrain when models are stale\nReproducibility: Models are versioned by task and timestamp\nAuditability: Full training logs are stored alongside models\n\n!!! warning “Safety-Critical Consideration” In production environments, always verify model checksums and training timestamps before deployment.",
    "crumbs": [
      "Tasks Guide",
      "Overview"
    ]
  },
  {
    "objectID": "docs/tasks.html#testing",
    "href": "docs/tasks.html#testing",
    "title": "Task Scripts",
    "section": "",
    "text": "The task scripts are covered by comprehensive safety-critical tests to ensure reliability in production environments.\n\n\nRun all ENTSO-E task tests:\nuv run pytest tests/test_tasks_entsoe.py -v\nRun specific test categories:\n# Run only safety-critical tests\nuv run pytest tests/test_tasks_entsoe.py::TestSafetyCriticalEntsoe -v\n\n# Run parameter validation tests\nuv run pytest tests/test_tasks_entsoe.py::TestSafetyCriticalEntsoe::test_train_lgbm_model_parameter_correctness -v\n\n# Run with coverage\nuv run pytest tests/test_tasks_entsoe.py --cov=spotforecast2.tasks.task_entsoe --cov-report=html\n\n\n\nThe test suite includes:\n\nParameter Validation: Ensures correct parameter passing between CLI and internal functions\nError Handling: Validates graceful degradation and meaningful error messages\nData Validation: Tests boundary conditions and edge cases\nIntegration Tests: Verifies end-to-end functionality\nRegression Tests: Protects against known historical bugs\nModel Selection Safety: Prevents model mismatch in production pipelines\n\n!!! tip “Continuous Testing” Run tests before deployment in production environments: bash     uv run pytest tests/ -v --tb=short",
    "crumbs": [
      "Tasks Guide",
      "Overview"
    ]
  },
  {
    "objectID": "docs/preprocessing/time_series_visualization.html",
    "href": "docs/preprocessing/time_series_visualization.html",
    "title": "Time Series Visualization",
    "section": "",
    "text": "This module provides interactive time series visualization using Plotly, with support for multiple datasets and flexible customization options.\n\n\nThe time series visualization module includes two main functions:\n\nvisualize_ts_plotly() - Visualize multiple time series datasets with Plotly\nvisualize_ts_comparison() - Compare datasets with optional statistical overlays\n\nThese functions provide a flexible, interactive way to explore time series data with support for train/validation/test splits or any custom dataset groupings.\n\n\n\nThe time series visualization functions require plotly:\nUsing pip:\npip install plotly\nUsing uv:\nuv pip install plotly\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom spotforecast2.preprocessing.time_series_visualization import visualize_ts_plotly\n\n# Create sample datasets\nnp.random.seed(42)\ndates_train = pd.date_range('2024-01-01', periods=100, freq='h')\ndates_val = pd.date_range('2024-05-11', periods=50, freq='h')\ndates_test = pd.date_range('2024-07-01', periods=30, freq='h')\n\ndata_train = pd.DataFrame({\n    'temperature': np.random.normal(20, 5, 100),\n    'humidity': np.random.normal(60, 10, 100)\n}, index=dates_train)\n\ndata_val = pd.DataFrame({\n    'temperature': np.random.normal(22, 5, 50),\n    'humidity': np.random.normal(55, 10, 50)\n}, index=dates_val)\n\ndata_test = pd.DataFrame({\n    'temperature': np.random.normal(25, 5, 30),\n    'humidity': np.random.normal(50, 10, 30)\n}, index=dates_test)\n\n# Visualize all datasets\ndataframes = {\n    'Train': data_train,\n    'Validation': data_val,\n    'Test': data_test\n}\n\nvisualize_ts_plotly(dataframes)\n\n\n\n# Visualize a single dataset\ndataframes = {'Data': data_train}\nvisualize_ts_plotly(dataframes, columns=['temperature'])\n\n\n\n# Customize colors and template\nvisualize_ts_plotly(\n    dataframes,\n    template='plotly_dark',\n    colors={\n        'Train': 'blue',\n        'Validation': 'green',\n        'Test': 'red'\n    },\n    figsize=(1400, 600)\n)\n\n\n\n\n\n\nVisualize multiple time series datasets interactively with Plotly.\nSignature:\ndef visualize_ts_plotly(\n    dataframes: Dict[str, pd.DataFrame],\n    columns: Optional[List[str]] = None,\n    title_suffix: str = \"\",\n    figsize: tuple[int, int] = (1000, 500),\n    template: str = \"plotly_white\",\n    colors: Optional[Dict[str, str]] = None,\n    **kwargs: Any,\n) -&gt; None\nParameters:\n\n\n\n\n\n\n\n\n\nParameter\nType\nDefault\nDescription\n\n\n\n\ndataframes\nDict[str, DataFrame]\nRequired\nDictionary mapping dataset names to DataFrames with datetime index\n\n\ncolumns\nlist[str]\nNone\nColumns to visualize. If None, all columns are used\n\n\ntitle_suffix\nstr\n“”\nSuffix to append to column names in titles (e.g., “[°C]”)\n\n\nfigsize\ntuple[int, int]\n(1000, 500)\nFigure size as (width, height) in pixels\n\n\ntemplate\nstr\n“plotly_white”\nPlotly template name (“plotly_white”, “plotly_dark”, “ggplot2”, etc.)\n\n\ncolors\nDict[str, str]\nNone\nDictionary mapping dataset names to colors. If None, uses default colors\n\n\n**kwargs\nAny\n-\nAdditional arguments passed to go.Scatter() (e.g., fill=‘tozeroy’)\n\n\n\nReturns:\nNone. Displays Plotly figures.\nRaises:\n\nValueError - If dataframes dict is empty, contains empty DataFrames, or if specified columns don’t exist\nImportError - If plotly is not installed\nTypeError - If dataframes parameter is not a dictionary\n\nExample:\nimport pandas as pd\nimport numpy as np\nfrom spotforecast2.preprocessing.time_series_visualization import visualize_ts_plotly\n\n# Create sample data\nnp.random.seed(42)\ndates = pd.date_range('2024-01-01', periods=100, freq='h')\ndf = pd.DataFrame({\n    'temperature': np.random.normal(20, 5, 100),\n    'humidity': np.random.normal(60, 10, 100)\n}, index=dates)\n\n# Visualize single dataset\nvisualize_ts_plotly({'Data': df})\n\n\n\nCompare multiple datasets with optional statistical overlays.\nSignature:\ndef visualize_ts_comparison(\n    dataframes: Dict[str, pd.DataFrame],\n    columns: Optional[List[str]] = None,\n    title_suffix: str = \"\",\n    figsize: tuple[int, int] = (1000, 500),\n    template: str = \"plotly_white\",\n    colors: Optional[Dict[str, str]] = None,\n    show_mean: bool = False,\n    **kwargs: Any,\n) -&gt; None\nParameters:\n\n\n\n\n\n\n\n\n\nParameter\nType\nDefault\nDescription\n\n\n\n\ndataframes\nDict[str, DataFrame]\nRequired\nDictionary mapping dataset names to DataFrames\n\n\ncolumns\nlist[str]\nNone\nColumns to visualize. If None, all columns are used\n\n\ntitle_suffix\nstr\n“”\nSuffix to append to titles\n\n\nfigsize\ntuple[int, int]\n(1000, 500)\nFigure size as (width, height) in pixels\n\n\ntemplate\nstr\n“plotly_white”\nPlotly template\n\n\ncolors\nDict[str, str]\nNone\nDictionary mapping dataset names to colors\n\n\nshow_mean\nbool\nFalse\nIf True, overlay the mean of all datasets\n\n\n**kwargs\nAny\n-\nAdditional arguments for go.Scatter()\n\n\n\nReturns:\nNone. Displays Plotly figures.\nRaises:\n\nValueError - If dataframes dict is empty\nImportError - If plotly is not installed\n\nExample:\nimport pandas as pd\nimport numpy as np\nfrom spotforecast2.preprocessing.time_series_visualization import visualize_ts_comparison\n\n# Create sample data\nnp.random.seed(42)\ndates1 = pd.date_range('2024-01-01', periods=100, freq='h')\ndates2 = pd.date_range('2024-05-11', periods=100, freq='h')\n\ndf1 = pd.DataFrame({\n    'value': np.random.normal(20, 5, 100)\n}, index=dates1)\n\ndf2 = pd.DataFrame({\n    'value': np.random.normal(22, 5, 100)\n}, index=dates2)\n\n# Compare with mean overlay\nvisualize_ts_comparison(\n    {'Dataset1': df1, 'Dataset2': df2},\n    show_mean=True\n)\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom spotforecast2.preprocessing.time_series_visualization import visualize_ts_plotly\n\n# Create time series data\nnp.random.seed(42)\nfull_data = pd.DataFrame({\n    'temperature': np.sin(np.linspace(0, 10, 300)) + np.random.normal(0, 0.1, 300),\n    'humidity': np.cos(np.linspace(0, 10, 300)) * 100 + np.random.normal(50, 5, 300)\n}, index=pd.date_range('2024-01-01', periods=300, freq='h'))\n\n# Split data\nsplit1 = int(0.6 * len(full_data))\nsplit2 = int(0.8 * len(full_data))\n\ndata_train = full_data.iloc[:split1]\ndata_val = full_data.iloc[split1:split2]\ndata_test = full_data.iloc[split2:]\n\n# Visualize\ndataframes = {\n    'Train': data_train,\n    'Validation': data_val,\n    'Test': data_test\n}\n\nvisualize_ts_plotly(\n    dataframes,\n    template='plotly_white',\n    figsize=(1200, 600)\n)\n\n\n\nfrom spotforecast2.preprocessing.time_series_visualization import visualize_ts_comparison\n\n# Create datasets from different time periods\ndates1 = pd.date_range('2024-01-01', periods=100, freq='h')\ndates2 = pd.date_range('2024-04-01', periods=100, freq='h')\ndates3 = pd.date_range('2024-07-01', periods=100, freq='h')\n\ndf1 = pd.DataFrame({\n    'temperature': np.random.normal(15, 3, 100)\n}, index=dates1)\n\ndf2 = pd.DataFrame({\n    'temperature': np.random.normal(22, 3, 100)\n}, index=dates2)\n\ndf3 = pd.DataFrame({\n    'temperature': np.random.normal(25, 3, 100)\n}, index=dates3)\n\n# Compare with mean\nvisualize_ts_comparison(\n    {\n        'Winter': df1,\n        'Spring': df2,\n        'Summer': df3\n    },\n    show_mean=True,\n    colors={\n        'Winter': 'blue',\n        'Spring': 'green',\n        'Summer': 'red'\n    }\n)\n\n\n\n# Function works with any number of datasets\ndataframes = {}\n\nfor i in range(5):\n    dates = pd.date_range(f'2024-{i+1:02d}-01', periods=50, freq='h')\n    dataframes[f'Month_{i+1}'] = pd.DataFrame({\n        'sales': np.random.gamma(2, 2, 50) * 1000\n    }, index=dates)\n\nvisualize_ts_plotly(\n    dataframes,\n    title_suffix='[USD]',\n    figsize=(1400, 600)\n)\n\n\n\n\n\n\nFigure size as (width, height) in pixels:\n# Small figure\nvisualize_ts_plotly(dataframes, figsize=(800, 400))\n\n# Large figure for detailed inspection\nvisualize_ts_plotly(dataframes, figsize=(1600, 800))\n\n\n\nPlotly provides several built-in templates:\n# Light theme (default)\nvisualize_ts_plotly(dataframes, template='plotly_white')\n\n# Dark theme\nvisualize_ts_plotly(dataframes, template='plotly_dark')\n\n# Minimal theme\nvisualize_ts_plotly(dataframes, template='plotly')\n\n# Other themes\nvisualize_ts_plotly(dataframes, template='ggplot2')\nvisualize_ts_plotly(dataframes, template='seaborn')\n\n\n\nDefine custom colors for each dataset:\ncolors = {\n    'Train': '#1f77b4',      # Blue\n    'Validation': '#ff7f0e', # Orange\n    'Test': '#2ca02c'        # Green\n}\n\nvisualize_ts_plotly(dataframes, colors=colors)\n\n\n\nPass additional options to Plotly Scatter:\nvisualize_ts_plotly(\n    dataframes,\n    fill='tozeroy',           # Fill area under line\n    line=dict(width=2),       # Line width\n    opacity=0.8               # Transparency\n)\n\n\n\n\n\n\nAlways use pandas datetime index for proper time axis handling:\n# Good\ndf = pd.DataFrame(data, index=pd.date_range('2024-01-01', periods=len(data), freq='h'))\n\n# Avoid\ndf = pd.DataFrame(data)  # Will use default integer index\n\n\n\nEnsure all DataFrames have consistent columns for comparison:\n# Verify columns match\ncolumns = set(df1.columns) & set(df2.columns) & set(df3.columns)\nif not columns:\n    raise ValueError(\"DataFrames have no common columns\")\n\n\n\nFor large time series, consider subsampling:\n# Subsample every 10th point\ndf_sub = df[::10]\nvisualize_ts_plotly({'Data': df_sub})\n\n\n\nUse descriptive names for datasets:\n# Good\ndataframes = {\n    'Training (2023)': data_train,\n    'Validation (Jan 2024)': data_val,\n    'Testing (Feb 2024)': data_test\n}\n\n# Avoid\ndataframes = {\n    'A': data_train,\n    'B': data_val,\n    'C': data_test\n}\n\n\n\n\n\n\nIf datasets overlap in time, use separate figures:\n# Visualize one column at a time\nfor col in dataframes[list(dataframes.keys())[0]].columns:\n    visualize_ts_plotly(dataframes, columns=[col])\n\n\n\nDownsample before visualization:\n# Downsample to hourly\ndf_downsampled = df.resample('1H').mean()\nvisualize_ts_plotly({'Data': df_downsampled})\n\n\n\nHandle missing values before visualization:\n# Forward fill missing values\ndf_filled = df.fillna(method='ffill')\nvisualize_ts_plotly({'Data': df_filled})\n\n\n\n\nThis module includes comprehensive pytest tests validating all documentation examples and API functionality. Tests are located in tests/test_docs_time_series_visualization_examples.py.\n\n\nRun all time series visualization tests:\nuv run pytest tests/test_docs_time_series_visualization_examples.py -v\nRun specific test class:\nuv run pytest tests/test_docs_time_series_visualization_examples.py::TestVisualizeTimeSeriesPlotlyBasic -v\n\n\n\nThe test suite includes 50 comprehensive tests covering:\n\nBasic Visualization (9 tests): Single/multiple dataset visualization, column selection, custom parameters\nComparison Functionality (6 tests): Dataset comparison, statistical overlays, customization\nComplete Workflows (3 tests): Train/val/test split visualization, multi-dataset comparison, dynamic datasets\nParameters & Configuration (8 tests): figsize options, template variations, color customization\nBest Practices (4 tests): Datetime index handling, consistent shapes, subsampling for large datasets\nEdge Cases (7 tests): Single value, constant values, NaN handling, negative/large values, many columns\nAPI Examples (5 tests): Quick start examples, API function validation\nData Integrity (3 tests): Index preservation, data value preservation, dataset independence\nSafety-Critical (5 tests): Error handling, empty input validation, determinism\n\n\n\n\nVerify all time series visualization tests pass:\nuv run pytest tests/test_docs_time_series_visualization_examples.py --tb=short -q\nExpected output: 50 passed\n\n\n\nAll code examples in this documentation have been validated with pytest: - Quick start examples (all variants) - Complete workflow examples (train/val/test split, comparison, dynamic) - Parameter configuration examples (figsize, templates, colors) - Best practices examples (datetime index, consistent shapes, large datasets) - Troubleshooting examples (overlapping datasets, memory issues, missing data)\n\n\n\n\n\nOutlier Detection and Visualization\nPlotly Documentation\n\n\n\n\n\nPlotly Dash and Plotly.py documentation: https://plotly.com/python/\nPandas datetime index: https://pandas.pydata.org/docs/user_guide/timeseries.html",
    "crumbs": [
      "Preprocessing Guides",
      "Time Series Visualization"
    ]
  },
  {
    "objectID": "docs/preprocessing/time_series_visualization.html#overview",
    "href": "docs/preprocessing/time_series_visualization.html#overview",
    "title": "Time Series Visualization",
    "section": "",
    "text": "The time series visualization module includes two main functions:\n\nvisualize_ts_plotly() - Visualize multiple time series datasets with Plotly\nvisualize_ts_comparison() - Compare datasets with optional statistical overlays\n\nThese functions provide a flexible, interactive way to explore time series data with support for train/validation/test splits or any custom dataset groupings.",
    "crumbs": [
      "Preprocessing Guides",
      "Time Series Visualization"
    ]
  },
  {
    "objectID": "docs/preprocessing/time_series_visualization.html#installation",
    "href": "docs/preprocessing/time_series_visualization.html#installation",
    "title": "Time Series Visualization",
    "section": "",
    "text": "The time series visualization functions require plotly:\nUsing pip:\npip install plotly\nUsing uv:\nuv pip install plotly",
    "crumbs": [
      "Preprocessing Guides",
      "Time Series Visualization"
    ]
  },
  {
    "objectID": "docs/preprocessing/time_series_visualization.html#quick-start",
    "href": "docs/preprocessing/time_series_visualization.html#quick-start",
    "title": "Time Series Visualization",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom spotforecast2.preprocessing.time_series_visualization import visualize_ts_plotly\n\n# Create sample datasets\nnp.random.seed(42)\ndates_train = pd.date_range('2024-01-01', periods=100, freq='h')\ndates_val = pd.date_range('2024-05-11', periods=50, freq='h')\ndates_test = pd.date_range('2024-07-01', periods=30, freq='h')\n\ndata_train = pd.DataFrame({\n    'temperature': np.random.normal(20, 5, 100),\n    'humidity': np.random.normal(60, 10, 100)\n}, index=dates_train)\n\ndata_val = pd.DataFrame({\n    'temperature': np.random.normal(22, 5, 50),\n    'humidity': np.random.normal(55, 10, 50)\n}, index=dates_val)\n\ndata_test = pd.DataFrame({\n    'temperature': np.random.normal(25, 5, 30),\n    'humidity': np.random.normal(50, 10, 30)\n}, index=dates_test)\n\n# Visualize all datasets\ndataframes = {\n    'Train': data_train,\n    'Validation': data_val,\n    'Test': data_test\n}\n\nvisualize_ts_plotly(dataframes)\n\n\n\n# Visualize a single dataset\ndataframes = {'Data': data_train}\nvisualize_ts_plotly(dataframes, columns=['temperature'])\n\n\n\n# Customize colors and template\nvisualize_ts_plotly(\n    dataframes,\n    template='plotly_dark',\n    colors={\n        'Train': 'blue',\n        'Validation': 'green',\n        'Test': 'red'\n    },\n    figsize=(1400, 600)\n)",
    "crumbs": [
      "Preprocessing Guides",
      "Time Series Visualization"
    ]
  },
  {
    "objectID": "docs/preprocessing/time_series_visualization.html#api-reference",
    "href": "docs/preprocessing/time_series_visualization.html#api-reference",
    "title": "Time Series Visualization",
    "section": "",
    "text": "Visualize multiple time series datasets interactively with Plotly.\nSignature:\ndef visualize_ts_plotly(\n    dataframes: Dict[str, pd.DataFrame],\n    columns: Optional[List[str]] = None,\n    title_suffix: str = \"\",\n    figsize: tuple[int, int] = (1000, 500),\n    template: str = \"plotly_white\",\n    colors: Optional[Dict[str, str]] = None,\n    **kwargs: Any,\n) -&gt; None\nParameters:\n\n\n\n\n\n\n\n\n\nParameter\nType\nDefault\nDescription\n\n\n\n\ndataframes\nDict[str, DataFrame]\nRequired\nDictionary mapping dataset names to DataFrames with datetime index\n\n\ncolumns\nlist[str]\nNone\nColumns to visualize. If None, all columns are used\n\n\ntitle_suffix\nstr\n“”\nSuffix to append to column names in titles (e.g., “[°C]”)\n\n\nfigsize\ntuple[int, int]\n(1000, 500)\nFigure size as (width, height) in pixels\n\n\ntemplate\nstr\n“plotly_white”\nPlotly template name (“plotly_white”, “plotly_dark”, “ggplot2”, etc.)\n\n\ncolors\nDict[str, str]\nNone\nDictionary mapping dataset names to colors. If None, uses default colors\n\n\n**kwargs\nAny\n-\nAdditional arguments passed to go.Scatter() (e.g., fill=‘tozeroy’)\n\n\n\nReturns:\nNone. Displays Plotly figures.\nRaises:\n\nValueError - If dataframes dict is empty, contains empty DataFrames, or if specified columns don’t exist\nImportError - If plotly is not installed\nTypeError - If dataframes parameter is not a dictionary\n\nExample:\nimport pandas as pd\nimport numpy as np\nfrom spotforecast2.preprocessing.time_series_visualization import visualize_ts_plotly\n\n# Create sample data\nnp.random.seed(42)\ndates = pd.date_range('2024-01-01', periods=100, freq='h')\ndf = pd.DataFrame({\n    'temperature': np.random.normal(20, 5, 100),\n    'humidity': np.random.normal(60, 10, 100)\n}, index=dates)\n\n# Visualize single dataset\nvisualize_ts_plotly({'Data': df})\n\n\n\nCompare multiple datasets with optional statistical overlays.\nSignature:\ndef visualize_ts_comparison(\n    dataframes: Dict[str, pd.DataFrame],\n    columns: Optional[List[str]] = None,\n    title_suffix: str = \"\",\n    figsize: tuple[int, int] = (1000, 500),\n    template: str = \"plotly_white\",\n    colors: Optional[Dict[str, str]] = None,\n    show_mean: bool = False,\n    **kwargs: Any,\n) -&gt; None\nParameters:\n\n\n\n\n\n\n\n\n\nParameter\nType\nDefault\nDescription\n\n\n\n\ndataframes\nDict[str, DataFrame]\nRequired\nDictionary mapping dataset names to DataFrames\n\n\ncolumns\nlist[str]\nNone\nColumns to visualize. If None, all columns are used\n\n\ntitle_suffix\nstr\n“”\nSuffix to append to titles\n\n\nfigsize\ntuple[int, int]\n(1000, 500)\nFigure size as (width, height) in pixels\n\n\ntemplate\nstr\n“plotly_white”\nPlotly template\n\n\ncolors\nDict[str, str]\nNone\nDictionary mapping dataset names to colors\n\n\nshow_mean\nbool\nFalse\nIf True, overlay the mean of all datasets\n\n\n**kwargs\nAny\n-\nAdditional arguments for go.Scatter()\n\n\n\nReturns:\nNone. Displays Plotly figures.\nRaises:\n\nValueError - If dataframes dict is empty\nImportError - If plotly is not installed\n\nExample:\nimport pandas as pd\nimport numpy as np\nfrom spotforecast2.preprocessing.time_series_visualization import visualize_ts_comparison\n\n# Create sample data\nnp.random.seed(42)\ndates1 = pd.date_range('2024-01-01', periods=100, freq='h')\ndates2 = pd.date_range('2024-05-11', periods=100, freq='h')\n\ndf1 = pd.DataFrame({\n    'value': np.random.normal(20, 5, 100)\n}, index=dates1)\n\ndf2 = pd.DataFrame({\n    'value': np.random.normal(22, 5, 100)\n}, index=dates2)\n\n# Compare with mean overlay\nvisualize_ts_comparison(\n    {'Dataset1': df1, 'Dataset2': df2},\n    show_mean=True\n)",
    "crumbs": [
      "Preprocessing Guides",
      "Time Series Visualization"
    ]
  },
  {
    "objectID": "docs/preprocessing/time_series_visualization.html#complete-workflow-examples",
    "href": "docs/preprocessing/time_series_visualization.html#complete-workflow-examples",
    "title": "Time Series Visualization",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom spotforecast2.preprocessing.time_series_visualization import visualize_ts_plotly\n\n# Create time series data\nnp.random.seed(42)\nfull_data = pd.DataFrame({\n    'temperature': np.sin(np.linspace(0, 10, 300)) + np.random.normal(0, 0.1, 300),\n    'humidity': np.cos(np.linspace(0, 10, 300)) * 100 + np.random.normal(50, 5, 300)\n}, index=pd.date_range('2024-01-01', periods=300, freq='h'))\n\n# Split data\nsplit1 = int(0.6 * len(full_data))\nsplit2 = int(0.8 * len(full_data))\n\ndata_train = full_data.iloc[:split1]\ndata_val = full_data.iloc[split1:split2]\ndata_test = full_data.iloc[split2:]\n\n# Visualize\ndataframes = {\n    'Train': data_train,\n    'Validation': data_val,\n    'Test': data_test\n}\n\nvisualize_ts_plotly(\n    dataframes,\n    template='plotly_white',\n    figsize=(1200, 600)\n)\n\n\n\nfrom spotforecast2.preprocessing.time_series_visualization import visualize_ts_comparison\n\n# Create datasets from different time periods\ndates1 = pd.date_range('2024-01-01', periods=100, freq='h')\ndates2 = pd.date_range('2024-04-01', periods=100, freq='h')\ndates3 = pd.date_range('2024-07-01', periods=100, freq='h')\n\ndf1 = pd.DataFrame({\n    'temperature': np.random.normal(15, 3, 100)\n}, index=dates1)\n\ndf2 = pd.DataFrame({\n    'temperature': np.random.normal(22, 3, 100)\n}, index=dates2)\n\ndf3 = pd.DataFrame({\n    'temperature': np.random.normal(25, 3, 100)\n}, index=dates3)\n\n# Compare with mean\nvisualize_ts_comparison(\n    {\n        'Winter': df1,\n        'Spring': df2,\n        'Summer': df3\n    },\n    show_mean=True,\n    colors={\n        'Winter': 'blue',\n        'Spring': 'green',\n        'Summer': 'red'\n    }\n)\n\n\n\n# Function works with any number of datasets\ndataframes = {}\n\nfor i in range(5):\n    dates = pd.date_range(f'2024-{i+1:02d}-01', periods=50, freq='h')\n    dataframes[f'Month_{i+1}'] = pd.DataFrame({\n        'sales': np.random.gamma(2, 2, 50) * 1000\n    }, index=dates)\n\nvisualize_ts_plotly(\n    dataframes,\n    title_suffix='[USD]',\n    figsize=(1400, 600)\n)",
    "crumbs": [
      "Preprocessing Guides",
      "Time Series Visualization"
    ]
  },
  {
    "objectID": "docs/preprocessing/time_series_visualization.html#parameters-and-configuration",
    "href": "docs/preprocessing/time_series_visualization.html#parameters-and-configuration",
    "title": "Time Series Visualization",
    "section": "",
    "text": "Figure size as (width, height) in pixels:\n# Small figure\nvisualize_ts_plotly(dataframes, figsize=(800, 400))\n\n# Large figure for detailed inspection\nvisualize_ts_plotly(dataframes, figsize=(1600, 800))\n\n\n\nPlotly provides several built-in templates:\n# Light theme (default)\nvisualize_ts_plotly(dataframes, template='plotly_white')\n\n# Dark theme\nvisualize_ts_plotly(dataframes, template='plotly_dark')\n\n# Minimal theme\nvisualize_ts_plotly(dataframes, template='plotly')\n\n# Other themes\nvisualize_ts_plotly(dataframes, template='ggplot2')\nvisualize_ts_plotly(dataframes, template='seaborn')\n\n\n\nDefine custom colors for each dataset:\ncolors = {\n    'Train': '#1f77b4',      # Blue\n    'Validation': '#ff7f0e', # Orange\n    'Test': '#2ca02c'        # Green\n}\n\nvisualize_ts_plotly(dataframes, colors=colors)\n\n\n\nPass additional options to Plotly Scatter:\nvisualize_ts_plotly(\n    dataframes,\n    fill='tozeroy',           # Fill area under line\n    line=dict(width=2),       # Line width\n    opacity=0.8               # Transparency\n)",
    "crumbs": [
      "Preprocessing Guides",
      "Time Series Visualization"
    ]
  },
  {
    "objectID": "docs/preprocessing/time_series_visualization.html#best-practices",
    "href": "docs/preprocessing/time_series_visualization.html#best-practices",
    "title": "Time Series Visualization",
    "section": "",
    "text": "Always use pandas datetime index for proper time axis handling:\n# Good\ndf = pd.DataFrame(data, index=pd.date_range('2024-01-01', periods=len(data), freq='h'))\n\n# Avoid\ndf = pd.DataFrame(data)  # Will use default integer index\n\n\n\nEnsure all DataFrames have consistent columns for comparison:\n# Verify columns match\ncolumns = set(df1.columns) & set(df2.columns) & set(df3.columns)\nif not columns:\n    raise ValueError(\"DataFrames have no common columns\")\n\n\n\nFor large time series, consider subsampling:\n# Subsample every 10th point\ndf_sub = df[::10]\nvisualize_ts_plotly({'Data': df_sub})\n\n\n\nUse descriptive names for datasets:\n# Good\ndataframes = {\n    'Training (2023)': data_train,\n    'Validation (Jan 2024)': data_val,\n    'Testing (Feb 2024)': data_test\n}\n\n# Avoid\ndataframes = {\n    'A': data_train,\n    'B': data_val,\n    'C': data_test\n}",
    "crumbs": [
      "Preprocessing Guides",
      "Time Series Visualization"
    ]
  },
  {
    "objectID": "docs/preprocessing/time_series_visualization.html#troubleshooting",
    "href": "docs/preprocessing/time_series_visualization.html#troubleshooting",
    "title": "Time Series Visualization",
    "section": "",
    "text": "If datasets overlap in time, use separate figures:\n# Visualize one column at a time\nfor col in dataframes[list(dataframes.keys())[0]].columns:\n    visualize_ts_plotly(dataframes, columns=[col])\n\n\n\nDownsample before visualization:\n# Downsample to hourly\ndf_downsampled = df.resample('1H').mean()\nvisualize_ts_plotly({'Data': df_downsampled})\n\n\n\nHandle missing values before visualization:\n# Forward fill missing values\ndf_filled = df.fillna(method='ffill')\nvisualize_ts_plotly({'Data': df_filled})",
    "crumbs": [
      "Preprocessing Guides",
      "Time Series Visualization"
    ]
  },
  {
    "objectID": "docs/preprocessing/time_series_visualization.html#testing",
    "href": "docs/preprocessing/time_series_visualization.html#testing",
    "title": "Time Series Visualization",
    "section": "",
    "text": "This module includes comprehensive pytest tests validating all documentation examples and API functionality. Tests are located in tests/test_docs_time_series_visualization_examples.py.\n\n\nRun all time series visualization tests:\nuv run pytest tests/test_docs_time_series_visualization_examples.py -v\nRun specific test class:\nuv run pytest tests/test_docs_time_series_visualization_examples.py::TestVisualizeTimeSeriesPlotlyBasic -v\n\n\n\nThe test suite includes 50 comprehensive tests covering:\n\nBasic Visualization (9 tests): Single/multiple dataset visualization, column selection, custom parameters\nComparison Functionality (6 tests): Dataset comparison, statistical overlays, customization\nComplete Workflows (3 tests): Train/val/test split visualization, multi-dataset comparison, dynamic datasets\nParameters & Configuration (8 tests): figsize options, template variations, color customization\nBest Practices (4 tests): Datetime index handling, consistent shapes, subsampling for large datasets\nEdge Cases (7 tests): Single value, constant values, NaN handling, negative/large values, many columns\nAPI Examples (5 tests): Quick start examples, API function validation\nData Integrity (3 tests): Index preservation, data value preservation, dataset independence\nSafety-Critical (5 tests): Error handling, empty input validation, determinism\n\n\n\n\nVerify all time series visualization tests pass:\nuv run pytest tests/test_docs_time_series_visualization_examples.py --tb=short -q\nExpected output: 50 passed\n\n\n\nAll code examples in this documentation have been validated with pytest: - Quick start examples (all variants) - Complete workflow examples (train/val/test split, comparison, dynamic) - Parameter configuration examples (figsize, templates, colors) - Best practices examples (datetime index, consistent shapes, large datasets) - Troubleshooting examples (overlapping datasets, memory issues, missing data)",
    "crumbs": [
      "Preprocessing Guides",
      "Time Series Visualization"
    ]
  },
  {
    "objectID": "docs/preprocessing/time_series_visualization.html#see-also",
    "href": "docs/preprocessing/time_series_visualization.html#see-also",
    "title": "Time Series Visualization",
    "section": "",
    "text": "Outlier Detection and Visualization\nPlotly Documentation",
    "crumbs": [
      "Preprocessing Guides",
      "Time Series Visualization"
    ]
  },
  {
    "objectID": "docs/preprocessing/time_series_visualization.html#references",
    "href": "docs/preprocessing/time_series_visualization.html#references",
    "title": "Time Series Visualization",
    "section": "",
    "text": "Plotly Dash and Plotly.py documentation: https://plotly.com/python/\nPandas datetime index: https://pandas.pydata.org/docs/user_guide/timeseries.html",
    "crumbs": [
      "Preprocessing Guides",
      "Time Series Visualization"
    ]
  },
  {
    "objectID": "docs/entsoe.html",
    "href": "docs/entsoe.html",
    "title": "ENTSO-E Energy Forecasting Guide",
    "section": "",
    "text": "This guide provides comprehensive examples for using spotforecast2 with ENTSO-E energy data. Examples are organized from beginner to advanced, with each code snippet backed by automated tests.\n\n\nBefore running these examples, ensure you have:\n\nspotforecast2 installed: pip install spotforecast2\nAn ENTSO-E API key (optional for training examples)\n\n\n\n\n\n\n\nThe simplest way to get started is using the default configuration:\nfrom spotforecast2 import Config\n\nconfig = Config()\nprint(config.API_COUNTRY_CODE)  # 'DE'\nprint(config.predict_size)      # 24\nprint(config.random_state)      # 314159\n\n\n\nCustomize parameters for your specific use case:\nfrom spotforecast2 import Config\nimport pandas as pd\n\nconfig = Config(\n    api_country_code='FR',\n    predict_size=48,\n    refit_size=14,\n    random_state=42\n)\nprint(config.API_COUNTRY_CODE)  # 'FR'\nprint(config.predict_size)      # 48\n\n\n\n\n\n\nParameter\nType\nDefault\nDescription\n\n\n\n\napi_country_code\nstr\n“DE”\nISO country code for ENTSO-E API\n\n\npredict_size\nint\n24\nNumber of hours to predict ahead\n\n\nrefit_size\nint\n7\nNumber of days between model refits\n\n\ntrain_size\nTimedelta\n3 years\nTraining data window\n\n\nrandom_state\nint\n314159\nRandom seed for reproducibility\n\n\nperiods\nList[Period]\n5 periods\nCyclical feature encodings\n\n\n\n\n\n\nView the cyclical encoding periods:\nfrom spotforecast2 import Config\n\nconfig = Config()\nfor period in config.periods:\n    print(f\"{period.name}: {period.n_periods} basis functions\")\n\n\n\n\n\n\n\nPeriods define cyclical time features using radial basis functions:\nfrom spotforecast2_safe.data import Period\n\ndaily = Period(\n    name='daily',\n    n_periods=12,\n    column='hour',\n    input_range=(1, 24)\n)\nprint(daily.name)        # 'daily'\nprint(daily.n_periods)   # 12\n\n\n\nTransform time features into smooth cyclical encodings:\nfrom spotforecast2_safe.preprocessing import RepeatingBasisFunction\nimport pandas as pd\n\nrbf = RepeatingBasisFunction(\n    n_periods=12,\n    column='hour',\n    input_range=(1, 24)\n)\n\ndf = pd.DataFrame({'hour': range(1, 25)})\nfeatures = rbf.transform(df)\nprint(features.shape)  # (24, 12)\n\n\n\nBuild complete exogenous feature sets including holidays and weekends:\nfrom spotforecast2_safe.preprocessing import ExogBuilder\nfrom spotforecast2_safe.data import Period\nimport pandas as pd\n\nperiods = [\n    Period(name='daily', n_periods=12, column='hour', input_range=(1, 24)),\n    Period(name='weekly', n_periods=7, column='dayofweek', input_range=(0, 6)),\n]\n\nbuilder = ExogBuilder(periods=periods, country_code='DE')\nX = builder.build(\n    pd.Timestamp('2025-01-01', tz='UTC'),\n    pd.Timestamp('2025-01-02', tz='UTC')\n)\nprint(X.shape)  # (25, 21) - 12 + 7 + 2 (holiday, weekend)\n\n\n\nCombine configuration and feature building:\nfrom spotforecast2 import Config\nfrom spotforecast2_safe.preprocessing import ExogBuilder\nimport pandas as pd\n\nconfig = Config()\nbuilder = ExogBuilder(\n    periods=config.periods,\n    country_code=config.API_COUNTRY_CODE\n)\nX = builder.build(\n    pd.Timestamp('2025-12-31', tz='UTC'),\n    pd.Timestamp('2026-01-01', tz='UTC')\n)\nprint(f\"Generated {X.shape[1]} features for {X.shape[0]} hours\")\n\n\n\n\n\n\n\nHandle missing values in time series data:\nfrom spotforecast2_safe.preprocessing import LinearlyInterpolateTS\nimport pandas as pd\nimport numpy as np\n\nts = pd.Series(\n    [1.0, np.nan, 3.0, np.nan, 5.0],\n    index=pd.date_range('2025-01-01', periods=5, freq='h')\n)\n\ninterpolator = LinearlyInterpolateTS()\nts_clean = interpolator.fit_transform(ts)\n\nprint(ts_clean.values)  # [1.0, 2.0, 3.0, 4.0, 5.0]\n\n\n\n\n\n\n\nCreate a LightGBM-based recursive forecaster:\nfrom spotforecast2.tasks.task_entsoe import ForecasterRecursiveLGBM, config\n\nmodel = ForecasterRecursiveLGBM(iteration=1)\n\nprint(model.name)             # 'lgbm'\nprint(model.random_state)     # 314159 (from config)\nprint(len(model.preprocessor.periods))  # 5 (from config)\n\n\n\nCreate an XGBoost-based recursive forecaster:\nfrom spotforecast2.tasks.task_entsoe import ForecasterRecursiveXGB, config\n\nmodel = ForecasterRecursiveXGB(iteration=1, lags=24)\n\nprint(model.name)  # 'xgb'\n\n\n\nOverride default configuration values:\nfrom spotforecast2.tasks.task_entsoe import ForecasterRecursiveLGBM\nfrom spotforecast2_safe.data import Period\n\ncustom_periods = [\n    Period(name='hourly', n_periods=24, column='hour', input_range=(1, 24)),\n]\n\nmodel = ForecasterRecursiveLGBM(\n    iteration=1,\n    lags=48,\n    periods=custom_periods,\n    country_code='FR',\n    random_state=42\n)\n\nprint(len(model.preprocessor.periods))  # 1\nprint(model.preprocessor.country_code)  # 'FR'\n\n\n\n\n\n\n\nFor users working in Jupyter Notebooks or Quarto, the entire ENTSO-E pipeline can be executed using the Python API. This approach is highly recommended for safety-critical research as it allows for precise control over time windows and hyperparameters.\nimport pandas as pd\nimport os\nfrom spotforecast2_safe.downloader.entsoe import download_new_data\nfrom spotforecast2_safe.manager.trainer import handle_training as handle_training_safe\nfrom spotforecast2_safe.manager.predictor import get_model_prediction as get_model_prediction_safe\nfrom spotforecast2.manager.plotter import make_plot\nfrom spotforecast2.tasks.task_entsoe import ForecasterRecursiveLGBM\n\n# 1. Setup Time Windows (Last 3 years until last month) and country:\ncountry_code = \"ES\"  \nnow = pd.Timestamp.now(tz='UTC').floor('D')\ncurrent_month_start = now.replace(day=1)\nlast_month_start = (current_month_start - pd.Timedelta(days=1)).replace(day=1)\n\n# 2. Download Data (Optional, requires ENTSOE_API_KEY)\napi_key = os.environ.get(\"ENTSOE_API_KEY\")\nif api_key:\n    download_new_data(api_key=api_key, start=\"202301010000\", country_code=country_code)\n\n# 3. Configure and Train\n# Explicit parameters override global configuration for reproducibility\nmodel_class = ForecasterRecursiveLGBM\nmodel_name = \"lgbm_advanced\"\n\nhandle_training_safe(\n    model_class=model_class,\n    model_name=model_name,\n    train_size=pd.Timedelta(days=3 * 365),\n    end_dev=last_month_start.strftime(\"%Y-%m-%d %H:%M%z\"),\n    country_code=country_code\n)\n\n# 4. Generate Predictions for the forecast horizon\n# The predictor will automatically load the model trained above\npredictions = get_model_prediction_safe(\n    model_name=model_name,\n    predict_size=24 * 31\n)\n\n# 5. Visualize Results\nif predictions:\n    make_plot(predictions)\n\n\n\n\n\n\n\nAccess the data storage location:\nfrom spotforecast2_safe.data import get_data_home\n\ndata_home = get_data_home()\nprint(data_home)  # ~/spotforecast2_data or SPOTFORECAST2_DATA\n\n\n\n\n\n\n\n# Download with API key\nuv run spotforecast2-entsoe download --api-key YOUR_API_KEY 202301010000\n\n# Download with date range\nuv run spotforecast2-entsoe download 202301010000 202312312300\n\n# Force re-download\nuv run spotforecast2-entsoe download --force 202301010000\n\n\n\n# Train LightGBM model\nuv run spotforecast2-entsoe train lgbm\n\n# Train XGBoost model\nuv run spotforecast2-entsoe train xgb\n\n# Force retraining\nuv run spotforecast2-entsoe train lgbm --force\n\n\n\n# Predict with default model (lgbm)\nuv run spotforecast2-entsoe predict\n\n# Predict with specific model\nuv run spotforecast2-entsoe predict lgbm\nuv run spotforecast2-entsoe predict xgb\n\n# Predict and generate plot\nuv run spotforecast2-entsoe predict --plot\n\n\n\nuv run spotforecast2-entsoe merge\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nENTSOE_API_KEY\nENTSO-E API key for data downloads\n\n\nSPOTFORECAST2_DATA\nCustom data directory (default: ~/spotforecast2_data)\n\n\n\n\n\n\n\nAll examples in this guide are validated by automated tests:\n# Run documentation example tests\nuv run pytest tests/test_docs_entsoe_examples.py -v\n\n# Run all ENTSO-E tests\nuv run pytest tests/test_tasks_entsoe.py -v\n\n\n\n\n\nTasks Overview - All available CLI commands\nAPI Reference - Detailed API documentation\nModel Persistence - Saving and loading models",
    "crumbs": [
      "Tasks Guide",
      "ENTSO-E Guide"
    ]
  },
  {
    "objectID": "docs/entsoe.html#prerequisites",
    "href": "docs/entsoe.html#prerequisites",
    "title": "ENTSO-E Energy Forecasting Guide",
    "section": "",
    "text": "Before running these examples, ensure you have:\n\nspotforecast2 installed: pip install spotforecast2\nAn ENTSO-E API key (optional for training examples)",
    "crumbs": [
      "Tasks Guide",
      "ENTSO-E Guide"
    ]
  },
  {
    "objectID": "docs/entsoe.html#configuration",
    "href": "docs/entsoe.html#configuration",
    "title": "ENTSO-E Energy Forecasting Guide",
    "section": "",
    "text": "The simplest way to get started is using the default configuration:\nfrom spotforecast2 import Config\n\nconfig = Config()\nprint(config.API_COUNTRY_CODE)  # 'DE'\nprint(config.predict_size)      # 24\nprint(config.random_state)      # 314159\n\n\n\nCustomize parameters for your specific use case:\nfrom spotforecast2 import Config\nimport pandas as pd\n\nconfig = Config(\n    api_country_code='FR',\n    predict_size=48,\n    refit_size=14,\n    random_state=42\n)\nprint(config.API_COUNTRY_CODE)  # 'FR'\nprint(config.predict_size)      # 48\n\n\n\n\n\n\nParameter\nType\nDefault\nDescription\n\n\n\n\napi_country_code\nstr\n“DE”\nISO country code for ENTSO-E API\n\n\npredict_size\nint\n24\nNumber of hours to predict ahead\n\n\nrefit_size\nint\n7\nNumber of days between model refits\n\n\ntrain_size\nTimedelta\n3 years\nTraining data window\n\n\nrandom_state\nint\n314159\nRandom seed for reproducibility\n\n\nperiods\nList[Period]\n5 periods\nCyclical feature encodings\n\n\n\n\n\n\nView the cyclical encoding periods:\nfrom spotforecast2 import Config\n\nconfig = Config()\nfor period in config.periods:\n    print(f\"{period.name}: {period.n_periods} basis functions\")",
    "crumbs": [
      "Tasks Guide",
      "ENTSO-E Guide"
    ]
  },
  {
    "objectID": "docs/entsoe.html#feature-engineering",
    "href": "docs/entsoe.html#feature-engineering",
    "title": "ENTSO-E Energy Forecasting Guide",
    "section": "",
    "text": "Periods define cyclical time features using radial basis functions:\nfrom spotforecast2_safe.data import Period\n\ndaily = Period(\n    name='daily',\n    n_periods=12,\n    column='hour',\n    input_range=(1, 24)\n)\nprint(daily.name)        # 'daily'\nprint(daily.n_periods)   # 12\n\n\n\nTransform time features into smooth cyclical encodings:\nfrom spotforecast2_safe.preprocessing import RepeatingBasisFunction\nimport pandas as pd\n\nrbf = RepeatingBasisFunction(\n    n_periods=12,\n    column='hour',\n    input_range=(1, 24)\n)\n\ndf = pd.DataFrame({'hour': range(1, 25)})\nfeatures = rbf.transform(df)\nprint(features.shape)  # (24, 12)\n\n\n\nBuild complete exogenous feature sets including holidays and weekends:\nfrom spotforecast2_safe.preprocessing import ExogBuilder\nfrom spotforecast2_safe.data import Period\nimport pandas as pd\n\nperiods = [\n    Period(name='daily', n_periods=12, column='hour', input_range=(1, 24)),\n    Period(name='weekly', n_periods=7, column='dayofweek', input_range=(0, 6)),\n]\n\nbuilder = ExogBuilder(periods=periods, country_code='DE')\nX = builder.build(\n    pd.Timestamp('2025-01-01', tz='UTC'),\n    pd.Timestamp('2025-01-02', tz='UTC')\n)\nprint(X.shape)  # (25, 21) - 12 + 7 + 2 (holiday, weekend)\n\n\n\nCombine configuration and feature building:\nfrom spotforecast2 import Config\nfrom spotforecast2_safe.preprocessing import ExogBuilder\nimport pandas as pd\n\nconfig = Config()\nbuilder = ExogBuilder(\n    periods=config.periods,\n    country_code=config.API_COUNTRY_CODE\n)\nX = builder.build(\n    pd.Timestamp('2025-12-31', tz='UTC'),\n    pd.Timestamp('2026-01-01', tz='UTC')\n)\nprint(f\"Generated {X.shape[1]} features for {X.shape[0]} hours\")",
    "crumbs": [
      "Tasks Guide",
      "ENTSO-E Guide"
    ]
  },
  {
    "objectID": "docs/entsoe.html#data-preprocessing",
    "href": "docs/entsoe.html#data-preprocessing",
    "title": "ENTSO-E Energy Forecasting Guide",
    "section": "",
    "text": "Handle missing values in time series data:\nfrom spotforecast2_safe.preprocessing import LinearlyInterpolateTS\nimport pandas as pd\nimport numpy as np\n\nts = pd.Series(\n    [1.0, np.nan, 3.0, np.nan, 5.0],\n    index=pd.date_range('2025-01-01', periods=5, freq='h')\n)\n\ninterpolator = LinearlyInterpolateTS()\nts_clean = interpolator.fit_transform(ts)\n\nprint(ts_clean.values)  # [1.0, 2.0, 3.0, 4.0, 5.0]",
    "crumbs": [
      "Tasks Guide",
      "ENTSO-E Guide"
    ]
  },
  {
    "objectID": "docs/entsoe.html#forecaster-models",
    "href": "docs/entsoe.html#forecaster-models",
    "title": "ENTSO-E Energy Forecasting Guide",
    "section": "",
    "text": "Create a LightGBM-based recursive forecaster:\nfrom spotforecast2.tasks.task_entsoe import ForecasterRecursiveLGBM, config\n\nmodel = ForecasterRecursiveLGBM(iteration=1)\n\nprint(model.name)             # 'lgbm'\nprint(model.random_state)     # 314159 (from config)\nprint(len(model.preprocessor.periods))  # 5 (from config)\n\n\n\nCreate an XGBoost-based recursive forecaster:\nfrom spotforecast2.tasks.task_entsoe import ForecasterRecursiveXGB, config\n\nmodel = ForecasterRecursiveXGB(iteration=1, lags=24)\n\nprint(model.name)  # 'xgb'\n\n\n\nOverride default configuration values:\nfrom spotforecast2.tasks.task_entsoe import ForecasterRecursiveLGBM\nfrom spotforecast2_safe.data import Period\n\ncustom_periods = [\n    Period(name='hourly', n_periods=24, column='hour', input_range=(1, 24)),\n]\n\nmodel = ForecasterRecursiveLGBM(\n    iteration=1,\n    lags=48,\n    periods=custom_periods,\n    country_code='FR',\n    random_state=42\n)\n\nprint(len(model.preprocessor.periods))  # 1\nprint(model.preprocessor.country_code)  # 'FR'",
    "crumbs": [
      "Tasks Guide",
      "ENTSO-E Guide"
    ]
  },
  {
    "objectID": "docs/entsoe.html#using-the-python-api-notebooks-quarto",
    "href": "docs/entsoe.html#using-the-python-api-notebooks-quarto",
    "title": "ENTSO-E Energy Forecasting Guide",
    "section": "",
    "text": "For users working in Jupyter Notebooks or Quarto, the entire ENTSO-E pipeline can be executed using the Python API. This approach is highly recommended for safety-critical research as it allows for precise control over time windows and hyperparameters.\nimport pandas as pd\nimport os\nfrom spotforecast2_safe.downloader.entsoe import download_new_data\nfrom spotforecast2_safe.manager.trainer import handle_training as handle_training_safe\nfrom spotforecast2_safe.manager.predictor import get_model_prediction as get_model_prediction_safe\nfrom spotforecast2.manager.plotter import make_plot\nfrom spotforecast2.tasks.task_entsoe import ForecasterRecursiveLGBM\n\n# 1. Setup Time Windows (Last 3 years until last month) and country:\ncountry_code = \"ES\"  \nnow = pd.Timestamp.now(tz='UTC').floor('D')\ncurrent_month_start = now.replace(day=1)\nlast_month_start = (current_month_start - pd.Timedelta(days=1)).replace(day=1)\n\n# 2. Download Data (Optional, requires ENTSOE_API_KEY)\napi_key = os.environ.get(\"ENTSOE_API_KEY\")\nif api_key:\n    download_new_data(api_key=api_key, start=\"202301010000\", country_code=country_code)\n\n# 3. Configure and Train\n# Explicit parameters override global configuration for reproducibility\nmodel_class = ForecasterRecursiveLGBM\nmodel_name = \"lgbm_advanced\"\n\nhandle_training_safe(\n    model_class=model_class,\n    model_name=model_name,\n    train_size=pd.Timedelta(days=3 * 365),\n    end_dev=last_month_start.strftime(\"%Y-%m-%d %H:%M%z\"),\n    country_code=country_code\n)\n\n# 4. Generate Predictions for the forecast horizon\n# The predictor will automatically load the model trained above\npredictions = get_model_prediction_safe(\n    model_name=model_name,\n    predict_size=24 * 31\n)\n\n# 5. Visualize Results\nif predictions:\n    make_plot(predictions)",
    "crumbs": [
      "Tasks Guide",
      "ENTSO-E Guide"
    ]
  },
  {
    "objectID": "docs/entsoe.html#file-paths",
    "href": "docs/entsoe.html#file-paths",
    "title": "ENTSO-E Energy Forecasting Guide",
    "section": "",
    "text": "Access the data storage location:\nfrom spotforecast2_safe.data import get_data_home\n\ndata_home = get_data_home()\nprint(data_home)  # ~/spotforecast2_data or SPOTFORECAST2_DATA",
    "crumbs": [
      "Tasks Guide",
      "ENTSO-E Guide"
    ]
  },
  {
    "objectID": "docs/entsoe.html#cli-commands",
    "href": "docs/entsoe.html#cli-commands",
    "title": "ENTSO-E Energy Forecasting Guide",
    "section": "",
    "text": "# Download with API key\nuv run spotforecast2-entsoe download --api-key YOUR_API_KEY 202301010000\n\n# Download with date range\nuv run spotforecast2-entsoe download 202301010000 202312312300\n\n# Force re-download\nuv run spotforecast2-entsoe download --force 202301010000\n\n\n\n# Train LightGBM model\nuv run spotforecast2-entsoe train lgbm\n\n# Train XGBoost model\nuv run spotforecast2-entsoe train xgb\n\n# Force retraining\nuv run spotforecast2-entsoe train lgbm --force\n\n\n\n# Predict with default model (lgbm)\nuv run spotforecast2-entsoe predict\n\n# Predict with specific model\nuv run spotforecast2-entsoe predict lgbm\nuv run spotforecast2-entsoe predict xgb\n\n# Predict and generate plot\nuv run spotforecast2-entsoe predict --plot\n\n\n\nuv run spotforecast2-entsoe merge",
    "crumbs": [
      "Tasks Guide",
      "ENTSO-E Guide"
    ]
  },
  {
    "objectID": "docs/entsoe.html#environment-variables",
    "href": "docs/entsoe.html#environment-variables",
    "title": "ENTSO-E Energy Forecasting Guide",
    "section": "",
    "text": "Variable\nDescription\n\n\n\n\nENTSOE_API_KEY\nENTSO-E API key for data downloads\n\n\nSPOTFORECAST2_DATA\nCustom data directory (default: ~/spotforecast2_data)",
    "crumbs": [
      "Tasks Guide",
      "ENTSO-E Guide"
    ]
  },
  {
    "objectID": "docs/entsoe.html#testing",
    "href": "docs/entsoe.html#testing",
    "title": "ENTSO-E Energy Forecasting Guide",
    "section": "",
    "text": "All examples in this guide are validated by automated tests:\n# Run documentation example tests\nuv run pytest tests/test_docs_entsoe_examples.py -v\n\n# Run all ENTSO-E tests\nuv run pytest tests/test_tasks_entsoe.py -v",
    "crumbs": [
      "Tasks Guide",
      "ENTSO-E Guide"
    ]
  },
  {
    "objectID": "docs/entsoe.html#see-also",
    "href": "docs/entsoe.html#see-also",
    "title": "ENTSO-E Energy Forecasting Guide",
    "section": "",
    "text": "Tasks Overview - All available CLI commands\nAPI Reference - Detailed API documentation\nModel Persistence - Saving and loading models",
    "crumbs": [
      "Tasks Guide",
      "ENTSO-E Guide"
    ]
  },
  {
    "objectID": "docs/reference/utils.validation.html",
    "href": "docs/reference/utils.validation.html",
    "title": "utils.validation",
    "section": "",
    "text": "utils.validation\nValidation utilities for time series forecasting.\nThis module provides validation functions for time series data and exogenous variables.\n\n\n\n\n\nName\nDescription\n\n\n\n\ncheck_exog\nValidate that exog is a pandas Series or DataFrame.\n\n\ncheck_exog_dtypes\nCheck that exogenous variables have valid data types (int, float, category).\n\n\ncheck_interval\nValidate that a confidence interval specification is valid.\n\n\ncheck_y\nValidate that y is a pandas Series without missing values.\n\n\nget_exog_dtypes\nExtract and store the data types of exogenous variables.\n\n\n\n\n\nutils.validation.check_exog(exog, allow_nan=True, series_id='`exog`')\nValidate that exog is a pandas Series or DataFrame.\nThis function ensures that exogenous variables meet basic requirements: - Must be a pandas Series or DataFrame - If Series, must have a name - Optionally warns if NaN values are present\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexog\nUnion[pd.Series, pd.DataFrame]\nExogenous variable/s included as predictor/s.\nrequired\n\n\nallow_nan\nbool\nIf True, allows NaN values but issues a warning. If False, raises no warning about NaN values. Defaults to True.\nTrue\n\n\nseries_id\nstr\nIdentifier of the series used in error messages. Defaults to “exog”.\n'exog'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf exog is not a pandas Series or DataFrame.\n\n\n\nValueError\nIf exog is a Series without a name.\n\n\n\n\n\n\nIf allow_nan=True and exog contains NaN values.\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_exog\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid DataFrame\n&gt;&gt;&gt; exog_df = pd.DataFrame({\"temp\": [20, 21, 22], \"humidity\": [50, 55, 60]})\n&gt;&gt;&gt; check_exog(exog_df)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid Series with name\n&gt;&gt;&gt; exog_series = pd.Series([1, 2, 3], name=\"temperature\")\n&gt;&gt;&gt; check_exog(exog_series)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: Series without name\n&gt;&gt;&gt; exog_no_name = pd.Series([1, 2, 3])\n&gt;&gt;&gt; try:\n...     check_exog(exog_no_name)\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: When `exog` is a pandas Series, it must have a name.\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not a Series/DataFrame\n&gt;&gt;&gt; try:\n...     check_exog([1, 2, 3])\n... except TypeError as e:\n...     print(f\"Error: {e}\")\nError: `exog` must be a pandas Series or DataFrame. Got &lt;class 'list'&gt;.\n\n\n\n\nutils.validation.check_exog_dtypes(\n    exog,\n    call_check_exog=True,\n    series_id='`exog`',\n)\nCheck that exogenous variables have valid data types (int, float, category).\nThis function validates that the exogenous variables (Series or DataFrame) contain only supported data types: integer, float, or category. It issues a warning if other types (like object/string) are found, as these may cause issues with some machine learning estimators.\nIt also strictly enforces that categorical columns must have integer categories.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexog\nUnion[pd.Series, pd.DataFrame]\nExogenous variables to check.\nrequired\n\n\ncall_check_exog\nbool\nIf True, calls check_exog() first to ensure basic validity. Defaults to True.\nTrue\n\n\nseries_id\nstr\nIdentifier used in warning/error messages. Defaults to “exog”.\n'exog'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf categorical columns contain non-integer categories.\n\n\n\n\n\n\nIf columns with unsupported data types (not int, float, category) are found.\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_exog_dtypes\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid types (float, int)\n&gt;&gt;&gt; df_valid = pd.DataFrame({\n...     \"a\": [1.0, 2.0, 3.0],\n...     \"b\": [1, 2, 3]\n... })\n&gt;&gt;&gt; check_exog_dtypes(df_valid)  # No warning\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid type (object/string)\n&gt;&gt;&gt; df_invalid = pd.DataFrame({\n...     \"a\": [1, 2, 3],\n...     \"b\": [\"x\", \"y\", \"z\"]\n... })\n&gt;&gt;&gt; check_exog_dtypes(df_invalid)\n... # Issues DataTypeWarning about column 'b'\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid categorical (with integer categories)\n&gt;&gt;&gt; df_cat = pd.DataFrame({\"a\": [1, 2, 1]})\n&gt;&gt;&gt; df_cat[\"a\"] = df_cat[\"a\"].astype(\"category\")\n&gt;&gt;&gt; check_exog_dtypes(df_cat)  # No warning\n\n\n\n\nutils.validation.check_interval(\n    interval=None,\n    ensure_symmetric_intervals=False,\n    quantiles=None,\n    alpha=None,\n    alpha_literal='alpha',\n)\nValidate that a confidence interval specification is valid.\nThis function checks that interval values are properly formatted and within valid ranges for confidence interval prediction.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninterval\nUnion[List[float], Tuple[float], None]\nConfidence interval percentiles (0-100 inclusive). Should be [lower_bound, upper_bound]. Example: [2.5, 97.5] for 95% interval.\nNone\n\n\nensure_symmetric_intervals\nbool\nIf True, ensure intervals are symmetric (lower + upper = 100).\nFalse\n\n\nquantiles\nUnion[List[float], Tuple[float], None]\nSequence of quantiles (0-1 inclusive). Currently not validated, reserved for future use.\nNone\n\n\nalpha\nOptional[float]\nConfidence level (1-alpha). Currently not validated, reserved for future use.\nNone\n\n\nalpha_literal\nOptional[str]\nName used in error messages for alpha parameter.\n'alpha'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf interval is not a list or tuple.\n\n\n\nValueError\nIf interval doesn’t have exactly 2 values, values out of range (0-100), lower &gt;= upper, or intervals not symmetric when required.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_interval\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid 95% confidence interval\n&gt;&gt;&gt; check_interval(interval=[2.5, 97.5])  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid symmetric interval\n&gt;&gt;&gt; check_interval(interval=[2.5, 97.5], ensure_symmetric_intervals=True)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not symmetric\n&gt;&gt;&gt; try:\n...     check_interval(interval=[5, 90], ensure_symmetric_intervals=True)\n... except ValueError as e:\n...     print(\"Error: Interval not symmetric\")\nError: Interval not symmetric\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: wrong number of values\n&gt;&gt;&gt; try:\n...     check_interval(interval=[2.5, 50, 97.5])\n... except ValueError as e:\n...     print(\"Error: Must have exactly 2 values\")\nError: Must have exactly 2 values\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: out of range\n&gt;&gt;&gt; try:\n...     check_interval(interval=[-5, 105])\n... except ValueError as e:\n...     print(\"Error: Values out of range\")\nError: Values out of range\n\n\n\n\nutils.validation.check_y(y, series_id='`y`')\nValidate that y is a pandas Series without missing values.\nThis function ensures that the input time series meets the basic requirements for forecasting: it must be a pandas Series and must not contain any NaN values.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ny\nAny\nTime series values to validate.\nrequired\n\n\nseries_id\nstr\nIdentifier of the series used in error messages. Defaults to “y”.\n'y'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf y is not a pandas Series.\n\n\n\nValueError\nIf y contains missing (NaN) values.\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_y\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid series\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n&gt;&gt;&gt; check_y(y)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not a Series\n&gt;&gt;&gt; try:\n...     check_y([1, 2, 3])\n... except TypeError as e:\n...     print(f\"Error: {e}\")\nError: `y` must be a pandas Series with a DatetimeIndex or a RangeIndex. Found &lt;class 'list'&gt;.\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: contains NaN\n&gt;&gt;&gt; y_with_nan = pd.Series([1, 2, np.nan, 4])\n&gt;&gt;&gt; try:\n...     check_y(y_with_nan)\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: `y` has missing values.\n\n\n\n\nutils.validation.get_exog_dtypes(exog)\nExtract and store the data types of exogenous variables.\nThis function returns a dictionary mapping column names to their data types. For Series, uses the series name as the key. For DataFrames, uses all column names.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexog\nUnion[pd.Series, pd.DataFrame]\nExogenous variable/s (Series or DataFrame).\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDict[str, type]\nDictionary mapping variable names to their pandas dtypes.\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import get_exog_dtypes\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DataFrame with mixed types\n&gt;&gt;&gt; exog_df = pd.DataFrame({\n...     \"temp\": pd.Series([20.5, 21.3, 22.1], dtype='float64'),\n...     \"day\": pd.Series([1, 2, 3], dtype='int64'),\n...     \"is_weekend\": pd.Series([False, False, True], dtype='bool')\n... })\n&gt;&gt;&gt; dtypes = get_exog_dtypes(exog_df)\n&gt;&gt;&gt; dtypes['temp']\ndtype('float64')\n&gt;&gt;&gt; dtypes['day']\ndtype('int64')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series\n&gt;&gt;&gt; exog_series = pd.Series([1.0, 2.0, 3.0], name=\"temperature\", dtype='float64')\n&gt;&gt;&gt; dtypes = get_exog_dtypes(exog_series)\n&gt;&gt;&gt; dtypes\n{'temperature': dtype('float64')}",
    "crumbs": [
      "API Reference",
      "Utils",
      "validation"
    ]
  },
  {
    "objectID": "docs/reference/utils.validation.html#functions",
    "href": "docs/reference/utils.validation.html#functions",
    "title": "utils.validation",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncheck_exog\nValidate that exog is a pandas Series or DataFrame.\n\n\ncheck_exog_dtypes\nCheck that exogenous variables have valid data types (int, float, category).\n\n\ncheck_interval\nValidate that a confidence interval specification is valid.\n\n\ncheck_y\nValidate that y is a pandas Series without missing values.\n\n\nget_exog_dtypes\nExtract and store the data types of exogenous variables.\n\n\n\n\n\nutils.validation.check_exog(exog, allow_nan=True, series_id='`exog`')\nValidate that exog is a pandas Series or DataFrame.\nThis function ensures that exogenous variables meet basic requirements: - Must be a pandas Series or DataFrame - If Series, must have a name - Optionally warns if NaN values are present\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexog\nUnion[pd.Series, pd.DataFrame]\nExogenous variable/s included as predictor/s.\nrequired\n\n\nallow_nan\nbool\nIf True, allows NaN values but issues a warning. If False, raises no warning about NaN values. Defaults to True.\nTrue\n\n\nseries_id\nstr\nIdentifier of the series used in error messages. Defaults to “exog”.\n'exog'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf exog is not a pandas Series or DataFrame.\n\n\n\nValueError\nIf exog is a Series without a name.\n\n\n\n\n\n\nIf allow_nan=True and exog contains NaN values.\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_exog\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid DataFrame\n&gt;&gt;&gt; exog_df = pd.DataFrame({\"temp\": [20, 21, 22], \"humidity\": [50, 55, 60]})\n&gt;&gt;&gt; check_exog(exog_df)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid Series with name\n&gt;&gt;&gt; exog_series = pd.Series([1, 2, 3], name=\"temperature\")\n&gt;&gt;&gt; check_exog(exog_series)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: Series without name\n&gt;&gt;&gt; exog_no_name = pd.Series([1, 2, 3])\n&gt;&gt;&gt; try:\n...     check_exog(exog_no_name)\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: When `exog` is a pandas Series, it must have a name.\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not a Series/DataFrame\n&gt;&gt;&gt; try:\n...     check_exog([1, 2, 3])\n... except TypeError as e:\n...     print(f\"Error: {e}\")\nError: `exog` must be a pandas Series or DataFrame. Got &lt;class 'list'&gt;.\n\n\n\n\nutils.validation.check_exog_dtypes(\n    exog,\n    call_check_exog=True,\n    series_id='`exog`',\n)\nCheck that exogenous variables have valid data types (int, float, category).\nThis function validates that the exogenous variables (Series or DataFrame) contain only supported data types: integer, float, or category. It issues a warning if other types (like object/string) are found, as these may cause issues with some machine learning estimators.\nIt also strictly enforces that categorical columns must have integer categories.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexog\nUnion[pd.Series, pd.DataFrame]\nExogenous variables to check.\nrequired\n\n\ncall_check_exog\nbool\nIf True, calls check_exog() first to ensure basic validity. Defaults to True.\nTrue\n\n\nseries_id\nstr\nIdentifier used in warning/error messages. Defaults to “exog”.\n'exog'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf categorical columns contain non-integer categories.\n\n\n\n\n\n\nIf columns with unsupported data types (not int, float, category) are found.\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_exog_dtypes\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid types (float, int)\n&gt;&gt;&gt; df_valid = pd.DataFrame({\n...     \"a\": [1.0, 2.0, 3.0],\n...     \"b\": [1, 2, 3]\n... })\n&gt;&gt;&gt; check_exog_dtypes(df_valid)  # No warning\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid type (object/string)\n&gt;&gt;&gt; df_invalid = pd.DataFrame({\n...     \"a\": [1, 2, 3],\n...     \"b\": [\"x\", \"y\", \"z\"]\n... })\n&gt;&gt;&gt; check_exog_dtypes(df_invalid)\n... # Issues DataTypeWarning about column 'b'\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid categorical (with integer categories)\n&gt;&gt;&gt; df_cat = pd.DataFrame({\"a\": [1, 2, 1]})\n&gt;&gt;&gt; df_cat[\"a\"] = df_cat[\"a\"].astype(\"category\")\n&gt;&gt;&gt; check_exog_dtypes(df_cat)  # No warning\n\n\n\n\nutils.validation.check_interval(\n    interval=None,\n    ensure_symmetric_intervals=False,\n    quantiles=None,\n    alpha=None,\n    alpha_literal='alpha',\n)\nValidate that a confidence interval specification is valid.\nThis function checks that interval values are properly formatted and within valid ranges for confidence interval prediction.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninterval\nUnion[List[float], Tuple[float], None]\nConfidence interval percentiles (0-100 inclusive). Should be [lower_bound, upper_bound]. Example: [2.5, 97.5] for 95% interval.\nNone\n\n\nensure_symmetric_intervals\nbool\nIf True, ensure intervals are symmetric (lower + upper = 100).\nFalse\n\n\nquantiles\nUnion[List[float], Tuple[float], None]\nSequence of quantiles (0-1 inclusive). Currently not validated, reserved for future use.\nNone\n\n\nalpha\nOptional[float]\nConfidence level (1-alpha). Currently not validated, reserved for future use.\nNone\n\n\nalpha_literal\nOptional[str]\nName used in error messages for alpha parameter.\n'alpha'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf interval is not a list or tuple.\n\n\n\nValueError\nIf interval doesn’t have exactly 2 values, values out of range (0-100), lower &gt;= upper, or intervals not symmetric when required.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_interval\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid 95% confidence interval\n&gt;&gt;&gt; check_interval(interval=[2.5, 97.5])  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid symmetric interval\n&gt;&gt;&gt; check_interval(interval=[2.5, 97.5], ensure_symmetric_intervals=True)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not symmetric\n&gt;&gt;&gt; try:\n...     check_interval(interval=[5, 90], ensure_symmetric_intervals=True)\n... except ValueError as e:\n...     print(\"Error: Interval not symmetric\")\nError: Interval not symmetric\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: wrong number of values\n&gt;&gt;&gt; try:\n...     check_interval(interval=[2.5, 50, 97.5])\n... except ValueError as e:\n...     print(\"Error: Must have exactly 2 values\")\nError: Must have exactly 2 values\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: out of range\n&gt;&gt;&gt; try:\n...     check_interval(interval=[-5, 105])\n... except ValueError as e:\n...     print(\"Error: Values out of range\")\nError: Values out of range\n\n\n\n\nutils.validation.check_y(y, series_id='`y`')\nValidate that y is a pandas Series without missing values.\nThis function ensures that the input time series meets the basic requirements for forecasting: it must be a pandas Series and must not contain any NaN values.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ny\nAny\nTime series values to validate.\nrequired\n\n\nseries_id\nstr\nIdentifier of the series used in error messages. Defaults to “y”.\n'y'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf y is not a pandas Series.\n\n\n\nValueError\nIf y contains missing (NaN) values.\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_y\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid series\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n&gt;&gt;&gt; check_y(y)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not a Series\n&gt;&gt;&gt; try:\n...     check_y([1, 2, 3])\n... except TypeError as e:\n...     print(f\"Error: {e}\")\nError: `y` must be a pandas Series with a DatetimeIndex or a RangeIndex. Found &lt;class 'list'&gt;.\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: contains NaN\n&gt;&gt;&gt; y_with_nan = pd.Series([1, 2, np.nan, 4])\n&gt;&gt;&gt; try:\n...     check_y(y_with_nan)\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: `y` has missing values.\n\n\n\n\nutils.validation.get_exog_dtypes(exog)\nExtract and store the data types of exogenous variables.\nThis function returns a dictionary mapping column names to their data types. For Series, uses the series name as the key. For DataFrames, uses all column names.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexog\nUnion[pd.Series, pd.DataFrame]\nExogenous variable/s (Series or DataFrame).\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDict[str, type]\nDictionary mapping variable names to their pandas dtypes.\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import get_exog_dtypes\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DataFrame with mixed types\n&gt;&gt;&gt; exog_df = pd.DataFrame({\n...     \"temp\": pd.Series([20.5, 21.3, 22.1], dtype='float64'),\n...     \"day\": pd.Series([1, 2, 3], dtype='int64'),\n...     \"is_weekend\": pd.Series([False, False, True], dtype='bool')\n... })\n&gt;&gt;&gt; dtypes = get_exog_dtypes(exog_df)\n&gt;&gt;&gt; dtypes['temp']\ndtype('float64')\n&gt;&gt;&gt; dtypes['day']\ndtype('int64')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series\n&gt;&gt;&gt; exog_series = pd.Series([1.0, 2.0, 3.0], name=\"temperature\", dtype='float64')\n&gt;&gt;&gt; dtypes = get_exog_dtypes(exog_series)\n&gt;&gt;&gt; dtypes\n{'temperature': dtype('float64')}",
    "crumbs": [
      "API Reference",
      "Utils",
      "validation"
    ]
  },
  {
    "objectID": "docs/reference/utils.forecaster_config.html",
    "href": "docs/reference/utils.forecaster_config.html",
    "title": "utils.forecaster_config",
    "section": "",
    "text": "utils.forecaster_config\nForecaster configuration utilities.\nThis module provides functions for initializing and validating forecaster configuration parameters like lags and weights.\n\n\n\n\n\nName\nDescription\n\n\n\n\ncheck_select_fit_kwargs\nCheck if fit_kwargs is a dict and select only keys used by estimator’s fit.\n\n\ninitialize_lags\nValidate and normalize lag specification for forecasting.\n\n\ninitialize_weights\nValidate and initialize weight function configuration for forecasting.\n\n\n\n\n\nutils.forecaster_config.check_select_fit_kwargs(estimator, fit_kwargs=None)\nCheck if fit_kwargs is a dict and select only keys used by estimator’s fit.\nThis function validates that fit_kwargs is a dictionary, warns about unused arguments, removes ‘sample_weight’ (which should be handled via weight_func), and returns a dictionary containing only the arguments accepted by the estimator’s fit method.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nestimator\nAny\nScikit-learn compatible estimator.\nrequired\n\n\nfit_kwargs\nOptional[dict]\nDictionary of arguments to pass to the estimator’s fit method.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict\nDictionary with only the arguments accepted by the estimator’s fit method.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf fit_kwargs is not a dict.\n\n\n\n\n\n\nIf fit_kwargs contains keys not used by fit method, or if ‘sample_weight’ is present (it gets removed).\n\n\n\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; from spotforecast2.utils.forecaster_config import check_select_fit_kwargs\n&gt;&gt;&gt;\n&gt;&gt;&gt; estimator = Ridge()\n&gt;&gt;&gt; # Valid argument for Ridge.fit\n&gt;&gt;&gt; kwargs = {\"sample_weight\": [1, 1], \"invalid_arg\": 10}\n&gt;&gt;&gt; # sample_weight is removed (should be passed via weight_func in forecaster)\n&gt;&gt;&gt; # invalid_arg is ignored\n&gt;&gt;&gt; filtered = check_select_fit_kwargs(estimator, kwargs)\n&gt;&gt;&gt; filtered\n{}\n\n\n\n\nutils.forecaster_config.initialize_lags(forecaster_name, lags)\nValidate and normalize lag specification for forecasting.\nThis function converts various lag specifications (int, list, tuple, range, ndarray) into a standardized format: sorted numpy array, lag names, and maximum lag value.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nforecaster_name\nstr\nName of the forecaster class for error messages.\nrequired\n\n\nlags\nAny\nLag specification in one of several formats: - int: Creates lags from 1 to lags (e.g., 5 → [1,2,3,4,5]) - list/tuple/range: Converted to numpy array - numpy.ndarray: Validated and used directly - None: Returns (None, None, None)\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nOptional[np.ndarray]\nTuple containing:\n\n\n\nOptional[List[str]]\n- lags: Sorted numpy array of lag values (or None)\n\n\n\nOptional[int]\n- lags_names: List of lag names like [‘lag_1’, ‘lag_2’, …] (or None)\n\n\n\nTuple[Optional[np.ndarray], Optional[List[str]], Optional[int]]\n- max_lag: Maximum lag value (or None)\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf lags &lt; 1, empty array, or not 1-dimensional.\n\n\n\nTypeError\nIf lags is not an integer, not in the right format for the forecaster, or array contains non-integer values.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_lags\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Integer input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", 3)\n&gt;&gt;&gt; lags\narray([1, 2, 3])\n&gt;&gt;&gt; names\n['lag_1', 'lag_2', 'lag_3']\n&gt;&gt;&gt; max_lag\n3\n&gt;&gt;&gt;\n&gt;&gt;&gt; # List input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", [1, 3, 5])\n&gt;&gt;&gt; lags\narray([1, 3, 5])\n&gt;&gt;&gt; names\n['lag_1', 'lag_3', 'lag_5']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Range input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", range(1, 4))\n&gt;&gt;&gt; lags\narray([1, 2, 3])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # None input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", None)\n&gt;&gt;&gt; lags is None\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: lags &lt; 1\n&gt;&gt;&gt; try:\n...     initialize_lags(\"ForecasterRecursive\", 0)\n... except ValueError as e:\n...     print(\"Error: Minimum value of lags allowed is 1\")\nError: Minimum value of lags allowed is 1\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: negative lags\n&gt;&gt;&gt; try:\n...     initialize_lags(\"ForecasterRecursive\", [1, -2, 3])\n... except ValueError as e:\n...     print(\"Error: Minimum value of lags allowed is 1\")\nError: Minimum value of lags allowed is 1\n\n\n\n\nutils.forecaster_config.initialize_weights(\n    forecaster_name,\n    estimator,\n    weight_func,\n    series_weights,\n)\nValidate and initialize weight function configuration for forecasting.\nThis function validates weight_func and series_weights, extracts source code from weight functions for serialization, and checks if the estimator supports sample weights in its fit method.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nforecaster_name\nstr\nName of the forecaster class.\nrequired\n\n\nestimator\nAny\nScikit-learn compatible estimator or pipeline.\nrequired\n\n\nweight_func\nAny\nWeight function specification: - Callable: Single weight function - dict: Dictionary of weight functions (for MultiSeries forecasters) - None: No weighting\nrequired\n\n\nseries_weights\nAny\nDictionary of series-level weights (for MultiSeries forecasters). - dict: Maps series names to weight values - None: No series weighting\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAny\nTuple containing:\n\n\n\nOptional[Union[str, dict]]\n- weight_func: Validated weight function (or None if invalid)\n\n\n\nAny\n- source_code_weight_func: Source code of weight function(s) for serialization (or None)\n\n\n\nTuple[Any, Optional[Union[str, dict]], Any]\n- series_weights: Validated series weights (or None if invalid)\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf weight_func is not Callable/dict (depending on forecaster type), or if series_weights is not a dict.\n\n\n\n\n\n\nIf estimator doesn’t support sample_weight.\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_weights\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Simple weight function\n&gt;&gt;&gt; def custom_weights(index):\n...     return np.ones(len(index))\n&gt;&gt;&gt;\n&gt;&gt;&gt; estimator = Ridge()\n&gt;&gt;&gt; wf, source, sw = initialize_weights(\n...     \"ForecasterRecursive\", estimator, custom_weights, None\n... )\n&gt;&gt;&gt; wf is not None\nTrue\n&gt;&gt;&gt; isinstance(source, str)\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # No weight function\n&gt;&gt;&gt; wf, source, sw = initialize_weights(\n...     \"ForecasterRecursive\", estimator, None, None\n... )\n&gt;&gt;&gt; wf is None\nTrue\n&gt;&gt;&gt; source is None\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid type for non-MultiSeries forecaster\n&gt;&gt;&gt; try:\n...     initialize_weights(\"ForecasterRecursive\", estimator, \"invalid\", None)\n... except TypeError as e:\n...     print(\"Error: weight_func must be Callable\")\nError: weight_func must be Callable",
    "crumbs": [
      "API Reference",
      "Utils",
      "forecaster_config"
    ]
  },
  {
    "objectID": "docs/reference/utils.forecaster_config.html#functions",
    "href": "docs/reference/utils.forecaster_config.html#functions",
    "title": "utils.forecaster_config",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncheck_select_fit_kwargs\nCheck if fit_kwargs is a dict and select only keys used by estimator’s fit.\n\n\ninitialize_lags\nValidate and normalize lag specification for forecasting.\n\n\ninitialize_weights\nValidate and initialize weight function configuration for forecasting.\n\n\n\n\n\nutils.forecaster_config.check_select_fit_kwargs(estimator, fit_kwargs=None)\nCheck if fit_kwargs is a dict and select only keys used by estimator’s fit.\nThis function validates that fit_kwargs is a dictionary, warns about unused arguments, removes ‘sample_weight’ (which should be handled via weight_func), and returns a dictionary containing only the arguments accepted by the estimator’s fit method.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nestimator\nAny\nScikit-learn compatible estimator.\nrequired\n\n\nfit_kwargs\nOptional[dict]\nDictionary of arguments to pass to the estimator’s fit method.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict\nDictionary with only the arguments accepted by the estimator’s fit method.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf fit_kwargs is not a dict.\n\n\n\n\n\n\nIf fit_kwargs contains keys not used by fit method, or if ‘sample_weight’ is present (it gets removed).\n\n\n\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; from spotforecast2.utils.forecaster_config import check_select_fit_kwargs\n&gt;&gt;&gt;\n&gt;&gt;&gt; estimator = Ridge()\n&gt;&gt;&gt; # Valid argument for Ridge.fit\n&gt;&gt;&gt; kwargs = {\"sample_weight\": [1, 1], \"invalid_arg\": 10}\n&gt;&gt;&gt; # sample_weight is removed (should be passed via weight_func in forecaster)\n&gt;&gt;&gt; # invalid_arg is ignored\n&gt;&gt;&gt; filtered = check_select_fit_kwargs(estimator, kwargs)\n&gt;&gt;&gt; filtered\n{}\n\n\n\n\nutils.forecaster_config.initialize_lags(forecaster_name, lags)\nValidate and normalize lag specification for forecasting.\nThis function converts various lag specifications (int, list, tuple, range, ndarray) into a standardized format: sorted numpy array, lag names, and maximum lag value.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nforecaster_name\nstr\nName of the forecaster class for error messages.\nrequired\n\n\nlags\nAny\nLag specification in one of several formats: - int: Creates lags from 1 to lags (e.g., 5 → [1,2,3,4,5]) - list/tuple/range: Converted to numpy array - numpy.ndarray: Validated and used directly - None: Returns (None, None, None)\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nOptional[np.ndarray]\nTuple containing:\n\n\n\nOptional[List[str]]\n- lags: Sorted numpy array of lag values (or None)\n\n\n\nOptional[int]\n- lags_names: List of lag names like [‘lag_1’, ‘lag_2’, …] (or None)\n\n\n\nTuple[Optional[np.ndarray], Optional[List[str]], Optional[int]]\n- max_lag: Maximum lag value (or None)\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf lags &lt; 1, empty array, or not 1-dimensional.\n\n\n\nTypeError\nIf lags is not an integer, not in the right format for the forecaster, or array contains non-integer values.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_lags\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Integer input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", 3)\n&gt;&gt;&gt; lags\narray([1, 2, 3])\n&gt;&gt;&gt; names\n['lag_1', 'lag_2', 'lag_3']\n&gt;&gt;&gt; max_lag\n3\n&gt;&gt;&gt;\n&gt;&gt;&gt; # List input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", [1, 3, 5])\n&gt;&gt;&gt; lags\narray([1, 3, 5])\n&gt;&gt;&gt; names\n['lag_1', 'lag_3', 'lag_5']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Range input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", range(1, 4))\n&gt;&gt;&gt; lags\narray([1, 2, 3])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # None input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", None)\n&gt;&gt;&gt; lags is None\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: lags &lt; 1\n&gt;&gt;&gt; try:\n...     initialize_lags(\"ForecasterRecursive\", 0)\n... except ValueError as e:\n...     print(\"Error: Minimum value of lags allowed is 1\")\nError: Minimum value of lags allowed is 1\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: negative lags\n&gt;&gt;&gt; try:\n...     initialize_lags(\"ForecasterRecursive\", [1, -2, 3])\n... except ValueError as e:\n...     print(\"Error: Minimum value of lags allowed is 1\")\nError: Minimum value of lags allowed is 1\n\n\n\n\nutils.forecaster_config.initialize_weights(\n    forecaster_name,\n    estimator,\n    weight_func,\n    series_weights,\n)\nValidate and initialize weight function configuration for forecasting.\nThis function validates weight_func and series_weights, extracts source code from weight functions for serialization, and checks if the estimator supports sample weights in its fit method.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nforecaster_name\nstr\nName of the forecaster class.\nrequired\n\n\nestimator\nAny\nScikit-learn compatible estimator or pipeline.\nrequired\n\n\nweight_func\nAny\nWeight function specification: - Callable: Single weight function - dict: Dictionary of weight functions (for MultiSeries forecasters) - None: No weighting\nrequired\n\n\nseries_weights\nAny\nDictionary of series-level weights (for MultiSeries forecasters). - dict: Maps series names to weight values - None: No series weighting\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAny\nTuple containing:\n\n\n\nOptional[Union[str, dict]]\n- weight_func: Validated weight function (or None if invalid)\n\n\n\nAny\n- source_code_weight_func: Source code of weight function(s) for serialization (or None)\n\n\n\nTuple[Any, Optional[Union[str, dict]], Any]\n- series_weights: Validated series weights (or None if invalid)\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf weight_func is not Callable/dict (depending on forecaster type), or if series_weights is not a dict.\n\n\n\n\n\n\nIf estimator doesn’t support sample_weight.\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_weights\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Simple weight function\n&gt;&gt;&gt; def custom_weights(index):\n...     return np.ones(len(index))\n&gt;&gt;&gt;\n&gt;&gt;&gt; estimator = Ridge()\n&gt;&gt;&gt; wf, source, sw = initialize_weights(\n...     \"ForecasterRecursive\", estimator, custom_weights, None\n... )\n&gt;&gt;&gt; wf is not None\nTrue\n&gt;&gt;&gt; isinstance(source, str)\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # No weight function\n&gt;&gt;&gt; wf, source, sw = initialize_weights(\n...     \"ForecasterRecursive\", estimator, None, None\n... )\n&gt;&gt;&gt; wf is None\nTrue\n&gt;&gt;&gt; source is None\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid type for non-MultiSeries forecaster\n&gt;&gt;&gt; try:\n...     initialize_weights(\"ForecasterRecursive\", estimator, \"invalid\", None)\n... except TypeError as e:\n...     print(\"Error: weight_func must be Callable\")\nError: weight_func must be Callable",
    "crumbs": [
      "API Reference",
      "Utils",
      "forecaster_config"
    ]
  },
  {
    "objectID": "docs/reference/tasks.task_n_to_1_with_covariates_and_dataframe.html",
    "href": "docs/reference/tasks.task_n_to_1_with_covariates_and_dataframe.html",
    "title": "tasks.task_n_to_1_with_covariates_and_dataframe",
    "section": "",
    "text": "tasks.task_n_to_1_with_covariates_and_dataframe\nN-to-1 Forecasting with Exogenous Covariates and Prediction Aggregation.\nThis module implements a complete end-to-end pipeline for multi-step time series forecasting with exogenous variables (weather, holidays, calendar features), followed by prediction aggregation using configurable weights.\n\n\n\nPerforms multi-output recursive forecasting with exogenous covariates\nAggregates predictions using weighted combinations\nSupports flexible model selection (string or object-based)\nAllows customization via kwargs for all underlying functions\n\n\n\n\n\nAutomatic weather, holiday, and calendar feature generation\nCyclical and polynomial feature engineering\nConfigurable recursive forecaster with LGBMRegressor default\nWeighted prediction aggregation\nComprehensive parameter flexibility via **kwargs\nDetailed logging and progress tracking\n\n\n\n\nBasic usage with default parameters:\n&gt;&gt;&gt; from spotforecast2.scripts.n_to_1_with_covariates import main\n&gt;&gt;&gt; main()\nWith custom forecast horizon and weights:\n&gt;&gt;&gt; predictions = main(\n...     forecast_horizon=48,\n...     weights=[1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0, 1.0]\n... )\nWith custom location (latitude, longitude):\n&gt;&gt;&gt; predictions = main(\n...     forecast_horizon=24,\n...     latitude=48.1351,\n...     longitude=11.5820,\n...     verbose=True\n... )\nWith feature engineering options:\n&gt;&gt;&gt; predictions = main(\n...     forecast_horizon=24,\n...     include_weather_windows=True,\n...     include_holiday_features=True,\n...     include_poly_features=True,\n... )\nPassing custom estimator object:\n&gt;&gt;&gt; from lightgbm import LGBMRegressor\n&gt;&gt;&gt; estimator = LGBMRegressor(n_estimators=200, learning_rate=0.01)\n&gt;&gt;&gt; predictions = main(forecast_horizon=24, estimator=estimator)\nAvailable Parameters:\n\n\n\nforecast_horizon (int): Number of steps ahead to forecast. Default: 24. contamination (float): Outlier detection threshold [0, 1]. Default: 0.01. window_size (int): Rolling window size for feature engineering. Default: 72. lags (int): Number of lag features to create. Default: 24. train_ratio (float): Train-test split ratio [0, 1]. Default: 0.8. verbose (bool): Enable detailed progress logging. Default: True.\nLocation & Time Parameters: latitude (float): Location latitude for sun features. Default: 51.5136 (Dortmund). longitude (float): Location longitude for sun features. Default: 7.4653 (Dortmund). timezone (str): Timezone for data processing. Default: “UTC”. country_code (str): Country code for holidays (ISO 3166-1 alpha-2). Default: “DE”. state (str): State/region code for holidays (depends on country). Default: “NW”.\n\n\n\ninclude_weather_windows (bool): Include rolling weather statistics. Default: False. include_holiday_features (bool): Include holiday indicator features. Default: False. include_poly_features (bool): Include polynomial interaction features. Default: False.\n\n\n\nestimator (Optional[Union[str, object]]): Forecaster estimator. Can be: - None: Uses default LGBMRegressor(n_estimators=100) - “ForecasterRecursive”: String reference (uses default) - LGBMRegressor(…): Custom estimator object Default: None.\n\n\n\nweights (Optional[Union[Dict[str, float], List[float], np.ndarray]]): Weights for prediction aggregation. Can be: - None: Defaults to uniform weights (1.0 for each column) - Dict: Column name -&gt; weight mapping - List/Array: Weights in column order Default: [1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0, 1.0].\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nmain\nExecute the complete N-to-1 forecasting pipeline with default parameters.\n\n\nn_to_1_with_covariates\nExecute N-to-1 forecasting pipeline with exogenous covariates.\n\n\n\n\n\ntasks.task_n_to_1_with_covariates_and_dataframe.main()\nExecute the complete N-to-1 forecasting pipeline with default parameters.\nThis is the entry point when running the script directly. It executes the full forecasting pipeline with default settings and prints comprehensive results.\nThe default configuration: - Forecasts 24 steps ahead - Uses Dortmund, Germany coordinates - Applies default contamination and window parameters - Aggregates with predefined weights - Provides verbose output\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone. Results are printed to stdout.\n\n\n\n\n\n\nRun the script directly:\n&gt;&gt;&gt; python n_to_1_with_covariates.py\nOr call main() programmatically:\n&gt;&gt;&gt; from spotforecast2.scripts.n_to_1_with_covariates import main\n&gt;&gt;&gt; main()\n\n\n\n\ntasks.task_n_to_1_with_covariates_and_dataframe.n_to_1_with_covariates(\n    data=None,\n    forecast_horizon=24,\n    contamination=0.01,\n    window_size=72,\n    lags=24,\n    train_ratio=0.8,\n    latitude=51.5136,\n    longitude=7.4653,\n    timezone='UTC',\n    country_code='DE',\n    state='NW',\n    estimator=None,\n    include_weather_windows=False,\n    include_holiday_features=False,\n    include_poly_features=False,\n    weights=None,\n    verbose=True,\n    show_progress=True,\n    **kwargs,\n)\nExecute N-to-1 forecasting pipeline with exogenous covariates.\nThis function performs a complete time series forecasting workflow: 1. Fetches and preprocesses data 2. Engineers features (calendar, weather, holidays, cyclical, polynomial) 3. Trains recursive forecaster on multiple targets 4. Aggregates predictions using weighted combination\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\nOptional[pd.DataFrame]\nOptional DataFrame with target time series data. If None, fetches data automatically. Default: None.\nNone\n\n\nforecast_horizon\nint\nNumber of forecast steps ahead. Determines how many time steps to predict into the future. Typical values: 24 (1 day), 48 (2 days), 168 (1 week). Default: 24.\n24\n\n\ncontamination\nfloat\nOutlier contamination level for anomaly detection. Expected proportion of outliers in the training data [0, 1]. Higher values detect fewer outliers. Default: 0.01 (1%).\n0.01\n\n\nwindow_size\nint\nRolling window size for feature engineering (hours). Size of the rolling window for computing statistics. Must be &gt; lags. Typical range: 24-168. Default: 72.\n72\n\n\nlags\nint\nNumber of lagged features to create. Creates AR(p) features with p=lags. Typical values: 12, 24, 48. Default: 24.\n24\n\n\ntrain_ratio\nfloat\nProportion of data for training [0, 1]. Remaining data (1 - train_ratio) used for validation/testing. Typical values: 0.7-0.9. Default: 0.8.\n0.8\n\n\nlatitude\nfloat\nGeographic latitude for solar features. Used to compute sunrise/sunset times for day/night features. Default: 51.5136 (Dortmund, Germany).\n51.5136\n\n\nlongitude\nfloat\nGeographic longitude for solar features. Used to compute sunrise/sunset times for day/night features. Default: 7.4653 (Dortmund, Germany).\n7.4653\n\n\ntimezone\nstr\nTimezone for time-based features. Any timezone recognized by pytz. Default: “UTC”.\n'UTC'\n\n\ncountry_code\nstr\nISO 3166-1 alpha-2 country code for holidays. Examples: “DE” (Germany), “US” (USA), “GB” (UK). Default: “DE”.\n'DE'\n\n\nstate\nstr\nState/region code for holidays. Country-dependent. For Germany: “BW”, “BY”, “NW”, etc. Default: “NW” (Nordrhein-Westfalen).\n'NW'\n\n\nestimator\nOptional[Union[str, object]]\nForecaster model. Can be: - None: Uses LGBMRegressor(n_estimators=100, verbose=-1). - “ForecasterRecursive”: References default estimator (same as None). - LGBMRegressor(…): Custom pre-configured estimator. - Any sklearn-compatible regressor. Default: None.\nNone\n\n\ninclude_weather_windows\nbool\nAdd rolling weather statistics. Creates moving averages, min, max of weather features over multiple windows (1D, 7D). Increases feature count significantly. Default: False.\nFalse\n\n\ninclude_holiday_features\nbool\nAdd holiday binary indicators. Creates features indicating holidays and special dates. Useful for capturing demand patterns around holidays. Default: False.\nFalse\n\n\ninclude_poly_features\nbool\nAdd polynomial interactions. Creates 2nd-order interaction terms between selected features. Useful for capturing non-linear relationships. Default: False.\nFalse\n\n\nweights\nOptional[Union[Dict[str, float], List[float], np.ndarray]]\nWeights for combining multi-output predictions. Can be: - None: Default weights [1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0, 1.0] - Dict: {“col_name”: weight, …} for specific columns - List: [w1, w2, …] in column order - np.ndarray: Same as list Default: None (uses default weights).\nNone\n\n\nverbose\nbool\nEnable progress logging. Prints intermediate results and timestamps. Default: True.\nTrue\n\n\nshow_progress\nbool\nShow a progress bar for major pipeline steps. Default: True.\nTrue\n\n\n**kwargs\nAny\nAdditional parameters for underlying functions. These are passed to n2n_predict_with_covariates(). Examples: - freq: Frequency for data resampling. Default: “h” (hourly). - columns: Specific columns to forecast. Default: None (all). Any parameter accepted by n2n_predict_with_covariates().\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTuple[pd.DataFrame, pd.Series, Dict, Dict]\nTuple[pd.DataFrame, pd.Series, Dict, Dict]: A tuple containing: - predictions (pd.DataFrame): Multi-output forecasts from recursive model. Each column represents a target variable. Index is datetime matching the forecast period. - combined_prediction (pd.Series): Aggregated forecast from weighted combination. Single column combining all output predictions. Index is datetime matching the forecast period. - model_metrics (Dict): Performance metrics from recursive forecaster. Keys may include: ‘mae’, ‘rmse’, ‘mape’, etc. - feature_info (Dict): Information about engineered features. Contains feature counts, types, and engineering details.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf forecast_horizon &lt;= 0 or invalid parameter combinations.\n\n\n\nFileNotFoundError\nIf data source files cannot be accessed.\n\n\n\nRuntimeError\nIf model training fails or data processing errors occur.\n\n\n\n\n\n\nBasic usage (uses all defaults):\n&gt;&gt;&gt; predictions, combined, metrics, features = n_to_1_with_covariates()\n&gt;&gt;&gt; print(f\"Predictions shape: {predictions.shape}\")\n&gt;&gt;&gt; print(f\"Combined forecast head:\\n{combined.head()}\")\nCustom location and forecast horizon:\n&gt;&gt;&gt; predictions, combined, metrics, features = n_to_1_with_covariates(\n...     forecast_horizon=48,\n...     latitude=48.1351,\n...     longitude=11.5820,\n...     country_code=\"DE\",\n...     state=\"BY\",\n...     verbose=True\n... )\nWith feature engineering enabled:\n&gt;&gt;&gt; predictions, combined, metrics, features = n_to_1_with_covariates(\n...     forecast_horizon=24,\n...     include_weather_windows=True,\n...     include_holiday_features=True,\n...     include_poly_features=True,\n...     verbose=True\n... )\nCustom estimator and weights:\n&gt;&gt;&gt; from lightgbm import LGBMRegressor\n&gt;&gt;&gt; custom_estimator = LGBMRegressor(\n...     n_estimators=200,\n...     learning_rate=0.01,\n...     max_depth=7\n... )\n&gt;&gt;&gt; custom_weights = [1.0, 1.0, -0.5, -0.5]\n&gt;&gt;&gt; predictions, combined, metrics, features = n_to_1_with_covariates(\n...     forecast_horizon=24,\n...     estimator=custom_estimator,\n...     weights=custom_weights,\n...     verbose=True\n... )\nWith all advanced options:\n&gt;&gt;&gt; predictions, combined, metrics, features = n_to_1_with_covariates(\n...     forecast_horizon=72,\n...     contamination=0.02,\n...     window_size=168,\n...     lags=48,\n...     train_ratio=0.75,\n...     latitude=50.1109,\n...     longitude=8.6821,\n...     timezone=\"Europe/Berlin\",\n...     country_code=\"DE\",\n...     state=\"HE\",\n...     include_weather_windows=True,\n...     include_holiday_features=True,\n...     include_poly_features=True,\n...     weights={\"power\": 1.0, \"demand\": 0.8},\n...     verbose=True,\n...     freq=\"h\",\n... )\n&gt;&gt;&gt; print(f\"Model Metrics: {metrics}\")\n&gt;&gt;&gt; print(f\"Feature Info: {features}\")",
    "crumbs": [
      "API Reference",
      "Tasks",
      "task_n_to_1_with_covariates_and_dataframe"
    ]
  },
  {
    "objectID": "docs/reference/tasks.task_n_to_1_with_covariates_and_dataframe.html#the-pipeline",
    "href": "docs/reference/tasks.task_n_to_1_with_covariates_and_dataframe.html#the-pipeline",
    "title": "tasks.task_n_to_1_with_covariates_and_dataframe",
    "section": "",
    "text": "Performs multi-output recursive forecasting with exogenous covariates\nAggregates predictions using weighted combinations\nSupports flexible model selection (string or object-based)\nAllows customization via kwargs for all underlying functions",
    "crumbs": [
      "API Reference",
      "Tasks",
      "task_n_to_1_with_covariates_and_dataframe"
    ]
  },
  {
    "objectID": "docs/reference/tasks.task_n_to_1_with_covariates_and_dataframe.html#key-features",
    "href": "docs/reference/tasks.task_n_to_1_with_covariates_and_dataframe.html#key-features",
    "title": "tasks.task_n_to_1_with_covariates_and_dataframe",
    "section": "",
    "text": "Automatic weather, holiday, and calendar feature generation\nCyclical and polynomial feature engineering\nConfigurable recursive forecaster with LGBMRegressor default\nWeighted prediction aggregation\nComprehensive parameter flexibility via **kwargs\nDetailed logging and progress tracking",
    "crumbs": [
      "API Reference",
      "Tasks",
      "task_n_to_1_with_covariates_and_dataframe"
    ]
  },
  {
    "objectID": "docs/reference/tasks.task_n_to_1_with_covariates_and_dataframe.html#examples",
    "href": "docs/reference/tasks.task_n_to_1_with_covariates_and_dataframe.html#examples",
    "title": "tasks.task_n_to_1_with_covariates_and_dataframe",
    "section": "",
    "text": "Basic usage with default parameters:\n&gt;&gt;&gt; from spotforecast2.scripts.n_to_1_with_covariates import main\n&gt;&gt;&gt; main()\nWith custom forecast horizon and weights:\n&gt;&gt;&gt; predictions = main(\n...     forecast_horizon=48,\n...     weights=[1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0, 1.0]\n... )\nWith custom location (latitude, longitude):\n&gt;&gt;&gt; predictions = main(\n...     forecast_horizon=24,\n...     latitude=48.1351,\n...     longitude=11.5820,\n...     verbose=True\n... )\nWith feature engineering options:\n&gt;&gt;&gt; predictions = main(\n...     forecast_horizon=24,\n...     include_weather_windows=True,\n...     include_holiday_features=True,\n...     include_poly_features=True,\n... )\nPassing custom estimator object:\n&gt;&gt;&gt; from lightgbm import LGBMRegressor\n&gt;&gt;&gt; estimator = LGBMRegressor(n_estimators=200, learning_rate=0.01)\n&gt;&gt;&gt; predictions = main(forecast_horizon=24, estimator=estimator)\nAvailable Parameters:",
    "crumbs": [
      "API Reference",
      "Tasks",
      "task_n_to_1_with_covariates_and_dataframe"
    ]
  },
  {
    "objectID": "docs/reference/tasks.task_n_to_1_with_covariates_and_dataframe.html#forecasting-parameters",
    "href": "docs/reference/tasks.task_n_to_1_with_covariates_and_dataframe.html#forecasting-parameters",
    "title": "tasks.task_n_to_1_with_covariates_and_dataframe",
    "section": "",
    "text": "forecast_horizon (int): Number of steps ahead to forecast. Default: 24. contamination (float): Outlier detection threshold [0, 1]. Default: 0.01. window_size (int): Rolling window size for feature engineering. Default: 72. lags (int): Number of lag features to create. Default: 24. train_ratio (float): Train-test split ratio [0, 1]. Default: 0.8. verbose (bool): Enable detailed progress logging. Default: True.\nLocation & Time Parameters: latitude (float): Location latitude for sun features. Default: 51.5136 (Dortmund). longitude (float): Location longitude for sun features. Default: 7.4653 (Dortmund). timezone (str): Timezone for data processing. Default: “UTC”. country_code (str): Country code for holidays (ISO 3166-1 alpha-2). Default: “DE”. state (str): State/region code for holidays (depends on country). Default: “NW”.",
    "crumbs": [
      "API Reference",
      "Tasks",
      "task_n_to_1_with_covariates_and_dataframe"
    ]
  },
  {
    "objectID": "docs/reference/tasks.task_n_to_1_with_covariates_and_dataframe.html#feature-engineering-parameters",
    "href": "docs/reference/tasks.task_n_to_1_with_covariates_and_dataframe.html#feature-engineering-parameters",
    "title": "tasks.task_n_to_1_with_covariates_and_dataframe",
    "section": "",
    "text": "include_weather_windows (bool): Include rolling weather statistics. Default: False. include_holiday_features (bool): Include holiday indicator features. Default: False. include_poly_features (bool): Include polynomial interaction features. Default: False.",
    "crumbs": [
      "API Reference",
      "Tasks",
      "task_n_to_1_with_covariates_and_dataframe"
    ]
  },
  {
    "objectID": "docs/reference/tasks.task_n_to_1_with_covariates_and_dataframe.html#model-parameters",
    "href": "docs/reference/tasks.task_n_to_1_with_covariates_and_dataframe.html#model-parameters",
    "title": "tasks.task_n_to_1_with_covariates_and_dataframe",
    "section": "",
    "text": "estimator (Optional[Union[str, object]]): Forecaster estimator. Can be: - None: Uses default LGBMRegressor(n_estimators=100) - “ForecasterRecursive”: String reference (uses default) - LGBMRegressor(…): Custom estimator object Default: None.",
    "crumbs": [
      "API Reference",
      "Tasks",
      "task_n_to_1_with_covariates_and_dataframe"
    ]
  },
  {
    "objectID": "docs/reference/tasks.task_n_to_1_with_covariates_and_dataframe.html#aggregation-parameters",
    "href": "docs/reference/tasks.task_n_to_1_with_covariates_and_dataframe.html#aggregation-parameters",
    "title": "tasks.task_n_to_1_with_covariates_and_dataframe",
    "section": "",
    "text": "weights (Optional[Union[Dict[str, float], List[float], np.ndarray]]): Weights for prediction aggregation. Can be: - None: Defaults to uniform weights (1.0 for each column) - Dict: Column name -&gt; weight mapping - List/Array: Weights in column order Default: [1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0, 1.0].",
    "crumbs": [
      "API Reference",
      "Tasks",
      "task_n_to_1_with_covariates_and_dataframe"
    ]
  },
  {
    "objectID": "docs/reference/tasks.task_n_to_1_with_covariates_and_dataframe.html#functions",
    "href": "docs/reference/tasks.task_n_to_1_with_covariates_and_dataframe.html#functions",
    "title": "tasks.task_n_to_1_with_covariates_and_dataframe",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nmain\nExecute the complete N-to-1 forecasting pipeline with default parameters.\n\n\nn_to_1_with_covariates\nExecute N-to-1 forecasting pipeline with exogenous covariates.\n\n\n\n\n\ntasks.task_n_to_1_with_covariates_and_dataframe.main()\nExecute the complete N-to-1 forecasting pipeline with default parameters.\nThis is the entry point when running the script directly. It executes the full forecasting pipeline with default settings and prints comprehensive results.\nThe default configuration: - Forecasts 24 steps ahead - Uses Dortmund, Germany coordinates - Applies default contamination and window parameters - Aggregates with predefined weights - Provides verbose output\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone. Results are printed to stdout.\n\n\n\n\n\n\nRun the script directly:\n&gt;&gt;&gt; python n_to_1_with_covariates.py\nOr call main() programmatically:\n&gt;&gt;&gt; from spotforecast2.scripts.n_to_1_with_covariates import main\n&gt;&gt;&gt; main()\n\n\n\n\ntasks.task_n_to_1_with_covariates_and_dataframe.n_to_1_with_covariates(\n    data=None,\n    forecast_horizon=24,\n    contamination=0.01,\n    window_size=72,\n    lags=24,\n    train_ratio=0.8,\n    latitude=51.5136,\n    longitude=7.4653,\n    timezone='UTC',\n    country_code='DE',\n    state='NW',\n    estimator=None,\n    include_weather_windows=False,\n    include_holiday_features=False,\n    include_poly_features=False,\n    weights=None,\n    verbose=True,\n    show_progress=True,\n    **kwargs,\n)\nExecute N-to-1 forecasting pipeline with exogenous covariates.\nThis function performs a complete time series forecasting workflow: 1. Fetches and preprocesses data 2. Engineers features (calendar, weather, holidays, cyclical, polynomial) 3. Trains recursive forecaster on multiple targets 4. Aggregates predictions using weighted combination\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\nOptional[pd.DataFrame]\nOptional DataFrame with target time series data. If None, fetches data automatically. Default: None.\nNone\n\n\nforecast_horizon\nint\nNumber of forecast steps ahead. Determines how many time steps to predict into the future. Typical values: 24 (1 day), 48 (2 days), 168 (1 week). Default: 24.\n24\n\n\ncontamination\nfloat\nOutlier contamination level for anomaly detection. Expected proportion of outliers in the training data [0, 1]. Higher values detect fewer outliers. Default: 0.01 (1%).\n0.01\n\n\nwindow_size\nint\nRolling window size for feature engineering (hours). Size of the rolling window for computing statistics. Must be &gt; lags. Typical range: 24-168. Default: 72.\n72\n\n\nlags\nint\nNumber of lagged features to create. Creates AR(p) features with p=lags. Typical values: 12, 24, 48. Default: 24.\n24\n\n\ntrain_ratio\nfloat\nProportion of data for training [0, 1]. Remaining data (1 - train_ratio) used for validation/testing. Typical values: 0.7-0.9. Default: 0.8.\n0.8\n\n\nlatitude\nfloat\nGeographic latitude for solar features. Used to compute sunrise/sunset times for day/night features. Default: 51.5136 (Dortmund, Germany).\n51.5136\n\n\nlongitude\nfloat\nGeographic longitude for solar features. Used to compute sunrise/sunset times for day/night features. Default: 7.4653 (Dortmund, Germany).\n7.4653\n\n\ntimezone\nstr\nTimezone for time-based features. Any timezone recognized by pytz. Default: “UTC”.\n'UTC'\n\n\ncountry_code\nstr\nISO 3166-1 alpha-2 country code for holidays. Examples: “DE” (Germany), “US” (USA), “GB” (UK). Default: “DE”.\n'DE'\n\n\nstate\nstr\nState/region code for holidays. Country-dependent. For Germany: “BW”, “BY”, “NW”, etc. Default: “NW” (Nordrhein-Westfalen).\n'NW'\n\n\nestimator\nOptional[Union[str, object]]\nForecaster model. Can be: - None: Uses LGBMRegressor(n_estimators=100, verbose=-1). - “ForecasterRecursive”: References default estimator (same as None). - LGBMRegressor(…): Custom pre-configured estimator. - Any sklearn-compatible regressor. Default: None.\nNone\n\n\ninclude_weather_windows\nbool\nAdd rolling weather statistics. Creates moving averages, min, max of weather features over multiple windows (1D, 7D). Increases feature count significantly. Default: False.\nFalse\n\n\ninclude_holiday_features\nbool\nAdd holiday binary indicators. Creates features indicating holidays and special dates. Useful for capturing demand patterns around holidays. Default: False.\nFalse\n\n\ninclude_poly_features\nbool\nAdd polynomial interactions. Creates 2nd-order interaction terms between selected features. Useful for capturing non-linear relationships. Default: False.\nFalse\n\n\nweights\nOptional[Union[Dict[str, float], List[float], np.ndarray]]\nWeights for combining multi-output predictions. Can be: - None: Default weights [1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0, 1.0] - Dict: {“col_name”: weight, …} for specific columns - List: [w1, w2, …] in column order - np.ndarray: Same as list Default: None (uses default weights).\nNone\n\n\nverbose\nbool\nEnable progress logging. Prints intermediate results and timestamps. Default: True.\nTrue\n\n\nshow_progress\nbool\nShow a progress bar for major pipeline steps. Default: True.\nTrue\n\n\n**kwargs\nAny\nAdditional parameters for underlying functions. These are passed to n2n_predict_with_covariates(). Examples: - freq: Frequency for data resampling. Default: “h” (hourly). - columns: Specific columns to forecast. Default: None (all). Any parameter accepted by n2n_predict_with_covariates().\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTuple[pd.DataFrame, pd.Series, Dict, Dict]\nTuple[pd.DataFrame, pd.Series, Dict, Dict]: A tuple containing: - predictions (pd.DataFrame): Multi-output forecasts from recursive model. Each column represents a target variable. Index is datetime matching the forecast period. - combined_prediction (pd.Series): Aggregated forecast from weighted combination. Single column combining all output predictions. Index is datetime matching the forecast period. - model_metrics (Dict): Performance metrics from recursive forecaster. Keys may include: ‘mae’, ‘rmse’, ‘mape’, etc. - feature_info (Dict): Information about engineered features. Contains feature counts, types, and engineering details.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf forecast_horizon &lt;= 0 or invalid parameter combinations.\n\n\n\nFileNotFoundError\nIf data source files cannot be accessed.\n\n\n\nRuntimeError\nIf model training fails or data processing errors occur.\n\n\n\n\n\n\nBasic usage (uses all defaults):\n&gt;&gt;&gt; predictions, combined, metrics, features = n_to_1_with_covariates()\n&gt;&gt;&gt; print(f\"Predictions shape: {predictions.shape}\")\n&gt;&gt;&gt; print(f\"Combined forecast head:\\n{combined.head()}\")\nCustom location and forecast horizon:\n&gt;&gt;&gt; predictions, combined, metrics, features = n_to_1_with_covariates(\n...     forecast_horizon=48,\n...     latitude=48.1351,\n...     longitude=11.5820,\n...     country_code=\"DE\",\n...     state=\"BY\",\n...     verbose=True\n... )\nWith feature engineering enabled:\n&gt;&gt;&gt; predictions, combined, metrics, features = n_to_1_with_covariates(\n...     forecast_horizon=24,\n...     include_weather_windows=True,\n...     include_holiday_features=True,\n...     include_poly_features=True,\n...     verbose=True\n... )\nCustom estimator and weights:\n&gt;&gt;&gt; from lightgbm import LGBMRegressor\n&gt;&gt;&gt; custom_estimator = LGBMRegressor(\n...     n_estimators=200,\n...     learning_rate=0.01,\n...     max_depth=7\n... )\n&gt;&gt;&gt; custom_weights = [1.0, 1.0, -0.5, -0.5]\n&gt;&gt;&gt; predictions, combined, metrics, features = n_to_1_with_covariates(\n...     forecast_horizon=24,\n...     estimator=custom_estimator,\n...     weights=custom_weights,\n...     verbose=True\n... )\nWith all advanced options:\n&gt;&gt;&gt; predictions, combined, metrics, features = n_to_1_with_covariates(\n...     forecast_horizon=72,\n...     contamination=0.02,\n...     window_size=168,\n...     lags=48,\n...     train_ratio=0.75,\n...     latitude=50.1109,\n...     longitude=8.6821,\n...     timezone=\"Europe/Berlin\",\n...     country_code=\"DE\",\n...     state=\"HE\",\n...     include_weather_windows=True,\n...     include_holiday_features=True,\n...     include_poly_features=True,\n...     weights={\"power\": 1.0, \"demand\": 0.8},\n...     verbose=True,\n...     freq=\"h\",\n... )\n&gt;&gt;&gt; print(f\"Model Metrics: {metrics}\")\n&gt;&gt;&gt; print(f\"Feature Info: {features}\")",
    "crumbs": [
      "API Reference",
      "Tasks",
      "task_n_to_1_with_covariates_and_dataframe"
    ]
  },
  {
    "objectID": "docs/reference/tasks.task_n_to_1_dataframe.html",
    "href": "docs/reference/tasks.task_n_to_1_dataframe.html",
    "title": "tasks.task_n_to_1_dataframe",
    "section": "",
    "text": "tasks.task_n_to_1_dataframe\ntasks.task_n_to_1_dataframe",
    "crumbs": [
      "API Reference",
      "Tasks",
      "task_n_to_1_dataframe"
    ]
  },
  {
    "objectID": "docs/reference/tasks.task_entsoe.html",
    "href": "docs/reference/tasks.task_entsoe.html",
    "title": "tasks.task_entsoe",
    "section": "",
    "text": "tasks.task_entsoe\nUnified CLI task script for ENTSO-E data downloading, model training, and prediction.\nThis script acts as the main entry point for the forecasting pipeline, integrating downloading, training, and plotting functionalities.\n\n\nuv run python src/spotforecast2/tasks/task_entsoe.py download –api-key  uv run python src/spotforecast2/tasks/task_entsoe.py train lgbm uv run python src/spotforecast2/tasks/task_entsoe.py predict –plot\n\n\n\nENTSOE_API_KEY: The API key for ENTSO-E Transparency Platform.\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nForecasterRecursiveLGBM\nLGBM forecaster with config-injected defaults.\n\n\nForecasterRecursiveXGB\nXGBoost forecaster with config-injected defaults.\n\n\n\n\n\ntasks.task_entsoe.ForecasterRecursiveLGBM(iteration, lags=12, **kwargs)\nLGBM forecaster with config-injected defaults.\nEnsures all model instances use consistent configuration values from ConfigEntsoe for periods, country_code, and random_state.\n\n\n\ntasks.task_entsoe.ForecasterRecursiveXGB(iteration, lags=12, **kwargs)\nXGBoost forecaster with config-injected defaults.\nEnsures all model instances use consistent configuration values from ConfigEntsoe for periods, country_code, and random_state.",
    "crumbs": [
      "API Reference",
      "Tasks",
      "task_entsoe"
    ]
  },
  {
    "objectID": "docs/reference/tasks.task_entsoe.html#usage",
    "href": "docs/reference/tasks.task_entsoe.html#usage",
    "title": "tasks.task_entsoe",
    "section": "",
    "text": "uv run python src/spotforecast2/tasks/task_entsoe.py download –api-key  uv run python src/spotforecast2/tasks/task_entsoe.py train lgbm uv run python src/spotforecast2/tasks/task_entsoe.py predict –plot",
    "crumbs": [
      "API Reference",
      "Tasks",
      "task_entsoe"
    ]
  },
  {
    "objectID": "docs/reference/tasks.task_entsoe.html#environment-variables",
    "href": "docs/reference/tasks.task_entsoe.html#environment-variables",
    "title": "tasks.task_entsoe",
    "section": "",
    "text": "ENTSOE_API_KEY: The API key for ENTSO-E Transparency Platform.",
    "crumbs": [
      "API Reference",
      "Tasks",
      "task_entsoe"
    ]
  },
  {
    "objectID": "docs/reference/tasks.task_entsoe.html#classes",
    "href": "docs/reference/tasks.task_entsoe.html#classes",
    "title": "tasks.task_entsoe",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nForecasterRecursiveLGBM\nLGBM forecaster with config-injected defaults.\n\n\nForecasterRecursiveXGB\nXGBoost forecaster with config-injected defaults.\n\n\n\n\n\ntasks.task_entsoe.ForecasterRecursiveLGBM(iteration, lags=12, **kwargs)\nLGBM forecaster with config-injected defaults.\nEnsures all model instances use consistent configuration values from ConfigEntsoe for periods, country_code, and random_state.\n\n\n\ntasks.task_entsoe.ForecasterRecursiveXGB(iteration, lags=12, **kwargs)\nXGBoost forecaster with config-injected defaults.\nEnsures all model instances use consistent configuration values from ConfigEntsoe for periods, country_code, and random_state.",
    "crumbs": [
      "API Reference",
      "Tasks",
      "task_entsoe"
    ]
  },
  {
    "objectID": "docs/reference/stats.autocorrelation.html",
    "href": "docs/reference/stats.autocorrelation.html",
    "title": "stats.autocorrelation",
    "section": "",
    "text": "stats.autocorrelation\n\n\n\n\n\nName\nDescription\n\n\n\n\ncalculate_lag_autocorrelation\nCalculate autocorrelation and partial autocorrelation for a time series.\n\n\n\n\n\nstats.autocorrelation.calculate_lag_autocorrelation(\n    data,\n    n_lags=50,\n    last_n_samples=None,\n    sort_by='partial_autocorrelation_abs',\n    acf_kwargs=None,\n    pacf_kwargs=None,\n)\nCalculate autocorrelation and partial autocorrelation for a time series.\nThis is a wrapper around statsmodels.acf and statsmodels.pacf.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.Series | pd.DataFrame\nTime series to calculate autocorrelation. If a DataFrame is provided, it must have exactly one column.\nrequired\n\n\nn_lags\nint\nNumber of lags to calculate autocorrelation. Default is 50.\n50\n\n\nlast_n_samples\nint | None\nNumber of most recent samples to use. If None, use the entire series. Note that partial correlations can only be computed for lags up to 50% of the sample size. For example, if the series has 10 samples, n_lags must be less than or equal to 5. This parameter is useful to speed up calculations when the series is very long. Default is None.\nNone\n\n\nsort_by\nstr\nSort results by lag, partial_autocorrelation_abs, partial_autocorrelation, autocorrelation_abs or autocorrelation. Default is partial_autocorrelation_abs.\n'partial_autocorrelation_abs'\n\n\nacf_kwargs\ndict[str, object] | None\nOptional arguments to pass to statsmodels.tsa.stattools.acf. Default is {}.\nNone\n\n\npacf_kwargs\ndict[str, object] | None\nOptional arguments to pass to statsmodels.tsa.stattools.pacf. Default is {}.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\nDataFrame with columns: lag, partial_autocorrelation_abs, partial_autocorrelation, autocorrelation_abs, autocorrelation.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf data is not a pandas Series or DataFrame with a single column.\n\n\n\nValueError\nIf data is a DataFrame with more than one column.\n\n\n\nTypeError\nIf n_lags is not a positive integer.\n\n\n\nTypeError\nIf last_n_samples is not None and not a positive integer.\n\n\n\nValueError\nIf sort_by is not one of the valid options.\n\n\n\n\n\n\nCalculate autocorrelation for a simple Series:\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast.stats.autocorrelation import calculate_lag_autocorrelation\n&gt;&gt;&gt;\n&gt;&gt;&gt; data = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; result = calculate_lag_autocorrelation(data=data, n_lags=4)\n&gt;&gt;&gt; result.head()\n   lag  partial_autocorrelation_abs  partial_autocorrelation  autocorrelation_abs  autocorrelation\n0    1                     0.999998                 0.999998             1.000000         1.000000\n1    2                     0.000002                -0.000002             0.645497         0.645497\n2    3                     0.000002                 0.000002             0.298549         0.298549\n3    4                     0.000001                -0.000001             0.068719         0.068719\nCalculate autocorrelation using only the last 8 samples:\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2.stats.autocorrelation import calculate_lag_autocorrelation\n&gt;&gt;&gt;\n&gt;&gt;&gt; data = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; result = calculate_lag_autocorrelation(\n...     data=data,\n...     n_lags=3,\n...     last_n_samples=8\n... )\n&gt;&gt;&gt; result.shape\n(3, 5)\nCalculate autocorrelation from a DataFrame with a single column:\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast.stats.autocorrelation import calculate_lag_autocorrelation\n&gt;&gt;&gt;\n&gt;&gt;&gt; data = pd.DataFrame({'value': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})\n&gt;&gt;&gt; result = calculate_lag_autocorrelation(data=data, n_lags=4)\n&gt;&gt;&gt; result.shape\n(4, 5)\nSort results by autocorrelation in descending order:\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast.stats.autocorrelation import calculate_lag_autocorrelation\n&gt;&gt;&gt;\n&gt;&gt;&gt; data = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; result = calculate_lag_autocorrelation(\n...     data=data,\n...     n_lags=4,\n...     sort_by='autocorrelation'\n... )\n&gt;&gt;&gt; result[['lag', 'autocorrelation']].head()\n   lag  autocorrelation\n0    1         1.000000\n1    2         0.645497\n2    3         0.298549\n3    4         0.068719",
    "crumbs": [
      "API Reference",
      "Stats",
      "autocorrelation"
    ]
  },
  {
    "objectID": "docs/reference/stats.autocorrelation.html#functions",
    "href": "docs/reference/stats.autocorrelation.html#functions",
    "title": "stats.autocorrelation",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncalculate_lag_autocorrelation\nCalculate autocorrelation and partial autocorrelation for a time series.\n\n\n\n\n\nstats.autocorrelation.calculate_lag_autocorrelation(\n    data,\n    n_lags=50,\n    last_n_samples=None,\n    sort_by='partial_autocorrelation_abs',\n    acf_kwargs=None,\n    pacf_kwargs=None,\n)\nCalculate autocorrelation and partial autocorrelation for a time series.\nThis is a wrapper around statsmodels.acf and statsmodels.pacf.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.Series | pd.DataFrame\nTime series to calculate autocorrelation. If a DataFrame is provided, it must have exactly one column.\nrequired\n\n\nn_lags\nint\nNumber of lags to calculate autocorrelation. Default is 50.\n50\n\n\nlast_n_samples\nint | None\nNumber of most recent samples to use. If None, use the entire series. Note that partial correlations can only be computed for lags up to 50% of the sample size. For example, if the series has 10 samples, n_lags must be less than or equal to 5. This parameter is useful to speed up calculations when the series is very long. Default is None.\nNone\n\n\nsort_by\nstr\nSort results by lag, partial_autocorrelation_abs, partial_autocorrelation, autocorrelation_abs or autocorrelation. Default is partial_autocorrelation_abs.\n'partial_autocorrelation_abs'\n\n\nacf_kwargs\ndict[str, object] | None\nOptional arguments to pass to statsmodels.tsa.stattools.acf. Default is {}.\nNone\n\n\npacf_kwargs\ndict[str, object] | None\nOptional arguments to pass to statsmodels.tsa.stattools.pacf. Default is {}.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\nDataFrame with columns: lag, partial_autocorrelation_abs, partial_autocorrelation, autocorrelation_abs, autocorrelation.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf data is not a pandas Series or DataFrame with a single column.\n\n\n\nValueError\nIf data is a DataFrame with more than one column.\n\n\n\nTypeError\nIf n_lags is not a positive integer.\n\n\n\nTypeError\nIf last_n_samples is not None and not a positive integer.\n\n\n\nValueError\nIf sort_by is not one of the valid options.\n\n\n\n\n\n\nCalculate autocorrelation for a simple Series:\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast.stats.autocorrelation import calculate_lag_autocorrelation\n&gt;&gt;&gt;\n&gt;&gt;&gt; data = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; result = calculate_lag_autocorrelation(data=data, n_lags=4)\n&gt;&gt;&gt; result.head()\n   lag  partial_autocorrelation_abs  partial_autocorrelation  autocorrelation_abs  autocorrelation\n0    1                     0.999998                 0.999998             1.000000         1.000000\n1    2                     0.000002                -0.000002             0.645497         0.645497\n2    3                     0.000002                 0.000002             0.298549         0.298549\n3    4                     0.000001                -0.000001             0.068719         0.068719\nCalculate autocorrelation using only the last 8 samples:\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2.stats.autocorrelation import calculate_lag_autocorrelation\n&gt;&gt;&gt;\n&gt;&gt;&gt; data = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; result = calculate_lag_autocorrelation(\n...     data=data,\n...     n_lags=3,\n...     last_n_samples=8\n... )\n&gt;&gt;&gt; result.shape\n(3, 5)\nCalculate autocorrelation from a DataFrame with a single column:\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast.stats.autocorrelation import calculate_lag_autocorrelation\n&gt;&gt;&gt;\n&gt;&gt;&gt; data = pd.DataFrame({'value': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})\n&gt;&gt;&gt; result = calculate_lag_autocorrelation(data=data, n_lags=4)\n&gt;&gt;&gt; result.shape\n(4, 5)\nSort results by autocorrelation in descending order:\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast.stats.autocorrelation import calculate_lag_autocorrelation\n&gt;&gt;&gt;\n&gt;&gt;&gt; data = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; result = calculate_lag_autocorrelation(\n...     data=data,\n...     n_lags=4,\n...     sort_by='autocorrelation'\n... )\n&gt;&gt;&gt; result[['lag', 'autocorrelation']].head()\n   lag  autocorrelation\n0    1         1.000000\n1    2         0.645497\n2    3         0.298549\n3    4         0.068719",
    "crumbs": [
      "API Reference",
      "Stats",
      "autocorrelation"
    ]
  },
  {
    "objectID": "docs/reference/preprocessing.split.html",
    "href": "docs/reference/preprocessing.split.html",
    "title": "preprocessing.split",
    "section": "",
    "text": "preprocessing.split\n\n\n\n\n\nName\nDescription\n\n\n\n\nsplit_abs_train_val_test\nSplits a time series DataFrame into training, validation, and test sets based on absolute timestamps.\n\n\nsplit_rel_train_val_test\nSplits a time series DataFrame into training, validation, and test sets by percentages.\n\n\n\n\n\npreprocessing.split.split_abs_train_val_test(\n    data,\n    end_train,\n    end_validation,\n    verbose=False,\n)\nSplits a time series DataFrame into training, validation, and test sets based on absolute timestamps.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame\nThe time series data with a DateTimeIndex.\nrequired\n\n\nend_train\npd.Timestamp\nThe end date for the training set.\nrequired\n\n\nend_validation\npd.Timestamp\nThe end date for the validation set.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ntuple\ntuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]\nA tuple containing: - data_train (pd.DataFrame): The training set. - data_val (pd.DataFrame): The validation set. - data_test (pd.DataFrame): The test set.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.split import split_train_val_test\n&gt;&gt;&gt; data = fetch_data(filename=\"data_in.csv\")\n&gt;&gt;&gt; end_train = pd.Timestamp('2020-12-31 23:00:00')\n&gt;&gt;&gt; end_validation = pd.Timestamp('2021-06-30 23:00:00')\n&gt;&gt;&gt; data_train, data_val, data_test = split_train_val_test(\n...     data,\n...     end_train=end_train,\n...     end_validation=end_validation,\n...     verbose=True\n... )\n\n\n\n\npreprocessing.split.split_rel_train_val_test(\n    data,\n    perc_train,\n    perc_val,\n    verbose=False,\n)\nSplits a time series DataFrame into training, validation, and test sets by percentages.\nThe test percentage is computed as 1 - perc_train - perc_val. Sizes are rounded to ensure the splits sum to the full dataset size.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame\nThe time series data with a DateTimeIndex.\nrequired\n\n\nperc_train\nfloat\nFraction of data used for training.\nrequired\n\n\nperc_val\nfloat\nFraction of data used for validation.\nrequired\n\n\nverbose\nbool\nWhether to print additional information.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ntuple\ntuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]\nA tuple containing: - data_train (pd.DataFrame): The training set. - data_val (pd.DataFrame): The validation set. - data_test (pd.DataFrame): The test set.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.split import split_rel_train_val_test\n&gt;&gt;&gt; data = fetch_data(filename=\"data_in.csv\")\n&gt;&gt;&gt; data_train, data_val, data_test = split_rel_train_val_test(\n...     data,\n...     perc_train=0.7,\n...     perc_val=0.2,\n...     verbose=True\n... )",
    "crumbs": [
      "API Reference",
      "Preprocessing",
      "split"
    ]
  },
  {
    "objectID": "docs/reference/preprocessing.split.html#functions",
    "href": "docs/reference/preprocessing.split.html#functions",
    "title": "preprocessing.split",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nsplit_abs_train_val_test\nSplits a time series DataFrame into training, validation, and test sets based on absolute timestamps.\n\n\nsplit_rel_train_val_test\nSplits a time series DataFrame into training, validation, and test sets by percentages.\n\n\n\n\n\npreprocessing.split.split_abs_train_val_test(\n    data,\n    end_train,\n    end_validation,\n    verbose=False,\n)\nSplits a time series DataFrame into training, validation, and test sets based on absolute timestamps.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame\nThe time series data with a DateTimeIndex.\nrequired\n\n\nend_train\npd.Timestamp\nThe end date for the training set.\nrequired\n\n\nend_validation\npd.Timestamp\nThe end date for the validation set.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ntuple\ntuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]\nA tuple containing: - data_train (pd.DataFrame): The training set. - data_val (pd.DataFrame): The validation set. - data_test (pd.DataFrame): The test set.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.split import split_train_val_test\n&gt;&gt;&gt; data = fetch_data(filename=\"data_in.csv\")\n&gt;&gt;&gt; end_train = pd.Timestamp('2020-12-31 23:00:00')\n&gt;&gt;&gt; end_validation = pd.Timestamp('2021-06-30 23:00:00')\n&gt;&gt;&gt; data_train, data_val, data_test = split_train_val_test(\n...     data,\n...     end_train=end_train,\n...     end_validation=end_validation,\n...     verbose=True\n... )\n\n\n\n\npreprocessing.split.split_rel_train_val_test(\n    data,\n    perc_train,\n    perc_val,\n    verbose=False,\n)\nSplits a time series DataFrame into training, validation, and test sets by percentages.\nThe test percentage is computed as 1 - perc_train - perc_val. Sizes are rounded to ensure the splits sum to the full dataset size.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame\nThe time series data with a DateTimeIndex.\nrequired\n\n\nperc_train\nfloat\nFraction of data used for training.\nrequired\n\n\nperc_val\nfloat\nFraction of data used for validation.\nrequired\n\n\nverbose\nbool\nWhether to print additional information.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ntuple\ntuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]\nA tuple containing: - data_train (pd.DataFrame): The training set. - data_val (pd.DataFrame): The validation set. - data_test (pd.DataFrame): The test set.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.split import split_rel_train_val_test\n&gt;&gt;&gt; data = fetch_data(filename=\"data_in.csv\")\n&gt;&gt;&gt; data_train, data_val, data_test = split_rel_train_val_test(\n...     data,\n...     perc_train=0.7,\n...     perc_val=0.2,\n...     verbose=True\n... )",
    "crumbs": [
      "API Reference",
      "Preprocessing",
      "split"
    ]
  },
  {
    "objectID": "docs/reference/preprocessing.outlier.html",
    "href": "docs/reference/preprocessing.outlier.html",
    "title": "preprocessing.outlier",
    "section": "",
    "text": "preprocessing.outlier\npreprocessing.outlier\nOutlier detection utilities (legacy wrapper for spotforecast2_safe).",
    "crumbs": [
      "API Reference",
      "Preprocessing",
      "outlier"
    ]
  },
  {
    "objectID": "docs/reference/preprocessing.curate_data.html",
    "href": "docs/reference/preprocessing.curate_data.html",
    "title": "preprocessing.curate_data",
    "section": "",
    "text": "preprocessing.curate_data\n\n\n\n\n\nName\nDescription\n\n\n\n\nagg_and_resample_data\nAggregates and resamples the data to (e.g.,hourly) frequency by computing the specified aggregation (e.g. for each hour).\n\n\nbasic_ts_checks\nChecks if the time series data has a datetime index and is sorted.\n\n\ncurate_holidays\nChecks if the holiday dataframe has the correct shape.\n\n\ncurate_weather\nChecks if the weather dataframe has the correct shape.\n\n\nget_start_end\nGet start and end date strings for data and covariate ranges.\n\n\n\n\n\npreprocessing.curate_data.agg_and_resample_data(\n    data,\n    rule='h',\n    closed='left',\n    label='left',\n    by='mean',\n    verbose=False,\n)\nAggregates and resamples the data to (e.g.,hourly) frequency by computing the specified aggregation (e.g. for each hour).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame\nThe dataset with a datetime index.\nrequired\n\n\nrule\nstr\nThe resample rule (e.g., ‘h’ for hourly, ‘D’ for daily). Default is ‘h’ which creates an hourly grid.\n'h'\n\n\nclosed\nstr\nWhich side of bin interval is closed. Default is ‘left’. Using closed=\"left\", label=\"left\" specifies that a time interval (e.g., 10:00 to 11:00) is labeled with the start timestamp (10:00). For consumption data, a different representation is usually more common: closed=\"left\", label=\"right\", so the interval is labeled with the end timestamp (11:00), since consumption is typically reported after one hour.\n'left'\n\n\nlabel\nstr\nWhich bin edge label to use. Default is ‘left’. See ‘closed’ parameter for details on labeling behavior.\n'left'\n\n\nby\nstr or callable\nAggregation method to apply (e.g., ‘mean’, ‘sum’, ‘median’). Default is ‘mean’. The aggregation serves robustness: if the data were more finely resolved (e.g., quarter-hourly), asfreq would only pick one value (sampling), while .agg(“mean”) forms the correct average over the hour. If the data is already hourly, .agg doesn’t change anything but ensures that no duplicates exist.\n'mean'\n\n\nverbose\nbool\nWhether to print additional information.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: Resampled and aggregated dataframe.\n\n\n\n\n\n\n\nresample(rule=“h”): Creates an hourly grid\nclosed/label: Control how time intervals are labeled\n.agg({…: by}): Aggregates values within each time bin\n\nExamples:: &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import agg_and_resample_data &gt;&gt;&gt; import pandas as pd &gt;&gt;&gt; date_rng = pd.date_range(start=‘2023-01-01’, end=‘2023-01-02’, freq=‘15T’) &gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=[‘date’]) &gt;&gt;&gt; data.set_index(‘date’, inplace=True) &gt;&gt;&gt; data[‘value’] = range(len(data)) &gt;&gt;&gt; resampled_data = agg_and_resample_data(data, rule=‘h’, by=‘mean’) &gt;&gt;&gt; print(resampled_data.head())\n\n\n\n\npreprocessing.curate_data.basic_ts_checks(data, verbose=False)\nChecks if the time series data has a datetime index and is sorted.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame\nThe main dataset.\nrequired\n\n\nverbose\nbool\nWhether to print additional information.\nFalse\n\n\n\n\n\n\n&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import basic_ts_checks\n&gt;&gt;&gt; data = fetch_data(filename=\"data_in.csv\")\n&gt;&gt;&gt; basic_ts_checks(data)\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf the index is not a datetime index.\n\n\n\nValueError\nIf the datetime index is not sorted in increasing order or is incomplete.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nbool\nbool\nTrue if the datetime index is valid, sorted, and complete.\n\n\n\n\n\n\n\npreprocessing.curate_data.curate_holidays(holiday_df, data, forecast_horizon)\nChecks if the holiday dataframe has the correct shape. Args: holiday_df (pd.DataFrame): DataFrame containing holiday information. data (pd.DataFrame): The main dataset. forecast_horizon (int): The forecast horizon in hours.\n\n\n&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data, fetch_holiday_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end, curate_holidays\n&gt;&gt;&gt; data = fetch_data(filename=\"data_in.csv\")\n&gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n...     data=data,\n...     forecast_horizon=24,\n...     verbose=False\n... )\n&gt;&gt;&gt; holiday_df = fetch_holiday_data(\n...     start='2023-01-01T00:00',\n...     end='2023-01-10T00:00',\n...     tz='UTC',\n...     freq='h',\n...     country_code='DE',\n...     state='NW'\n... )\n&gt;&gt;&gt; FORECAST_HORIZON = 24\n&gt;&gt;&gt; curate_holidays(holiday_df, data, forecast_horizon=FORECAST_HORIZON)\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAssertionError\nIf the holiday dataframe does not have the correct number of rows.\n\n\n\n\n\n\n\npreprocessing.curate_data.curate_weather(weather_df, data, forecast_horizon)\nChecks if the weather dataframe has the correct shape.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nweather_df\npd.DataFrame\nDataFrame containing weather information.\nrequired\n\n\ndata\npd.DataFrame\nThe main dataset.\nrequired\n\n\nforecast_horizon\nint\nThe forecast horizon in hours.\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data, fetch_weather_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end, curate_weather\n&gt;&gt;&gt; data = fetch_data(filename=\"data_in.csv\")\n&gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n...     data=data,\n...     forecast_horizon=24,\n...     verbose=False\n... )\n&gt;&gt;&gt; weather_df = fetch_weather_data(\n...     cov_start=COV_START,\n...     cov_end=COV_END,\n...     tz='UTC',\n...     freq='h',\n...     latitude=51.5136,\n...     longitude=7.4653\n... )\n&gt;&gt;&gt; FORECAST_HORIZON = 24\n&gt;&gt;&gt; curate_weather(weather_df, data, forecast_horizon=FORECAST_HORIZON)\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAssertionError\nIf the weather dataframe does not have the correct number of rows.\n\n\n\n\n\n\n\npreprocessing.curate_data.get_start_end(data, forecast_horizon, verbose=True)\nGet start and end date strings for data and covariate ranges. Covariate range is extended by the forecast horizon.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame\nThe dataset with a datetime index.\nrequired\n\n\nforecast_horizon\nint\nThe forecast horizon in hours.\nrequired\n\n\nverbose\nbool\nWhether to print the determined date ranges.\nTrue\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple[str, str, str, str]\ntuple[str, str, str, str]: (data_start, data_end, covariate_start, covariate_end) Date strings in the format “YYYY-MM-DDTHH:MM” for data and covariate ranges.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; date_rng = pd.date_range(start='2023-01-01', end='2023-01-10', freq='h')\n&gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=['date'])\n&gt;&gt;&gt; data.set_index('date', inplace=True)\n&gt;&gt;&gt; start, end, cov_start, cov_end = get_start_end(data, forecast_horizon=24, verbose=False)\n&gt;&gt;&gt; print(start, end, cov_start, cov_end)\n2023-01-01T00:00 2023-01-10T00:00 2023-01-01T00:00 2023-01-11T00:00",
    "crumbs": [
      "API Reference",
      "Preprocessing",
      "curate_data"
    ]
  },
  {
    "objectID": "docs/reference/preprocessing.curate_data.html#functions",
    "href": "docs/reference/preprocessing.curate_data.html#functions",
    "title": "preprocessing.curate_data",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nagg_and_resample_data\nAggregates and resamples the data to (e.g.,hourly) frequency by computing the specified aggregation (e.g. for each hour).\n\n\nbasic_ts_checks\nChecks if the time series data has a datetime index and is sorted.\n\n\ncurate_holidays\nChecks if the holiday dataframe has the correct shape.\n\n\ncurate_weather\nChecks if the weather dataframe has the correct shape.\n\n\nget_start_end\nGet start and end date strings for data and covariate ranges.\n\n\n\n\n\npreprocessing.curate_data.agg_and_resample_data(\n    data,\n    rule='h',\n    closed='left',\n    label='left',\n    by='mean',\n    verbose=False,\n)\nAggregates and resamples the data to (e.g.,hourly) frequency by computing the specified aggregation (e.g. for each hour).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame\nThe dataset with a datetime index.\nrequired\n\n\nrule\nstr\nThe resample rule (e.g., ‘h’ for hourly, ‘D’ for daily). Default is ‘h’ which creates an hourly grid.\n'h'\n\n\nclosed\nstr\nWhich side of bin interval is closed. Default is ‘left’. Using closed=\"left\", label=\"left\" specifies that a time interval (e.g., 10:00 to 11:00) is labeled with the start timestamp (10:00). For consumption data, a different representation is usually more common: closed=\"left\", label=\"right\", so the interval is labeled with the end timestamp (11:00), since consumption is typically reported after one hour.\n'left'\n\n\nlabel\nstr\nWhich bin edge label to use. Default is ‘left’. See ‘closed’ parameter for details on labeling behavior.\n'left'\n\n\nby\nstr or callable\nAggregation method to apply (e.g., ‘mean’, ‘sum’, ‘median’). Default is ‘mean’. The aggregation serves robustness: if the data were more finely resolved (e.g., quarter-hourly), asfreq would only pick one value (sampling), while .agg(“mean”) forms the correct average over the hour. If the data is already hourly, .agg doesn’t change anything but ensures that no duplicates exist.\n'mean'\n\n\nverbose\nbool\nWhether to print additional information.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: Resampled and aggregated dataframe.\n\n\n\n\n\n\n\nresample(rule=“h”): Creates an hourly grid\nclosed/label: Control how time intervals are labeled\n.agg({…: by}): Aggregates values within each time bin\n\nExamples:: &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import agg_and_resample_data &gt;&gt;&gt; import pandas as pd &gt;&gt;&gt; date_rng = pd.date_range(start=‘2023-01-01’, end=‘2023-01-02’, freq=‘15T’) &gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=[‘date’]) &gt;&gt;&gt; data.set_index(‘date’, inplace=True) &gt;&gt;&gt; data[‘value’] = range(len(data)) &gt;&gt;&gt; resampled_data = agg_and_resample_data(data, rule=‘h’, by=‘mean’) &gt;&gt;&gt; print(resampled_data.head())\n\n\n\n\npreprocessing.curate_data.basic_ts_checks(data, verbose=False)\nChecks if the time series data has a datetime index and is sorted.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame\nThe main dataset.\nrequired\n\n\nverbose\nbool\nWhether to print additional information.\nFalse\n\n\n\n\n\n\n&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import basic_ts_checks\n&gt;&gt;&gt; data = fetch_data(filename=\"data_in.csv\")\n&gt;&gt;&gt; basic_ts_checks(data)\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf the index is not a datetime index.\n\n\n\nValueError\nIf the datetime index is not sorted in increasing order or is incomplete.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nbool\nbool\nTrue if the datetime index is valid, sorted, and complete.\n\n\n\n\n\n\n\npreprocessing.curate_data.curate_holidays(holiday_df, data, forecast_horizon)\nChecks if the holiday dataframe has the correct shape. Args: holiday_df (pd.DataFrame): DataFrame containing holiday information. data (pd.DataFrame): The main dataset. forecast_horizon (int): The forecast horizon in hours.\n\n\n&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data, fetch_holiday_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end, curate_holidays\n&gt;&gt;&gt; data = fetch_data(filename=\"data_in.csv\")\n&gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n...     data=data,\n...     forecast_horizon=24,\n...     verbose=False\n... )\n&gt;&gt;&gt; holiday_df = fetch_holiday_data(\n...     start='2023-01-01T00:00',\n...     end='2023-01-10T00:00',\n...     tz='UTC',\n...     freq='h',\n...     country_code='DE',\n...     state='NW'\n... )\n&gt;&gt;&gt; FORECAST_HORIZON = 24\n&gt;&gt;&gt; curate_holidays(holiday_df, data, forecast_horizon=FORECAST_HORIZON)\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAssertionError\nIf the holiday dataframe does not have the correct number of rows.\n\n\n\n\n\n\n\npreprocessing.curate_data.curate_weather(weather_df, data, forecast_horizon)\nChecks if the weather dataframe has the correct shape.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nweather_df\npd.DataFrame\nDataFrame containing weather information.\nrequired\n\n\ndata\npd.DataFrame\nThe main dataset.\nrequired\n\n\nforecast_horizon\nint\nThe forecast horizon in hours.\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data, fetch_weather_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end, curate_weather\n&gt;&gt;&gt; data = fetch_data(filename=\"data_in.csv\")\n&gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n...     data=data,\n...     forecast_horizon=24,\n...     verbose=False\n... )\n&gt;&gt;&gt; weather_df = fetch_weather_data(\n...     cov_start=COV_START,\n...     cov_end=COV_END,\n...     tz='UTC',\n...     freq='h',\n...     latitude=51.5136,\n...     longitude=7.4653\n... )\n&gt;&gt;&gt; FORECAST_HORIZON = 24\n&gt;&gt;&gt; curate_weather(weather_df, data, forecast_horizon=FORECAST_HORIZON)\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAssertionError\nIf the weather dataframe does not have the correct number of rows.\n\n\n\n\n\n\n\npreprocessing.curate_data.get_start_end(data, forecast_horizon, verbose=True)\nGet start and end date strings for data and covariate ranges. Covariate range is extended by the forecast horizon.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame\nThe dataset with a datetime index.\nrequired\n\n\nforecast_horizon\nint\nThe forecast horizon in hours.\nrequired\n\n\nverbose\nbool\nWhether to print the determined date ranges.\nTrue\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple[str, str, str, str]\ntuple[str, str, str, str]: (data_start, data_end, covariate_start, covariate_end) Date strings in the format “YYYY-MM-DDTHH:MM” for data and covariate ranges.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; date_rng = pd.date_range(start='2023-01-01', end='2023-01-10', freq='h')\n&gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=['date'])\n&gt;&gt;&gt; data.set_index('date', inplace=True)\n&gt;&gt;&gt; start, end, cov_start, cov_end = get_start_end(data, forecast_horizon=24, verbose=False)\n&gt;&gt;&gt; print(start, end, cov_start, cov_end)\n2023-01-01T00:00 2023-01-10T00:00 2023-01-01T00:00 2023-01-11T00:00",
    "crumbs": [
      "API Reference",
      "Preprocessing",
      "curate_data"
    ]
  },
  {
    "objectID": "docs/reference/preprocessing._differentiator.html",
    "href": "docs/reference/preprocessing._differentiator.html",
    "title": "preprocessing._differentiator",
    "section": "",
    "text": "preprocessing._differentiator\npreprocessing._differentiator\nTime series differentiator transformer.\nAll classes are imported from spotforecast2_safe.preprocessing._differentiator.",
    "crumbs": [
      "API Reference",
      "Preprocessing",
      "_differentiator"
    ]
  },
  {
    "objectID": "docs/reference/preprocessing._binner.html",
    "href": "docs/reference/preprocessing._binner.html",
    "title": "preprocessing._binner",
    "section": "",
    "text": "preprocessing._binner\nQuantileBinner class for binning data into quantile-based bins.\nThis module contains the QuantileBinner class which bins data into quantile-based bins using numpy.percentile with optimized performance using numpy.searchsorted.\n\n\n\n\n\nName\nDescription\n\n\n\n\nQuantileBinner\nBin data into quantile-based bins using numpy.percentile.\n\n\n\n\n\npreprocessing._binner.QuantileBinner(\n    n_bins,\n    method='linear',\n    subsample=200000,\n    dtype=np.float64,\n    random_state=789654,\n)\nBin data into quantile-based bins using numpy.percentile.\nThis class is similar to sklearn’s KBinsDiscretizer but optimized for performance using numpy.searchsorted for fast bin assignment. Bin intervals are defined following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside the range are clipped to the first or last bin.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn_bins\nint\nThe number of quantile-based bins to create. Must be &gt;= 2.\nrequired\n\n\nmethod\nstr\nThe method used to compute quantiles, passed to numpy.percentile. Default is ‘linear’. Valid values: “inverse_cdf”, “averaged_inverse_cdf”, “closest_observation”, “interpolated_inverse_cdf”, “hazen”, “weibull”, “linear”, “median_unbiased”, “normal_unbiased”.\n'linear'\n\n\nsubsample\nint\nMaximum number of samples for computing quantiles. If dataset has more samples, a random subset is used. Default 200000.\n200000\n\n\ndtype\ntype\nData type for bin indices. Default is numpy.float64.\nnp.float64\n\n\nrandom_state\nint\nRandom seed for subset generation. Default 789654.\n789654\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nn_bins\nint\nNumber of bins to create.\n\n\nmethod\nstr\nQuantile computation method.\n\n\nsubsample\nint\nMaximum samples for quantile computation.\n\n\ndtype\ntype\nData type for bin indices.\n\n\nrandom_state\nint\nRandom seed.\n\n\nn_bins_\nint\nActual number of bins after fitting (may differ from n_bins if duplicate edges are found).\n\n\nbin_edges_\nnp.ndarray\nEdges of the bins learned during fitting.\n\n\ninternal_edges_\nnp.ndarray\nInternal edges for optimized bin assignment.\n\n\nintervals_\ndict\nMapping from bin index to (lower, upper) interval bounds.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Basic usage: create 3 quantile bins\n&gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; _ = binner.fit(X)\n&gt;&gt;&gt; result = binner.transform(np.array([1.5, 5.5, 9.5]))\n&gt;&gt;&gt; print(result)\n[0. 1. 2.]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check bin intervals\n&gt;&gt;&gt; print(binner.n_bins_)\n3\n&gt;&gt;&gt; assert len(binner.intervals_) == 3\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use fit_transform for one-step operation\n&gt;&gt;&gt; X2 = np.array([10, 20, 30, 40, 50])\n&gt;&gt;&gt; binner2 = QuantileBinner(n_bins=2)\n&gt;&gt;&gt; bins = binner2.fit_transform(X2)\n&gt;&gt;&gt; print(bins)\n[0. 0. 1. 1. 1.]\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfit\nLearn bin edges based on quantiles from training data.\n\n\nfit_transform\nFit to data, then transform it.\n\n\nget_params\nGet parameters of the quantile binner.\n\n\nset_params\nSet parameters of the QuantileBinner.\n\n\ntransform\nAssign new data to learned bins.\n\n\n\n\n\npreprocessing._binner.QuantileBinner.fit(X, y=None)\nLearn bin edges based on quantiles from training data.\nComputes quantile-based bin edges using numpy.percentile. If the dataset contains more samples than subsample, a random subset is used. Duplicate edges (which can occur with repeated values) are removed automatically.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nTraining data (1D numpy array) for computing quantiles.\nrequired\n\n\ny\nobject\nIgnored.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nobject\nSelf for method chaining.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf input data X is empty.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit with basic data\n&gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; _ = binner.fit(X)\n&gt;&gt;&gt; print(binner.n_bins_)\n3\n&gt;&gt;&gt; print(len(binner.bin_edges_))\n4\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit with repeated values (may reduce number of bins)\n&gt;&gt;&gt; X_repeated = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3])\n&gt;&gt;&gt; binner2 = QuantileBinner(n_bins=5)\n&gt;&gt;&gt; _ = binner2.fit(X_repeated)\n&gt;&gt;&gt; # n_bins_ may be less than 5 due to duplicates\n&gt;&gt;&gt; assert binner2.n_bins_ &lt;= 5\n\n\n\n\npreprocessing._binner.QuantileBinner.fit_transform(X, y=None, **fit_params)\nFit to data, then transform it.\nFits transformer to X and y with optional parameters fit_params and returns a transformed version of X.\n\n\nX : array-like of shape (n_samples, n_features) Input samples.\n\n\n\nTarget values (None for unsupervised transformations).\n**fit_params : dict Additional fit parameters.\n\n\n\nX_new : ndarray array of shape (n_samples, n_features_new) Transformed array.\n\n\n\n\npreprocessing._binner.QuantileBinner.get_params(deep=True)\nGet parameters of the quantile binner.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict[str, Any]\nDictionary containing n_bins, method, subsample, dtype, and\n\n\n\ndict[str, Any]\nrandom_state parameters.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=5, method='median_unbiased', subsample=1000)\n&gt;&gt;&gt; params = binner.get_params()\n&gt;&gt;&gt; print(params['n_bins'])\n5\n&gt;&gt;&gt; print(params['method'])\nmedian_unbiased\n&gt;&gt;&gt; print(params['subsample'])\n1000\n\n\n\n\npreprocessing._binner.QuantileBinner.set_params(**params)\nSet parameters of the QuantileBinner.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n**params\nAny\nParameter names and values to set as keyword arguments.\n{}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nself\n'QuantileBinner'\nReturns the updated QuantileBinner instance.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; print(binner.n_bins)\n3\n&gt;&gt;&gt; binner.set_params(n_bins=5, method='weibull')\n&gt;&gt;&gt; print(binner.n_bins)\n5\n&gt;&gt;&gt; print(binner.method)\nweibull\n\n\n\n\npreprocessing._binner.QuantileBinner.transform(X, y=None)\nAssign new data to learned bins.\nUses numpy.searchsorted for efficient bin assignment. Values are assigned to bins following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside the fitted range are clipped to the first or last bin.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nData to assign to bins (1D numpy array).\nrequired\n\n\ny\nobject\nIgnored.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nBin indices as numpy array with dtype specified in init.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNotFittedError\nIf fit() has not been called yet.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit and transform\n&gt;&gt;&gt; X_train = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; _ = binner.fit(X_train)\n&gt;&gt;&gt;\n&gt;&gt;&gt; X_test = np.array([1.5, 5.5, 9.5])\n&gt;&gt;&gt; result = binner.transform(X_test)\n&gt;&gt;&gt; print(result)\n[0. 1. 2.]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Values outside range are clipped\n&gt;&gt;&gt; X_extreme = np.array([0, 100])\n&gt;&gt;&gt; result_extreme = binner.transform(X_extreme)\n&gt;&gt;&gt; print(result_extreme)  # Both clipped to valid bin indices\n[0. 2.]",
    "crumbs": [
      "API Reference",
      "Preprocessing",
      "_binner"
    ]
  },
  {
    "objectID": "docs/reference/preprocessing._binner.html#classes",
    "href": "docs/reference/preprocessing._binner.html#classes",
    "title": "preprocessing._binner",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nQuantileBinner\nBin data into quantile-based bins using numpy.percentile.\n\n\n\n\n\npreprocessing._binner.QuantileBinner(\n    n_bins,\n    method='linear',\n    subsample=200000,\n    dtype=np.float64,\n    random_state=789654,\n)\nBin data into quantile-based bins using numpy.percentile.\nThis class is similar to sklearn’s KBinsDiscretizer but optimized for performance using numpy.searchsorted for fast bin assignment. Bin intervals are defined following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside the range are clipped to the first or last bin.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn_bins\nint\nThe number of quantile-based bins to create. Must be &gt;= 2.\nrequired\n\n\nmethod\nstr\nThe method used to compute quantiles, passed to numpy.percentile. Default is ‘linear’. Valid values: “inverse_cdf”, “averaged_inverse_cdf”, “closest_observation”, “interpolated_inverse_cdf”, “hazen”, “weibull”, “linear”, “median_unbiased”, “normal_unbiased”.\n'linear'\n\n\nsubsample\nint\nMaximum number of samples for computing quantiles. If dataset has more samples, a random subset is used. Default 200000.\n200000\n\n\ndtype\ntype\nData type for bin indices. Default is numpy.float64.\nnp.float64\n\n\nrandom_state\nint\nRandom seed for subset generation. Default 789654.\n789654\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nn_bins\nint\nNumber of bins to create.\n\n\nmethod\nstr\nQuantile computation method.\n\n\nsubsample\nint\nMaximum samples for quantile computation.\n\n\ndtype\ntype\nData type for bin indices.\n\n\nrandom_state\nint\nRandom seed.\n\n\nn_bins_\nint\nActual number of bins after fitting (may differ from n_bins if duplicate edges are found).\n\n\nbin_edges_\nnp.ndarray\nEdges of the bins learned during fitting.\n\n\ninternal_edges_\nnp.ndarray\nInternal edges for optimized bin assignment.\n\n\nintervals_\ndict\nMapping from bin index to (lower, upper) interval bounds.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Basic usage: create 3 quantile bins\n&gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; _ = binner.fit(X)\n&gt;&gt;&gt; result = binner.transform(np.array([1.5, 5.5, 9.5]))\n&gt;&gt;&gt; print(result)\n[0. 1. 2.]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check bin intervals\n&gt;&gt;&gt; print(binner.n_bins_)\n3\n&gt;&gt;&gt; assert len(binner.intervals_) == 3\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use fit_transform for one-step operation\n&gt;&gt;&gt; X2 = np.array([10, 20, 30, 40, 50])\n&gt;&gt;&gt; binner2 = QuantileBinner(n_bins=2)\n&gt;&gt;&gt; bins = binner2.fit_transform(X2)\n&gt;&gt;&gt; print(bins)\n[0. 0. 1. 1. 1.]\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfit\nLearn bin edges based on quantiles from training data.\n\n\nfit_transform\nFit to data, then transform it.\n\n\nget_params\nGet parameters of the quantile binner.\n\n\nset_params\nSet parameters of the QuantileBinner.\n\n\ntransform\nAssign new data to learned bins.\n\n\n\n\n\npreprocessing._binner.QuantileBinner.fit(X, y=None)\nLearn bin edges based on quantiles from training data.\nComputes quantile-based bin edges using numpy.percentile. If the dataset contains more samples than subsample, a random subset is used. Duplicate edges (which can occur with repeated values) are removed automatically.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nTraining data (1D numpy array) for computing quantiles.\nrequired\n\n\ny\nobject\nIgnored.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nobject\nSelf for method chaining.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf input data X is empty.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit with basic data\n&gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; _ = binner.fit(X)\n&gt;&gt;&gt; print(binner.n_bins_)\n3\n&gt;&gt;&gt; print(len(binner.bin_edges_))\n4\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit with repeated values (may reduce number of bins)\n&gt;&gt;&gt; X_repeated = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3])\n&gt;&gt;&gt; binner2 = QuantileBinner(n_bins=5)\n&gt;&gt;&gt; _ = binner2.fit(X_repeated)\n&gt;&gt;&gt; # n_bins_ may be less than 5 due to duplicates\n&gt;&gt;&gt; assert binner2.n_bins_ &lt;= 5\n\n\n\n\npreprocessing._binner.QuantileBinner.fit_transform(X, y=None, **fit_params)\nFit to data, then transform it.\nFits transformer to X and y with optional parameters fit_params and returns a transformed version of X.\n\n\nX : array-like of shape (n_samples, n_features) Input samples.\n\n\n\nTarget values (None for unsupervised transformations).\n**fit_params : dict Additional fit parameters.\n\n\n\nX_new : ndarray array of shape (n_samples, n_features_new) Transformed array.\n\n\n\n\npreprocessing._binner.QuantileBinner.get_params(deep=True)\nGet parameters of the quantile binner.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict[str, Any]\nDictionary containing n_bins, method, subsample, dtype, and\n\n\n\ndict[str, Any]\nrandom_state parameters.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=5, method='median_unbiased', subsample=1000)\n&gt;&gt;&gt; params = binner.get_params()\n&gt;&gt;&gt; print(params['n_bins'])\n5\n&gt;&gt;&gt; print(params['method'])\nmedian_unbiased\n&gt;&gt;&gt; print(params['subsample'])\n1000\n\n\n\n\npreprocessing._binner.QuantileBinner.set_params(**params)\nSet parameters of the QuantileBinner.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n**params\nAny\nParameter names and values to set as keyword arguments.\n{}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nself\n'QuantileBinner'\nReturns the updated QuantileBinner instance.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; print(binner.n_bins)\n3\n&gt;&gt;&gt; binner.set_params(n_bins=5, method='weibull')\n&gt;&gt;&gt; print(binner.n_bins)\n5\n&gt;&gt;&gt; print(binner.method)\nweibull\n\n\n\n\npreprocessing._binner.QuantileBinner.transform(X, y=None)\nAssign new data to learned bins.\nUses numpy.searchsorted for efficient bin assignment. Values are assigned to bins following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside the fitted range are clipped to the first or last bin.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nData to assign to bins (1D numpy array).\nrequired\n\n\ny\nobject\nIgnored.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nBin indices as numpy array with dtype specified in init.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNotFittedError\nIf fit() has not been called yet.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit and transform\n&gt;&gt;&gt; X_train = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; _ = binner.fit(X_train)\n&gt;&gt;&gt;\n&gt;&gt;&gt; X_test = np.array([1.5, 5.5, 9.5])\n&gt;&gt;&gt; result = binner.transform(X_test)\n&gt;&gt;&gt; print(result)\n[0. 1. 2.]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Values outside range are clipped\n&gt;&gt;&gt; X_extreme = np.array([0, 100])\n&gt;&gt;&gt; result_extreme = binner.transform(X_extreme)\n&gt;&gt;&gt; print(result_extreme)  # Both clipped to valid bin indices\n[0. 2.]",
    "crumbs": [
      "API Reference",
      "Preprocessing",
      "_binner"
    ]
  },
  {
    "objectID": "docs/reference/model_selection.utils_common.html",
    "href": "docs/reference/model_selection.utils_common.html",
    "title": "model_selection.utils_common",
    "section": "",
    "text": "model_selection.utils_common\nCommon validation and initialization utilities for model selection.\n\n\n\n\n\nName\nDescription\n\n\n\n\ncheck_backtesting_input\nThis is a helper function to check most inputs of backtesting functions in\n\n\ncheck_one_step_ahead_input\nThis is a helper function to check most inputs of hyperparameter tuning\n\n\ninitialize_lags_grid\nInitialize lags grid and lags label for model selection.\n\n\nselect_n_jobs_backtesting\nSelect the optimal number of jobs to use in the backtesting process. This\n\n\n\n\n\nmodel_selection.utils_common.check_backtesting_input(\n    forecaster,\n    cv,\n    metric,\n    add_aggregated_metric=True,\n    y=None,\n    series=None,\n    exog=None,\n    interval=None,\n    interval_method='bootstrapping',\n    alpha=None,\n    n_boot=250,\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,\n    random_state=123,\n    return_predictors=False,\n    freeze_params=True,\n    n_jobs='auto',\n    show_progress=True,\n    suppress_warnings=False,\n)\nThis is a helper function to check most inputs of backtesting functions in modules model_selection.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nforecaster\nobject\nForecaster model.\nrequired\n\n\ncv\nobject\nTimeSeriesFold object with the information needed to split the data into folds.\nrequired\n\n\nmetric\nstr | Callable | list[str | Callable]\nMetric used to quantify the goodness of fit of the model.\nrequired\n\n\nadd_aggregated_metric\nbool\nIf True, the aggregated metrics (average, weighted average and pooling) over all levels are also returned (only multiseries).\nTrue\n\n\ny\npd.Series | None\nTraining time series for uni-series forecasters.\nNone\n\n\nseries\npd.DataFrame | dict[str, pd.Series | pd.DataFrame]\nTraining time series for multi-series forecasters.\nNone\n\n\nexog\npd.Series | pd.DataFrame | dict[str, pd.Series | pd.DataFrame] | None\nExogenous variables.\nNone\n\n\ninterval\nfloat | list[float] | tuple[float] | str | object | None\nSpecifies whether probabilistic predictions should be estimated and the method to use. The following options are supported: - If float, represents the nominal (expected) coverage (between 0 and 1). For instance, interval=0.95 corresponds to [2.5, 97.5] percentiles. - If list or tuple: Sequence of percentiles to compute, each value must be between 0 and 100 inclusive. For example, a 95% confidence interval can be specified as interval = [2.5, 97.5] or multiple percentiles (e.g. 10, 50 and 90) as interval = [10, 50, 90]. - If ‘bootstrapping’ (str): n_boot bootstrapping predictions will be generated. - If scipy.stats distribution object, the distribution parameters will be estimated for each prediction. - If None, no probabilistic predictions are estimated.\nNone\n\n\ninterval_method\nstr\nTechnique used to estimate prediction intervals. Available options: - ‘bootstrapping’: Bootstrapping is used to generate prediction intervals. - ‘conformal’: Employs the conformal prediction split method for interval estimation.\n'bootstrapping'\n\n\nalpha\nfloat | None\nThe confidence intervals used in ForecasterStats are (1 - alpha) %.\nNone\n\n\nn_boot\nint\nNumber of bootstrapping iterations to perform when estimating prediction intervals.\n250\n\n\nuse_in_sample_residuals\nbool\nIf True, residuals from the training data are used as proxy of prediction error to create prediction intervals. If False, out_sample_residuals are used if they are already stored inside the forecaster.\nTrue\n\n\nuse_binned_residuals\nbool\nIf True, residuals are selected based on the predicted values (binned selection). If False, residuals are selected randomly.\nTrue\n\n\nrandom_state\nint\nSeed for the random number generator to ensure reproducibility.\n123\n\n\nreturn_predictors\nbool\nIf True, the predictors used to make the predictions are also returned.\nFalse\n\n\nn_jobs\nint | str\nThe number of jobs to run in parallel. If -1, then the number of jobs is set to the number of cores. If ‘auto’, n_jobs is set using the function select_n_jobs_fit_forecaster.\n'auto'\n\n\nfreeze_params\nbool\nDetermines whether to freeze the model parameters after the first fit for estimators that perform automatic model selection. - If True, the model parameters found during the first fit (e.g., order and seasonal_order for Arima, or smoothing parameters for Ets) are reused in all subsequent refits. This avoids re-running the automatic selection procedure in each fold and reduces runtime. - If False, automatic model selection is performed independently in each refit, allowing parameters to adapt across folds. This increases runtime and adds a params column to the output with the parameters selected per fold.\nTrue\n\n\nshow_progress\nbool\nWhether to show a progress bar.\nTrue\n\n\nsuppress_warnings\nbool\nIf True, spotforecast warnings will be suppressed during the backtesting process.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2.model_selection.utils_common import check_backtesting_input\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from spotforecast2.model_selection import TimeSeriesFold\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from sklearn.metrics import mean_squared_error\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; forecaster = ForecasterRecursive(LinearRegression(), lags=2)\n&gt;&gt;&gt; cv = TimeSeriesFold(\n...     steps=3,\n...     initial_train_size=5,\n...     gap=0,\n...     refit=False,\n...     fixed_train_size=False,\n...     allow_incomplete_fold=True\n... )\n&gt;&gt;&gt; check_backtesting_input(\n...     forecaster=forecaster,\n...     cv=cv,\n...     metric=mean_squared_error,\n...     y=y\n... )\n\n\n\n\nmodel_selection.utils_common.check_one_step_ahead_input(\n    forecaster,\n    cv,\n    metric,\n    y=None,\n    series=None,\n    exog=None,\n    show_progress=True,\n    suppress_warnings=False,\n)\nThis is a helper function to check most inputs of hyperparameter tuning functions in modules model_selection when using a OneStepAheadFold.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nforecaster\nobject\nForecaster model.\nrequired\n\n\ncv\nobject\nOneStepAheadFold object with the information needed to split the data into folds.\nrequired\n\n\nmetric\nstr | Callable | list[str | Callable]\nMetric used to quantify the goodness of fit of the model.\nrequired\n\n\ny\npd.Series | None\nTraining time series for uni-series forecasters.\nNone\n\n\nseries\npd.DataFrame | dict[str, pd.Series | pd.DataFrame]\nTraining time series for multi-series forecasters.\nNone\n\n\nexog\npd.Series | pd.DataFrame | dict[str, pd.Series | pd.DataFrame] | None\nExogenous variables.\nNone\n\n\nshow_progress\nbool\nWhether to show a progress bar.\nTrue\n\n\nsuppress_warnings\nbool\nIf True, spotforecast warnings will be suppressed during the hyperparameter search.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2.model_selection.utils_common import check_one_step_ahead_input\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from spotforecast2.model_selection import OneStepAheadFold\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from sklearn.metrics import mean_squared_error\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; forecaster = ForecasterRecursive(LinearRegression(), lags=2)\n&gt;&gt;&gt; cv = OneStepAheadFold(\n...     initial_train_size=5,\n...     return_all_predictions=False\n... )\n&gt;&gt;&gt; check_one_step_ahead_input(\n...     forecaster=forecaster,\n...     cv=cv,\n...     metric=mean_squared_error,\n...     y=y\n... )\n\n\n\n\nmodel_selection.utils_common.initialize_lags_grid(forecaster, lags_grid=None)\nInitialize lags grid and lags label for model selection.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nforecaster\nobject\nForecaster model. ForecasterRecursive, ForecasterDirect, ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate.\nrequired\n\n\nlags_grid\nlist[int | list[int] | np.ndarray[int] | range[int]] | dict[str, list[int | list[int] | np.ndarray[int] | range[int]]] | None\nLists of lags to try, containing int, lists, numpy ndarray, or range objects. If dict, the keys are used as labels in the results DataFrame, and the values are used as the lists of lags to try.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ntuple\ntuple[dict[str, int], str]\n(lags_grid, lags_label) - lags_grid (dict): Dictionary with lags configuration for each iteration. - lags_label (str): Label for lags representation in the results object.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotforecast2.model_selection.utils_common import initialize_lags_grid\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; forecaster = ForecasterRecursive(LinearRegression(), lags=2)\n&gt;&gt;&gt; lags_grid = [2, 4]\n&gt;&gt;&gt; lags_grid, lags_label = initialize_lags_grid(forecaster, lags_grid)\n&gt;&gt;&gt; print(lags_grid)\n{'2': 2, '4': 4}\n&gt;&gt;&gt; print(lags_label)\nvalues\n\n\n\n\nmodel_selection.utils_common.select_n_jobs_backtesting(forecaster, refit)\nSelect the optimal number of jobs to use in the backtesting process. This selection is based on heuristics and is not guaranteed to be optimal.\nThe number of jobs is chosen as follows:\n\nIf refit is an integer, then n_jobs = 1. This is because parallelization doesn’t work with intermittent refit.\nIf forecaster is ‘ForecasterRecursive’ and estimator is a linear estimator, then n_jobs = 1.\nIf forecaster is ‘ForecasterRecursive’ and estimator is not a linear estimator then n_jobs = cpu_count() - 1.\nIf forecaster is ‘ForecasterDirect’ or ‘ForecasterDirectMultiVariate’ and refit = True, then n_jobs = cpu_count() - 1.\nIf forecaster is ‘ForecasterDirect’ or ‘ForecasterDirectMultiVariate’ and refit = False, then n_jobs = 1.\nIf forecaster is ‘ForecasterRecursiveMultiSeries’, then n_jobs = cpu_count() - 1.\nIf forecaster is ‘ForecasterStats’ or ‘ForecasterEquivalentDate’, then n_jobs = 1.\nIf estimator is a LGBMRegressor(n_jobs=1), then n_jobs = cpu_count() - 1.\nIf estimator is a LGBMRegressor with internal n_jobs != 1, then n_jobs = 1. This is because lightgbm is highly optimized for gradient boosting and parallelizes operations at a very fine-grained level, making additional parallelization unnecessary and potentially harmful due to resource contention.\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nforecaster\nobject\nForecaster model.\nrequired\n\n\nrefit\nbool | int\nIf the forecaster is refitted during the backtesting process.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nint\nint\nThe number of jobs to run in parallel.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotforecast2.model_selection.utils_common import select_n_jobs_backtesting\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; forecaster = ForecasterRecursive(LinearRegression(), lags=2)\n&gt;&gt;&gt; select_n_jobs_backtesting(forecaster, refit=True)\n1",
    "crumbs": [
      "API Reference",
      "Model Selection",
      "utils_common"
    ]
  },
  {
    "objectID": "docs/reference/model_selection.utils_common.html#functions",
    "href": "docs/reference/model_selection.utils_common.html#functions",
    "title": "model_selection.utils_common",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncheck_backtesting_input\nThis is a helper function to check most inputs of backtesting functions in\n\n\ncheck_one_step_ahead_input\nThis is a helper function to check most inputs of hyperparameter tuning\n\n\ninitialize_lags_grid\nInitialize lags grid and lags label for model selection.\n\n\nselect_n_jobs_backtesting\nSelect the optimal number of jobs to use in the backtesting process. This\n\n\n\n\n\nmodel_selection.utils_common.check_backtesting_input(\n    forecaster,\n    cv,\n    metric,\n    add_aggregated_metric=True,\n    y=None,\n    series=None,\n    exog=None,\n    interval=None,\n    interval_method='bootstrapping',\n    alpha=None,\n    n_boot=250,\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,\n    random_state=123,\n    return_predictors=False,\n    freeze_params=True,\n    n_jobs='auto',\n    show_progress=True,\n    suppress_warnings=False,\n)\nThis is a helper function to check most inputs of backtesting functions in modules model_selection.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nforecaster\nobject\nForecaster model.\nrequired\n\n\ncv\nobject\nTimeSeriesFold object with the information needed to split the data into folds.\nrequired\n\n\nmetric\nstr | Callable | list[str | Callable]\nMetric used to quantify the goodness of fit of the model.\nrequired\n\n\nadd_aggregated_metric\nbool\nIf True, the aggregated metrics (average, weighted average and pooling) over all levels are also returned (only multiseries).\nTrue\n\n\ny\npd.Series | None\nTraining time series for uni-series forecasters.\nNone\n\n\nseries\npd.DataFrame | dict[str, pd.Series | pd.DataFrame]\nTraining time series for multi-series forecasters.\nNone\n\n\nexog\npd.Series | pd.DataFrame | dict[str, pd.Series | pd.DataFrame] | None\nExogenous variables.\nNone\n\n\ninterval\nfloat | list[float] | tuple[float] | str | object | None\nSpecifies whether probabilistic predictions should be estimated and the method to use. The following options are supported: - If float, represents the nominal (expected) coverage (between 0 and 1). For instance, interval=0.95 corresponds to [2.5, 97.5] percentiles. - If list or tuple: Sequence of percentiles to compute, each value must be between 0 and 100 inclusive. For example, a 95% confidence interval can be specified as interval = [2.5, 97.5] or multiple percentiles (e.g. 10, 50 and 90) as interval = [10, 50, 90]. - If ‘bootstrapping’ (str): n_boot bootstrapping predictions will be generated. - If scipy.stats distribution object, the distribution parameters will be estimated for each prediction. - If None, no probabilistic predictions are estimated.\nNone\n\n\ninterval_method\nstr\nTechnique used to estimate prediction intervals. Available options: - ‘bootstrapping’: Bootstrapping is used to generate prediction intervals. - ‘conformal’: Employs the conformal prediction split method for interval estimation.\n'bootstrapping'\n\n\nalpha\nfloat | None\nThe confidence intervals used in ForecasterStats are (1 - alpha) %.\nNone\n\n\nn_boot\nint\nNumber of bootstrapping iterations to perform when estimating prediction intervals.\n250\n\n\nuse_in_sample_residuals\nbool\nIf True, residuals from the training data are used as proxy of prediction error to create prediction intervals. If False, out_sample_residuals are used if they are already stored inside the forecaster.\nTrue\n\n\nuse_binned_residuals\nbool\nIf True, residuals are selected based on the predicted values (binned selection). If False, residuals are selected randomly.\nTrue\n\n\nrandom_state\nint\nSeed for the random number generator to ensure reproducibility.\n123\n\n\nreturn_predictors\nbool\nIf True, the predictors used to make the predictions are also returned.\nFalse\n\n\nn_jobs\nint | str\nThe number of jobs to run in parallel. If -1, then the number of jobs is set to the number of cores. If ‘auto’, n_jobs is set using the function select_n_jobs_fit_forecaster.\n'auto'\n\n\nfreeze_params\nbool\nDetermines whether to freeze the model parameters after the first fit for estimators that perform automatic model selection. - If True, the model parameters found during the first fit (e.g., order and seasonal_order for Arima, or smoothing parameters for Ets) are reused in all subsequent refits. This avoids re-running the automatic selection procedure in each fold and reduces runtime. - If False, automatic model selection is performed independently in each refit, allowing parameters to adapt across folds. This increases runtime and adds a params column to the output with the parameters selected per fold.\nTrue\n\n\nshow_progress\nbool\nWhether to show a progress bar.\nTrue\n\n\nsuppress_warnings\nbool\nIf True, spotforecast warnings will be suppressed during the backtesting process.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2.model_selection.utils_common import check_backtesting_input\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from spotforecast2.model_selection import TimeSeriesFold\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from sklearn.metrics import mean_squared_error\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; forecaster = ForecasterRecursive(LinearRegression(), lags=2)\n&gt;&gt;&gt; cv = TimeSeriesFold(\n...     steps=3,\n...     initial_train_size=5,\n...     gap=0,\n...     refit=False,\n...     fixed_train_size=False,\n...     allow_incomplete_fold=True\n... )\n&gt;&gt;&gt; check_backtesting_input(\n...     forecaster=forecaster,\n...     cv=cv,\n...     metric=mean_squared_error,\n...     y=y\n... )\n\n\n\n\nmodel_selection.utils_common.check_one_step_ahead_input(\n    forecaster,\n    cv,\n    metric,\n    y=None,\n    series=None,\n    exog=None,\n    show_progress=True,\n    suppress_warnings=False,\n)\nThis is a helper function to check most inputs of hyperparameter tuning functions in modules model_selection when using a OneStepAheadFold.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nforecaster\nobject\nForecaster model.\nrequired\n\n\ncv\nobject\nOneStepAheadFold object with the information needed to split the data into folds.\nrequired\n\n\nmetric\nstr | Callable | list[str | Callable]\nMetric used to quantify the goodness of fit of the model.\nrequired\n\n\ny\npd.Series | None\nTraining time series for uni-series forecasters.\nNone\n\n\nseries\npd.DataFrame | dict[str, pd.Series | pd.DataFrame]\nTraining time series for multi-series forecasters.\nNone\n\n\nexog\npd.Series | pd.DataFrame | dict[str, pd.Series | pd.DataFrame] | None\nExogenous variables.\nNone\n\n\nshow_progress\nbool\nWhether to show a progress bar.\nTrue\n\n\nsuppress_warnings\nbool\nIf True, spotforecast warnings will be suppressed during the hyperparameter search.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2.model_selection.utils_common import check_one_step_ahead_input\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from spotforecast2.model_selection import OneStepAheadFold\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from sklearn.metrics import mean_squared_error\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; forecaster = ForecasterRecursive(LinearRegression(), lags=2)\n&gt;&gt;&gt; cv = OneStepAheadFold(\n...     initial_train_size=5,\n...     return_all_predictions=False\n... )\n&gt;&gt;&gt; check_one_step_ahead_input(\n...     forecaster=forecaster,\n...     cv=cv,\n...     metric=mean_squared_error,\n...     y=y\n... )\n\n\n\n\nmodel_selection.utils_common.initialize_lags_grid(forecaster, lags_grid=None)\nInitialize lags grid and lags label for model selection.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nforecaster\nobject\nForecaster model. ForecasterRecursive, ForecasterDirect, ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate.\nrequired\n\n\nlags_grid\nlist[int | list[int] | np.ndarray[int] | range[int]] | dict[str, list[int | list[int] | np.ndarray[int] | range[int]]] | None\nLists of lags to try, containing int, lists, numpy ndarray, or range objects. If dict, the keys are used as labels in the results DataFrame, and the values are used as the lists of lags to try.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ntuple\ntuple[dict[str, int], str]\n(lags_grid, lags_label) - lags_grid (dict): Dictionary with lags configuration for each iteration. - lags_label (str): Label for lags representation in the results object.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotforecast2.model_selection.utils_common import initialize_lags_grid\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; forecaster = ForecasterRecursive(LinearRegression(), lags=2)\n&gt;&gt;&gt; lags_grid = [2, 4]\n&gt;&gt;&gt; lags_grid, lags_label = initialize_lags_grid(forecaster, lags_grid)\n&gt;&gt;&gt; print(lags_grid)\n{'2': 2, '4': 4}\n&gt;&gt;&gt; print(lags_label)\nvalues\n\n\n\n\nmodel_selection.utils_common.select_n_jobs_backtesting(forecaster, refit)\nSelect the optimal number of jobs to use in the backtesting process. This selection is based on heuristics and is not guaranteed to be optimal.\nThe number of jobs is chosen as follows:\n\nIf refit is an integer, then n_jobs = 1. This is because parallelization doesn’t work with intermittent refit.\nIf forecaster is ‘ForecasterRecursive’ and estimator is a linear estimator, then n_jobs = 1.\nIf forecaster is ‘ForecasterRecursive’ and estimator is not a linear estimator then n_jobs = cpu_count() - 1.\nIf forecaster is ‘ForecasterDirect’ or ‘ForecasterDirectMultiVariate’ and refit = True, then n_jobs = cpu_count() - 1.\nIf forecaster is ‘ForecasterDirect’ or ‘ForecasterDirectMultiVariate’ and refit = False, then n_jobs = 1.\nIf forecaster is ‘ForecasterRecursiveMultiSeries’, then n_jobs = cpu_count() - 1.\nIf forecaster is ‘ForecasterStats’ or ‘ForecasterEquivalentDate’, then n_jobs = 1.\nIf estimator is a LGBMRegressor(n_jobs=1), then n_jobs = cpu_count() - 1.\nIf estimator is a LGBMRegressor with internal n_jobs != 1, then n_jobs = 1. This is because lightgbm is highly optimized for gradient boosting and parallelizes operations at a very fine-grained level, making additional parallelization unnecessary and potentially harmful due to resource contention.\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nforecaster\nobject\nForecaster model.\nrequired\n\n\nrefit\nbool | int\nIf the forecaster is refitted during the backtesting process.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nint\nint\nThe number of jobs to run in parallel.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotforecast2.model_selection.utils_common import select_n_jobs_backtesting\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; forecaster = ForecasterRecursive(LinearRegression(), lags=2)\n&gt;&gt;&gt; select_n_jobs_backtesting(forecaster, refit=True)\n1",
    "crumbs": [
      "API Reference",
      "Model Selection",
      "utils_common"
    ]
  },
  {
    "objectID": "docs/reference/model_selection.split_ts_cv.html",
    "href": "docs/reference/model_selection.split_ts_cv.html",
    "title": "model_selection.split_ts_cv",
    "section": "",
    "text": "model_selection.split_ts_cv\nTime series cross-validation splitting.\n\n\n\n\n\nName\nDescription\n\n\n\n\nTimeSeriesFold\nClass to split time series data into train and test folds.\n\n\n\n\n\nmodel_selection.split_ts_cv.TimeSeriesFold(\n    steps,\n    initial_train_size=None,\n    fold_stride=None,\n    window_size=None,\n    differentiation=None,\n    refit=False,\n    fixed_train_size=True,\n    gap=0,\n    skip_folds=None,\n    allow_incomplete_fold=True,\n    return_all_indexes=False,\n    verbose=True,\n)\nClass to split time series data into train and test folds.\nWhen used within a backtesting or hyperparameter search, the arguments ‘initial_train_size’, ‘window_size’ and ‘differentiation’ are not required as they are automatically set by the backtesting or hyperparameter search functions.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsteps\nint\nNumber of observations used to be predicted in each fold. This is also commonly referred to as the forecast horizon or test size.\nrequired\n\n\ninitial_train_size\nint | str | pd.Timestamp | None\nNumber of observations used for initial training. - If None or 0, the initial forecaster is not trained in the first fold. - If an integer, the number of observations used for initial training. - If a date string or pandas Timestamp, it is the last date included in the initial training set. Defaults to None.\nNone\n\n\nfold_stride\nint | None\nNumber of observations that the start of the test set advances between consecutive folds. - If None, it defaults to the same value as steps, meaning that folds are placed back-to-back without overlap. - If fold_stride &lt; steps, test sets overlap and multiple forecasts will be generated for the same observations. - If fold_stride &gt; steps, gaps are left between consecutive test sets. Defaults to None.\nNone\n\n\nwindow_size\nint | None\nNumber of observations needed to generate the autoregressive predictors. Defaults to None.\nNone\n\n\ndifferentiation\nint | None\nNumber of observations to use for differentiation. This is used to extend the last_window as many observations as the differentiation order. Defaults to None.\nNone\n\n\nrefit\nbool | int\nWhether to refit the forecaster in each fold. - If True, the forecaster is refitted in each fold. - If False, the forecaster is trained only in the first fold. - If an integer, the forecaster is trained in the first fold and then refitted every refit folds. Defaults to False.\nFalse\n\n\nfixed_train_size\nbool\nWhether the training size is fixed or increases in each fold. Defaults to True.\nTrue\n\n\ngap\nint\nNumber of observations between the end of the training set and the start of the test set. Defaults to 0.\n0\n\n\nskip_folds\nint | list[int] | None\nNumber of folds to skip. - If an integer, every ‘skip_folds’-th is returned. - If a list, the indexes of the folds to skip. For example, if skip_folds=3 and there are 10 folds, the returned folds are 0, 3, 6, and 9. If skip_folds=[1, 2, 3], the returned folds are 0, 4, 5, 6, 7, 8, and 9. Defaults to None.\nNone\n\n\nallow_incomplete_fold\nbool\nWhether to allow the last fold to include fewer observations than steps. If False, the last fold is excluded if it is incomplete. Defaults to True.\nTrue\n\n\nreturn_all_indexes\nbool\nWhether to return all indexes or only the start and end indexes of each fold. Defaults to False.\nFalse\n\n\nverbose\nbool\nWhether to print information about generated folds. Defaults to True.\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nsteps\n\nNumber of observations used to be predicted in each fold.\n\n\ninitial_train_size\n\nNumber of observations used for initial training. If None or 0, the initial forecaster is not trained in the first fold.\n\n\nfold_stride\n\nNumber of observations that the start of the test set advances between consecutive folds.\n\n\noverlapping_folds\n\nWhether the folds overlap.\n\n\nwindow_size\n\nNumber of observations needed to generate the autoregressive predictors.\n\n\ndifferentiation\n\nNumber of observations to use for differentiation. This is used to extend the last_window as many observations as the differentiation order.\n\n\nrefit\n\nWhether to refit the forecaster in each fold.\n\n\nfixed_train_size\n\nWhether the training size is fixed or increases in each fold.\n\n\ngap\n\nNumber of observations between the end of the training set and the start of the test set.\n\n\nskip_folds\n\nNumber of folds to skip.\n\n\nallow_incomplete_fold\n\nWhether to allow the last fold to include fewer observations than steps.\n\n\nreturn_all_indexes\n\nWhether to return all indexes or only the start and end indexes of each fold.\n\n\nverbose\n\nWhether to print information about generated folds.\n\n\n\n\n\n\nBasic usage with fixed train size:\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.model_selection import TimeSeriesFold\n&gt;&gt;&gt; # Create sample time series data\n&gt;&gt;&gt; dates = pd.date_range('2020-01-01', periods=100, freq='D')\n&gt;&gt;&gt; y = pd.Series(np.arange(100), index=dates)\n&gt;&gt;&gt; # Create fold splitter\n&gt;&gt;&gt; cv = TimeSeriesFold(\n...     steps=10,\n...     initial_train_size=50,\n...     refit=True,\n...     fixed_train_size=True\n... )\n&gt;&gt;&gt; # Get folds\n&gt;&gt;&gt; folds = cv.split(y)\n&gt;&gt;&gt; print(f\"Number of folds: {len(folds)}\")\nNumber of folds: 4\nOverlapping folds with custom stride:\n&gt;&gt;&gt; cv = TimeSeriesFold(\n...     steps=30,\n...     initial_train_size=50,\n...     fold_stride=7,\n...     fixed_train_size=False\n... )\n&gt;&gt;&gt; folds = cv.split(y)\n&gt;&gt;&gt; # First test fold covers [50, 80), second [57, 87), etc.\nReturn as pandas DataFrame:\n&gt;&gt;&gt; cv = TimeSeriesFold(steps=10, initial_train_size=50)\n&gt;&gt;&gt; folds_df = cv.split(y, as_pandas=True)\n&gt;&gt;&gt; print(folds_df.columns.tolist())\n['fold', 'train_start', 'train_end', 'last_window_start', 'last_window_end', 'test_start', 'test_end', 'test_start_with_gap', 'test_end_with_gap', 'fit_forecaster']\nSkip folds for faster evaluation:\n&gt;&gt;&gt; cv = TimeSeriesFold(\n...     steps=5,\n...     initial_train_size=50,\n...     skip_folds=2\n... )\n&gt;&gt;&gt; folds = cv.split(y)\n&gt;&gt;&gt; # Returns folds 0, 2, 4, 6, ...\n\n\n\nReturned values are the positions of the observations and not the actual values of the index, so they can be used to slice the data directly using iloc. For example, if the input series is X = [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], the initial_train_size = 3, window_size = 2, steps = 4, and gap = 1, the output of the first fold will: [0, [0, 3], [1, 3], [3, 8], [4, 8], True].\nThe first element is the fold number, the first list [0, 3] indicates that the training set goes from the first to the third observation. The second list [1, 3] indicates that the last window seen by the forecaster during training goes from the second to the third observation. The third list [3, 8] indicates that the test set goes from the fourth to the eighth observation. The fourth list [4, 8] indicates that the test set including the gap goes from the fifth to the eighth observation. The boolean False indicates that the forecaster should not be trained in this fold.\nFollowing the python convention, the start index is inclusive and the end index is exclusive. This means that the last index is not included in the slice.\nAs an example, with initial_train_size=50, steps=30, and fold_stride=7, the first test fold will cover observations [50, 80), the second fold [57, 87), and the third fold [64, 94). This configuration produces multiple forecasts for the same observations, which is often desirable in rolling-origin evaluation.\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nsplit\nSplit the time series data into train and test folds.\n\n\n\n\n\nmodel_selection.split_ts_cv.TimeSeriesFold.split(X, as_pandas=False)\nSplit the time series data into train and test folds.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\npd.Series | pd.DataFrame | pd.Index | dict[str, pd.Series | pd.DataFrame]\nTime series data or index to split. Can be a pandas Series, DataFrame, Index, or a dictionary of Series/DataFrames.\nrequired\n\n\nas_pandas\nbool\nIf True, the folds are returned as a DataFrame. This is useful to visualize the folds in a more interpretable way. Defaults to False.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist | pd.DataFrame\nA list of lists containing the indices (position) for each fold, or a\n\n\n\nlist | pd.DataFrame\nDataFrame if as_pandas=True. Each list contains 4 lists and a boolean\n\n\n\nlist | pd.DataFrame\nwith the following information:\n\n\n\nlist | pd.DataFrame\n- fold: fold number.\n\n\n\nlist | pd.DataFrame\n- [train_start, train_end]: list with the start and end positions of the training set.\n\n\n\nlist | pd.DataFrame\n- [last_window_start, last_window_end]: list with the start and end positions of the last window seen by the forecaster during training. The last window is used to generate the lags use as predictors. If differentiation is included, the interval is extended as many observations as the differentiation order. If the argument window_size is None, this list is empty.\n\n\n\nlist | pd.DataFrame\n- [test_start, test_end]: list with the start and end positions of the test set. These are the observations used to evaluate the forecaster.\n\n\n\nlist | pd.DataFrame\n- [test_start_with_gap, test_end_with_gap]: list with the start and end positions of the test set including the gap. The gap is the number of observations between the end of the training set and the start of the test set.\n\n\n\nlist | pd.DataFrame\n- fit_forecaster: boolean indicating whether the forecaster should be fitted in this fold.\n\n\n\n\n\n\nThe returned values are the positions of the observations and not the actual values of the index, so they can be used to slice the data directly using iloc.\nIf as_pandas is True, the folds are returned as a DataFrame with the following columns: ‘fold’, ‘train_start’, ‘train_end’, ‘last_window_start’, ‘last_window_end’, ‘test_start’, ‘test_end’, ‘test_start_with_gap’, ‘test_end_with_gap’, ‘fit_forecaster’.\nFollowing the python convention, the start index is inclusive and the end index is exclusive. This means that the last index is not included in the slice.",
    "crumbs": [
      "API Reference",
      "Model Selection",
      "split_ts_cv"
    ]
  },
  {
    "objectID": "docs/reference/model_selection.split_ts_cv.html#classes",
    "href": "docs/reference/model_selection.split_ts_cv.html#classes",
    "title": "model_selection.split_ts_cv",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nTimeSeriesFold\nClass to split time series data into train and test folds.\n\n\n\n\n\nmodel_selection.split_ts_cv.TimeSeriesFold(\n    steps,\n    initial_train_size=None,\n    fold_stride=None,\n    window_size=None,\n    differentiation=None,\n    refit=False,\n    fixed_train_size=True,\n    gap=0,\n    skip_folds=None,\n    allow_incomplete_fold=True,\n    return_all_indexes=False,\n    verbose=True,\n)\nClass to split time series data into train and test folds.\nWhen used within a backtesting or hyperparameter search, the arguments ‘initial_train_size’, ‘window_size’ and ‘differentiation’ are not required as they are automatically set by the backtesting or hyperparameter search functions.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsteps\nint\nNumber of observations used to be predicted in each fold. This is also commonly referred to as the forecast horizon or test size.\nrequired\n\n\ninitial_train_size\nint | str | pd.Timestamp | None\nNumber of observations used for initial training. - If None or 0, the initial forecaster is not trained in the first fold. - If an integer, the number of observations used for initial training. - If a date string or pandas Timestamp, it is the last date included in the initial training set. Defaults to None.\nNone\n\n\nfold_stride\nint | None\nNumber of observations that the start of the test set advances between consecutive folds. - If None, it defaults to the same value as steps, meaning that folds are placed back-to-back without overlap. - If fold_stride &lt; steps, test sets overlap and multiple forecasts will be generated for the same observations. - If fold_stride &gt; steps, gaps are left between consecutive test sets. Defaults to None.\nNone\n\n\nwindow_size\nint | None\nNumber of observations needed to generate the autoregressive predictors. Defaults to None.\nNone\n\n\ndifferentiation\nint | None\nNumber of observations to use for differentiation. This is used to extend the last_window as many observations as the differentiation order. Defaults to None.\nNone\n\n\nrefit\nbool | int\nWhether to refit the forecaster in each fold. - If True, the forecaster is refitted in each fold. - If False, the forecaster is trained only in the first fold. - If an integer, the forecaster is trained in the first fold and then refitted every refit folds. Defaults to False.\nFalse\n\n\nfixed_train_size\nbool\nWhether the training size is fixed or increases in each fold. Defaults to True.\nTrue\n\n\ngap\nint\nNumber of observations between the end of the training set and the start of the test set. Defaults to 0.\n0\n\n\nskip_folds\nint | list[int] | None\nNumber of folds to skip. - If an integer, every ‘skip_folds’-th is returned. - If a list, the indexes of the folds to skip. For example, if skip_folds=3 and there are 10 folds, the returned folds are 0, 3, 6, and 9. If skip_folds=[1, 2, 3], the returned folds are 0, 4, 5, 6, 7, 8, and 9. Defaults to None.\nNone\n\n\nallow_incomplete_fold\nbool\nWhether to allow the last fold to include fewer observations than steps. If False, the last fold is excluded if it is incomplete. Defaults to True.\nTrue\n\n\nreturn_all_indexes\nbool\nWhether to return all indexes or only the start and end indexes of each fold. Defaults to False.\nFalse\n\n\nverbose\nbool\nWhether to print information about generated folds. Defaults to True.\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nsteps\n\nNumber of observations used to be predicted in each fold.\n\n\ninitial_train_size\n\nNumber of observations used for initial training. If None or 0, the initial forecaster is not trained in the first fold.\n\n\nfold_stride\n\nNumber of observations that the start of the test set advances between consecutive folds.\n\n\noverlapping_folds\n\nWhether the folds overlap.\n\n\nwindow_size\n\nNumber of observations needed to generate the autoregressive predictors.\n\n\ndifferentiation\n\nNumber of observations to use for differentiation. This is used to extend the last_window as many observations as the differentiation order.\n\n\nrefit\n\nWhether to refit the forecaster in each fold.\n\n\nfixed_train_size\n\nWhether the training size is fixed or increases in each fold.\n\n\ngap\n\nNumber of observations between the end of the training set and the start of the test set.\n\n\nskip_folds\n\nNumber of folds to skip.\n\n\nallow_incomplete_fold\n\nWhether to allow the last fold to include fewer observations than steps.\n\n\nreturn_all_indexes\n\nWhether to return all indexes or only the start and end indexes of each fold.\n\n\nverbose\n\nWhether to print information about generated folds.\n\n\n\n\n\n\nBasic usage with fixed train size:\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.model_selection import TimeSeriesFold\n&gt;&gt;&gt; # Create sample time series data\n&gt;&gt;&gt; dates = pd.date_range('2020-01-01', periods=100, freq='D')\n&gt;&gt;&gt; y = pd.Series(np.arange(100), index=dates)\n&gt;&gt;&gt; # Create fold splitter\n&gt;&gt;&gt; cv = TimeSeriesFold(\n...     steps=10,\n...     initial_train_size=50,\n...     refit=True,\n...     fixed_train_size=True\n... )\n&gt;&gt;&gt; # Get folds\n&gt;&gt;&gt; folds = cv.split(y)\n&gt;&gt;&gt; print(f\"Number of folds: {len(folds)}\")\nNumber of folds: 4\nOverlapping folds with custom stride:\n&gt;&gt;&gt; cv = TimeSeriesFold(\n...     steps=30,\n...     initial_train_size=50,\n...     fold_stride=7,\n...     fixed_train_size=False\n... )\n&gt;&gt;&gt; folds = cv.split(y)\n&gt;&gt;&gt; # First test fold covers [50, 80), second [57, 87), etc.\nReturn as pandas DataFrame:\n&gt;&gt;&gt; cv = TimeSeriesFold(steps=10, initial_train_size=50)\n&gt;&gt;&gt; folds_df = cv.split(y, as_pandas=True)\n&gt;&gt;&gt; print(folds_df.columns.tolist())\n['fold', 'train_start', 'train_end', 'last_window_start', 'last_window_end', 'test_start', 'test_end', 'test_start_with_gap', 'test_end_with_gap', 'fit_forecaster']\nSkip folds for faster evaluation:\n&gt;&gt;&gt; cv = TimeSeriesFold(\n...     steps=5,\n...     initial_train_size=50,\n...     skip_folds=2\n... )\n&gt;&gt;&gt; folds = cv.split(y)\n&gt;&gt;&gt; # Returns folds 0, 2, 4, 6, ...\n\n\n\nReturned values are the positions of the observations and not the actual values of the index, so they can be used to slice the data directly using iloc. For example, if the input series is X = [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], the initial_train_size = 3, window_size = 2, steps = 4, and gap = 1, the output of the first fold will: [0, [0, 3], [1, 3], [3, 8], [4, 8], True].\nThe first element is the fold number, the first list [0, 3] indicates that the training set goes from the first to the third observation. The second list [1, 3] indicates that the last window seen by the forecaster during training goes from the second to the third observation. The third list [3, 8] indicates that the test set goes from the fourth to the eighth observation. The fourth list [4, 8] indicates that the test set including the gap goes from the fifth to the eighth observation. The boolean False indicates that the forecaster should not be trained in this fold.\nFollowing the python convention, the start index is inclusive and the end index is exclusive. This means that the last index is not included in the slice.\nAs an example, with initial_train_size=50, steps=30, and fold_stride=7, the first test fold will cover observations [50, 80), the second fold [57, 87), and the third fold [64, 94). This configuration produces multiple forecasts for the same observations, which is often desirable in rolling-origin evaluation.\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nsplit\nSplit the time series data into train and test folds.\n\n\n\n\n\nmodel_selection.split_ts_cv.TimeSeriesFold.split(X, as_pandas=False)\nSplit the time series data into train and test folds.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\npd.Series | pd.DataFrame | pd.Index | dict[str, pd.Series | pd.DataFrame]\nTime series data or index to split. Can be a pandas Series, DataFrame, Index, or a dictionary of Series/DataFrames.\nrequired\n\n\nas_pandas\nbool\nIf True, the folds are returned as a DataFrame. This is useful to visualize the folds in a more interpretable way. Defaults to False.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist | pd.DataFrame\nA list of lists containing the indices (position) for each fold, or a\n\n\n\nlist | pd.DataFrame\nDataFrame if as_pandas=True. Each list contains 4 lists and a boolean\n\n\n\nlist | pd.DataFrame\nwith the following information:\n\n\n\nlist | pd.DataFrame\n- fold: fold number.\n\n\n\nlist | pd.DataFrame\n- [train_start, train_end]: list with the start and end positions of the training set.\n\n\n\nlist | pd.DataFrame\n- [last_window_start, last_window_end]: list with the start and end positions of the last window seen by the forecaster during training. The last window is used to generate the lags use as predictors. If differentiation is included, the interval is extended as many observations as the differentiation order. If the argument window_size is None, this list is empty.\n\n\n\nlist | pd.DataFrame\n- [test_start, test_end]: list with the start and end positions of the test set. These are the observations used to evaluate the forecaster.\n\n\n\nlist | pd.DataFrame\n- [test_start_with_gap, test_end_with_gap]: list with the start and end positions of the test set including the gap. The gap is the number of observations between the end of the training set and the start of the test set.\n\n\n\nlist | pd.DataFrame\n- fit_forecaster: boolean indicating whether the forecaster should be fitted in this fold.\n\n\n\n\n\n\nThe returned values are the positions of the observations and not the actual values of the index, so they can be used to slice the data directly using iloc.\nIf as_pandas is True, the folds are returned as a DataFrame with the following columns: ‘fold’, ‘train_start’, ‘train_end’, ‘last_window_start’, ‘last_window_end’, ‘test_start’, ‘test_end’, ‘test_start_with_gap’, ‘test_end_with_gap’, ‘fit_forecaster’.\nFollowing the python convention, the start index is inclusive and the end index is exclusive. This means that the last index is not included in the slice.",
    "crumbs": [
      "API Reference",
      "Model Selection",
      "split_ts_cv"
    ]
  },
  {
    "objectID": "docs/reference/model_selection.random_search.html",
    "href": "docs/reference/model_selection.random_search.html",
    "title": "model_selection.random_search",
    "section": "",
    "text": "model_selection.random_search\nRandom search hyperparameter optimization for forecasters.\n\n\n\n\n\nName\nDescription\n\n\n\n\nrandom_search_forecaster\nRandom search over parameter distributions for a Forecaster.\n\n\n\n\n\nmodel_selection.random_search.random_search_forecaster(\n    forecaster,\n    y,\n    cv,\n    param_distributions,\n    metric,\n    exog=None,\n    lags_grid=None,\n    n_iter=10,\n    random_state=123,\n    return_best=True,\n    n_jobs='auto',\n    verbose=False,\n    show_progress=True,\n    suppress_warnings=False,\n    output_file=None,\n)\nRandom search over parameter distributions for a Forecaster.\nPerforms random sampling of parameter settings from distributions for a Forecaster object. Validation is done using time series backtesting with the provided cross-validation strategy. This is more efficient than grid search when exploring large parameter spaces.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nforecaster\nobject\nForecaster model (ForecasterRecursive or ForecasterDirect).\nrequired\n\n\ny\npd.Series\nTraining time series.\nrequired\n\n\ncv\nTimeSeriesFold | OneStepAheadFold\nCross-validation strategy (TimeSeriesFold or OneStepAheadFold) with information needed to split the data into folds.\nrequired\n\n\nparam_distributions\ndict\nDictionary with parameter names (str) as keys and distributions or lists of parameters to try as values. Use scipy.stats distributions for continuous parameters.\nrequired\n\n\nmetric\nstr | Callable | list[str | Callable]\nMetric(s) to quantify model goodness of fit. If str: ‘mean_squared_error’, ‘mean_absolute_error’, ‘mean_absolute_percentage_error’, ‘mean_squared_log_error’, ‘mean_absolute_scaled_error’, ‘root_mean_squared_scaled_error’. If Callable: Function with arguments (y_true, y_pred, y_train) that returns a float. If list: Multiple strings and/or Callables.\nrequired\n\n\nexog\npd.Series | pd.DataFrame | None\nExogenous variable(s) included as predictors. Must have the same number of observations as y and aligned so that y[i] is regressed on exog[i]. Default is None.\nNone\n\n\nlags_grid\nlist[int | list[int] | np.ndarray[int] | range[int]] | dict[str, list[int | list[int] | np.ndarray[int] | range[int]]] | None\nLists of lags to try. Can be int, lists, numpy ndarray, or range objects. If dict, keys are used as labels in results DataFrame. Default is None.\nNone\n\n\nn_iter\nint\nNumber of parameter settings sampled per lags configuration. Trades off runtime vs solution quality. Default is 10.\n10\n\n\nrandom_state\nint\nSeed for random sampling for reproducible output. Default is 123.\n123\n\n\nreturn_best\nbool\nIf True, refit the forecaster using best parameters on the whole dataset. Default is True.\nTrue\n\n\nn_jobs\nint | str\nNumber of jobs to run in parallel. If -1, uses all cores. If ‘auto’, uses select_n_jobs_backtesting. Default is ‘auto’.\n'auto'\n\n\nverbose\nbool\nIf True, print number of folds used for cv. Default is False.\nFalse\n\n\nshow_progress\nbool\nWhether to show a progress bar. Default is True.\nTrue\n\n\nsuppress_warnings\nbool\nIf True, suppress spotforecast warnings during hyperparameter search. Default is False.\nFalse\n\n\noutput_file\nstr | None\nFilename or full path to save results as TSV. If None, results are not saved to file. Default is None.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\nResults for each parameter combination with columns: lags (lags\n\n\n\npd.DataFrame\nconfiguration), lags_label (descriptive label), params (parameters\n\n\n\npd.DataFrame\nconfiguration), metric (metric value), and additional columns with\n\n\n\npd.DataFrame\nparam=value pairs.\n\n\n\n\n\n\nBasic random search with continuous parameter distributions:\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; from scipy.stats import uniform\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from spotforecast2.model_selection import TimeSeriesFold\n&gt;&gt;&gt; from spotforecast2.model_selection.random_search import random_search_forecaster\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; np.random.seed(123)\n&gt;&gt;&gt; y = pd.Series(np.random.randn(50), name='y')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Set up forecaster and cross-validation\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; cv = TimeSeriesFold(steps=3, initial_train_size=20, refit=False)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Define parameter distributions with scipy.stats\n&gt;&gt;&gt; param_distributions = {\n...     'estimator__alpha': uniform(0.1, 10.0)  # Uniform between 0.1 and 10.1\n... }\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Run random search\n&gt;&gt;&gt; results = random_search_forecaster(\n...     forecaster=forecaster,\n...     y=y,\n...     cv=cv,\n...     param_distributions=param_distributions,\n...     metric='mean_squared_error',\n...     n_iter=5,\n...     random_state=42,\n...     return_best=False,\n...     verbose=False,\n...     show_progress=False\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check results\n&gt;&gt;&gt; print(results.shape[0])\n5\n&gt;&gt;&gt; print('estimator__alpha' in results.columns)\nTrue\n&gt;&gt;&gt; print('mean_squared_error' in results.columns)\nTrue",
    "crumbs": [
      "API Reference",
      "Model Selection",
      "random_search"
    ]
  },
  {
    "objectID": "docs/reference/model_selection.random_search.html#functions",
    "href": "docs/reference/model_selection.random_search.html#functions",
    "title": "model_selection.random_search",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nrandom_search_forecaster\nRandom search over parameter distributions for a Forecaster.\n\n\n\n\n\nmodel_selection.random_search.random_search_forecaster(\n    forecaster,\n    y,\n    cv,\n    param_distributions,\n    metric,\n    exog=None,\n    lags_grid=None,\n    n_iter=10,\n    random_state=123,\n    return_best=True,\n    n_jobs='auto',\n    verbose=False,\n    show_progress=True,\n    suppress_warnings=False,\n    output_file=None,\n)\nRandom search over parameter distributions for a Forecaster.\nPerforms random sampling of parameter settings from distributions for a Forecaster object. Validation is done using time series backtesting with the provided cross-validation strategy. This is more efficient than grid search when exploring large parameter spaces.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nforecaster\nobject\nForecaster model (ForecasterRecursive or ForecasterDirect).\nrequired\n\n\ny\npd.Series\nTraining time series.\nrequired\n\n\ncv\nTimeSeriesFold | OneStepAheadFold\nCross-validation strategy (TimeSeriesFold or OneStepAheadFold) with information needed to split the data into folds.\nrequired\n\n\nparam_distributions\ndict\nDictionary with parameter names (str) as keys and distributions or lists of parameters to try as values. Use scipy.stats distributions for continuous parameters.\nrequired\n\n\nmetric\nstr | Callable | list[str | Callable]\nMetric(s) to quantify model goodness of fit. If str: ‘mean_squared_error’, ‘mean_absolute_error’, ‘mean_absolute_percentage_error’, ‘mean_squared_log_error’, ‘mean_absolute_scaled_error’, ‘root_mean_squared_scaled_error’. If Callable: Function with arguments (y_true, y_pred, y_train) that returns a float. If list: Multiple strings and/or Callables.\nrequired\n\n\nexog\npd.Series | pd.DataFrame | None\nExogenous variable(s) included as predictors. Must have the same number of observations as y and aligned so that y[i] is regressed on exog[i]. Default is None.\nNone\n\n\nlags_grid\nlist[int | list[int] | np.ndarray[int] | range[int]] | dict[str, list[int | list[int] | np.ndarray[int] | range[int]]] | None\nLists of lags to try. Can be int, lists, numpy ndarray, or range objects. If dict, keys are used as labels in results DataFrame. Default is None.\nNone\n\n\nn_iter\nint\nNumber of parameter settings sampled per lags configuration. Trades off runtime vs solution quality. Default is 10.\n10\n\n\nrandom_state\nint\nSeed for random sampling for reproducible output. Default is 123.\n123\n\n\nreturn_best\nbool\nIf True, refit the forecaster using best parameters on the whole dataset. Default is True.\nTrue\n\n\nn_jobs\nint | str\nNumber of jobs to run in parallel. If -1, uses all cores. If ‘auto’, uses select_n_jobs_backtesting. Default is ‘auto’.\n'auto'\n\n\nverbose\nbool\nIf True, print number of folds used for cv. Default is False.\nFalse\n\n\nshow_progress\nbool\nWhether to show a progress bar. Default is True.\nTrue\n\n\nsuppress_warnings\nbool\nIf True, suppress spotforecast warnings during hyperparameter search. Default is False.\nFalse\n\n\noutput_file\nstr | None\nFilename or full path to save results as TSV. If None, results are not saved to file. Default is None.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\nResults for each parameter combination with columns: lags (lags\n\n\n\npd.DataFrame\nconfiguration), lags_label (descriptive label), params (parameters\n\n\n\npd.DataFrame\nconfiguration), metric (metric value), and additional columns with\n\n\n\npd.DataFrame\nparam=value pairs.\n\n\n\n\n\n\nBasic random search with continuous parameter distributions:\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; from scipy.stats import uniform\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from spotforecast2.model_selection import TimeSeriesFold\n&gt;&gt;&gt; from spotforecast2.model_selection.random_search import random_search_forecaster\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; np.random.seed(123)\n&gt;&gt;&gt; y = pd.Series(np.random.randn(50), name='y')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Set up forecaster and cross-validation\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; cv = TimeSeriesFold(steps=3, initial_train_size=20, refit=False)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Define parameter distributions with scipy.stats\n&gt;&gt;&gt; param_distributions = {\n...     'estimator__alpha': uniform(0.1, 10.0)  # Uniform between 0.1 and 10.1\n... }\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Run random search\n&gt;&gt;&gt; results = random_search_forecaster(\n...     forecaster=forecaster,\n...     y=y,\n...     cv=cv,\n...     param_distributions=param_distributions,\n...     metric='mean_squared_error',\n...     n_iter=5,\n...     random_state=42,\n...     return_best=False,\n...     verbose=False,\n...     show_progress=False\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check results\n&gt;&gt;&gt; print(results.shape[0])\n5\n&gt;&gt;&gt; print('estimator__alpha' in results.columns)\nTrue\n&gt;&gt;&gt; print('mean_squared_error' in results.columns)\nTrue",
    "crumbs": [
      "API Reference",
      "Model Selection",
      "random_search"
    ]
  },
  {
    "objectID": "docs/reference/model_selection.bayesian_search.html",
    "href": "docs/reference/model_selection.bayesian_search.html",
    "title": "model_selection.bayesian_search",
    "section": "",
    "text": "model_selection.bayesian_search\nBayesian hyperparameter search functions for forecasters using Optuna.\n\n\n\n\n\nName\nDescription\n\n\n\n\nbayesian_search_forecaster\nBayesian hyperparameter optimization for a Forecaster using Optuna.\n\n\n\n\n\nmodel_selection.bayesian_search.bayesian_search_forecaster(\n    forecaster,\n    y,\n    cv,\n    search_space,\n    metric,\n    exog=None,\n    n_trials=10,\n    random_state=123,\n    return_best=True,\n    n_jobs='auto',\n    verbose=False,\n    show_progress=True,\n    suppress_warnings=False,\n    output_file=None,\n    kwargs_create_study=None,\n    kwargs_study_optimize=None,\n)\nBayesian hyperparameter optimization for a Forecaster using Optuna.\nPerforms Bayesian hyperparameter search using the Optuna library for a Forecaster object. Validation is done using time series backtesting with the provided cross-validation strategy.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nforecaster\nobject\nForecaster model. Can be ForecasterRecursive, ForecasterDirect, or any compatible forecaster class.\nrequired\n\n\ny\npd.Series\nTraining time series values. Must be a pandas Series with a datetime or numeric index.\nrequired\n\n\ncv\nTimeSeriesFold | OneStepAheadFold\nCross-validation strategy with information needed to split the data into folds. Must be an instance of TimeSeriesFold or OneStepAheadFold.\nrequired\n\n\nsearch_space\nCallable\nCallable function with argument trial that returns a dictionary with parameter names (str) as keys and Trial objects from optuna (trial.suggest_float, trial.suggest_int, trial.suggest_categorical) as values. Can optionally include ‘lags’ key to search over different lag configurations.\nrequired\n\n\nmetric\nstr | Callable | list[str | Callable]\nMetric(s) to quantify model goodness of fit. Can be: - str: One of ‘mean_squared_error’, ‘mean_absolute_error’, ‘mean_absolute_percentage_error’, ‘mean_squared_log_error’, ‘mean_absolute_scaled_error’, ‘root_mean_squared_scaled_error’. - Callable: Function with arguments (y_true, y_pred) or (y_true, y_pred, y_train) that returns a float. - list: List containing multiple strings and/or Callables.\nrequired\n\n\nexog\npd.Series | pd.DataFrame | None\nExogenous variable(s) included as predictors. Must have the same number of observations as y and aligned so that y[i] is regressed on exog[i]. Default is None.\nNone\n\n\nn_trials\nint\nNumber of parameter settings sampled during optimization. Default is 10.\n10\n\n\nrandom_state\nint\nSeed for sampling reproducibility. When passing a custom sampler in kwargs_create_study, set the seed within the sampler (e.g., {‘sampler’: TPESampler(seed=145)}). Default is 123.\n123\n\n\nreturn_best\nbool\nIf True, refit the forecaster using the best parameters found on the whole dataset at the end. Default is True.\nTrue\n\n\nn_jobs\nint | str\nNumber of parallel jobs. If -1, uses all cores. If ‘auto’, uses spotforecast.skforecast.utils.select_n_jobs_backtesting to automatically determine the number of jobs. Default is ‘auto’.\n'auto'\n\n\nverbose\nbool\nIf True, print number of folds used for cross-validation. Default is False.\nFalse\n\n\nshow_progress\nbool\nWhether to show an Optuna progress bar during optimization. Default is True.\nTrue\n\n\nsuppress_warnings\nbool\nIf True, suppress spotforecast warnings during hyperparameter search. Default is False.\nFalse\n\n\noutput_file\nstr | None\nFilename or full path to save results as TSV. If None, results are not saved to file. Default is None.\nNone\n\n\nkwargs_create_study\ndict | None\nAdditional keyword arguments passed to optuna.create_study(). If not specified, direction is set to ‘minimize’ and TPESampler(seed=123) is used. Default is {}.\nNone\n\n\nkwargs_study_optimize\ndict | None\nAdditional keyword arguments passed to study.optimize(). Default is {}.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple[pd.DataFrame, object]\ntuple[pd.DataFrame, object]: A tuple containing: - results: DataFrame with columns ‘lags’, ‘params’, metric values, and individual parameter columns. Sorted by the first metric. - best_trial: Best optimization result as an optuna.FrozenTrial object containing the best parameters and metric value.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf exog length doesn’t match y length when return_best=True.\n\n\n\nTypeError\nIf cv is not an instance of TimeSeriesFold or OneStepAheadFold.\n\n\n\nValueError\nIf metric list contains duplicate metric names.",
    "crumbs": [
      "API Reference",
      "Model Selection",
      "bayesian_search"
    ]
  },
  {
    "objectID": "docs/reference/model_selection.bayesian_search.html#functions",
    "href": "docs/reference/model_selection.bayesian_search.html#functions",
    "title": "model_selection.bayesian_search",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nbayesian_search_forecaster\nBayesian hyperparameter optimization for a Forecaster using Optuna.\n\n\n\n\n\nmodel_selection.bayesian_search.bayesian_search_forecaster(\n    forecaster,\n    y,\n    cv,\n    search_space,\n    metric,\n    exog=None,\n    n_trials=10,\n    random_state=123,\n    return_best=True,\n    n_jobs='auto',\n    verbose=False,\n    show_progress=True,\n    suppress_warnings=False,\n    output_file=None,\n    kwargs_create_study=None,\n    kwargs_study_optimize=None,\n)\nBayesian hyperparameter optimization for a Forecaster using Optuna.\nPerforms Bayesian hyperparameter search using the Optuna library for a Forecaster object. Validation is done using time series backtesting with the provided cross-validation strategy.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nforecaster\nobject\nForecaster model. Can be ForecasterRecursive, ForecasterDirect, or any compatible forecaster class.\nrequired\n\n\ny\npd.Series\nTraining time series values. Must be a pandas Series with a datetime or numeric index.\nrequired\n\n\ncv\nTimeSeriesFold | OneStepAheadFold\nCross-validation strategy with information needed to split the data into folds. Must be an instance of TimeSeriesFold or OneStepAheadFold.\nrequired\n\n\nsearch_space\nCallable\nCallable function with argument trial that returns a dictionary with parameter names (str) as keys and Trial objects from optuna (trial.suggest_float, trial.suggest_int, trial.suggest_categorical) as values. Can optionally include ‘lags’ key to search over different lag configurations.\nrequired\n\n\nmetric\nstr | Callable | list[str | Callable]\nMetric(s) to quantify model goodness of fit. Can be: - str: One of ‘mean_squared_error’, ‘mean_absolute_error’, ‘mean_absolute_percentage_error’, ‘mean_squared_log_error’, ‘mean_absolute_scaled_error’, ‘root_mean_squared_scaled_error’. - Callable: Function with arguments (y_true, y_pred) or (y_true, y_pred, y_train) that returns a float. - list: List containing multiple strings and/or Callables.\nrequired\n\n\nexog\npd.Series | pd.DataFrame | None\nExogenous variable(s) included as predictors. Must have the same number of observations as y and aligned so that y[i] is regressed on exog[i]. Default is None.\nNone\n\n\nn_trials\nint\nNumber of parameter settings sampled during optimization. Default is 10.\n10\n\n\nrandom_state\nint\nSeed for sampling reproducibility. When passing a custom sampler in kwargs_create_study, set the seed within the sampler (e.g., {‘sampler’: TPESampler(seed=145)}). Default is 123.\n123\n\n\nreturn_best\nbool\nIf True, refit the forecaster using the best parameters found on the whole dataset at the end. Default is True.\nTrue\n\n\nn_jobs\nint | str\nNumber of parallel jobs. If -1, uses all cores. If ‘auto’, uses spotforecast.skforecast.utils.select_n_jobs_backtesting to automatically determine the number of jobs. Default is ‘auto’.\n'auto'\n\n\nverbose\nbool\nIf True, print number of folds used for cross-validation. Default is False.\nFalse\n\n\nshow_progress\nbool\nWhether to show an Optuna progress bar during optimization. Default is True.\nTrue\n\n\nsuppress_warnings\nbool\nIf True, suppress spotforecast warnings during hyperparameter search. Default is False.\nFalse\n\n\noutput_file\nstr | None\nFilename or full path to save results as TSV. If None, results are not saved to file. Default is None.\nNone\n\n\nkwargs_create_study\ndict | None\nAdditional keyword arguments passed to optuna.create_study(). If not specified, direction is set to ‘minimize’ and TPESampler(seed=123) is used. Default is {}.\nNone\n\n\nkwargs_study_optimize\ndict | None\nAdditional keyword arguments passed to study.optimize(). Default is {}.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple[pd.DataFrame, object]\ntuple[pd.DataFrame, object]: A tuple containing: - results: DataFrame with columns ‘lags’, ‘params’, metric values, and individual parameter columns. Sorted by the first metric. - best_trial: Best optimization result as an optuna.FrozenTrial object containing the best parameters and metric value.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf exog length doesn’t match y length when return_best=True.\n\n\n\nTypeError\nIf cv is not an instance of TimeSeriesFold or OneStepAheadFold.\n\n\n\nValueError\nIf metric list contains duplicate metric names.",
    "crumbs": [
      "API Reference",
      "Model Selection",
      "bayesian_search"
    ]
  },
  {
    "objectID": "docs/reference/manager.plotter.html",
    "href": "docs/reference/manager.plotter.html",
    "title": "manager.plotter",
    "section": "",
    "text": "manager.plotter\nModule for generating interactive prediction plots.\nThis module provides the PredictionFigure class and make_plot function to visualize time series forecasting results, including actual values, predictions, and performance metrics.\n\n\n\n\n\nName\nDescription\n\n\n\n\nPredictionFigure\nEncapsulates the generation of an interactive Plotly figure for predictions.\n\n\n\n\n\nmanager.plotter.PredictionFigure(prediction_package)\nEncapsulates the generation of an interactive Plotly figure for predictions.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprediction_package\nDict[str, Any]\nA dictionary containing prediction data and metrics. Expected keys include: - ‘train_actual’: pd.Series - ‘future_actual’: pd.Series - ‘train_pred’: pd.Series - ‘future_pred’: pd.Series - ‘future_forecast’: pd.Series (e.g., benchmark/ENTSOE) - ‘metrics_train’: Dict[str, float] - ‘metrics_future’: Dict[str, float] - ‘metrics_future_one_day’: Dict[str, float] - ‘metrics_forecast’: Dict[str, float] - ‘metrics_forecast_one_day’: Dict[str, float]\nrequired\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nmake_plot\nGenerate the Plotly figure with traces and annotations. The following traces are added:\n\n\n\n\n\nmanager.plotter.PredictionFigure.make_plot()\nGenerate the Plotly figure with traces and annotations. The following traces are added: - Actual values - Predicted values - Future forecast values - Actual values of the last week\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.manager.plotter import PredictionFigure\n&gt;&gt;&gt; # Create synthetic data\n&gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=100, freq=\"h\", tz=\"UTC\")\n&gt;&gt;&gt; train_end = dates[70]\n&gt;&gt;&gt; y = pd.Series(np.random.rand(100) * 100, index=dates, name=\"load\")\n&gt;&gt;&gt; p = y + np.random.normal(0, 5, 100)\n&gt;&gt;&gt; pkg = {\n...     \"train_actual\": y.loc[:train_end],\n...     \"future_actual\": y.loc[train_end:],\n...     \"train_pred\": p.loc[:train_end],\n...     \"future_pred\": p.loc[train_end:],\n...     \"metrics_train\": {\"mae\": 5.0, \"mape\": 0.1},\n...     \"metrics_future\": {\"mae\": 6.0, \"mape\": 0.12},\n...     \"metrics_future_one_day\": {\"mae\": 4.5, \"mape\": 0.08},\n... }\n&gt;&gt;&gt; fig = PredictionFigure(pkg).make_plot()\n&gt;&gt;&gt; isinstance(fig.data, tuple)\nTrue\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nmake_plot\nGenerate and optionally save an interactive prediction plot.\n\n\nplot_actual_vs_predicted\nPlot actual vs predicted combined values for model comparison.\n\n\n\n\n\nmanager.plotter.make_plot(prediction_package, output_path=None)\nGenerate and optionally save an interactive prediction plot.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprediction_package\nDict[str, Any]\nDictionary of results (actuals, preds, metrics).\nrequired\n\n\noutput_path\nOptional[Union[str, Path]]\nPath to save the HTML file. If None, it defaults to ‘index.html’ in the package’s data home directory.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ngo.Figure\nThe generated Plotly Figure object.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotforecast2.manager.plotter import make_plot\n&gt;&gt;&gt; # fig = make_plot(results)\n\n\n\n\nmanager.plotter.plot_actual_vs_predicted(\n    actual_combined,\n    baseline_combined,\n    covariates_combined,\n    custom_lgbm_combined,\n    html_path=None,\n)\nPlot actual vs predicted combined values for model comparison.\nThis function creates an interactive Plotly figure comparing ground truth with predictions from three different forecasting models: baseline, covariate-enhanced, and custom LightGBM. The plot includes interactive hover information and can be saved as a standalone HTML file.\n\n\n\nInteractive visualization for model validation\nSupports HTML export for audit trails\nShows all models simultaneously for easy comparison\nUses consistent color scheme and line styles\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nactual_combined\npd.Series\nGround truth combined series with datetime index.\nrequired\n\n\nbaseline_combined\npd.Series\nBaseline combined prediction series. Must have same index as actual_combined.\nrequired\n\n\ncovariates_combined\npd.Series\nCovariate-enhanced combined prediction series. Must have same index as actual_combined.\nrequired\n\n\ncustom_lgbm_combined\npd.Series\nCustom LightGBM (optimized params) combined prediction series. Must have same index as actual_combined.\nrequired\n\n\nhtml_path\nOptional[str]\nIf set, save the plot as a single self-contained HTML file to this path. If None, displays plot interactively only.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone. Displays plot and optionally saves to HTML file.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf series indices don’t align or are empty.\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from spotforecast2.manager.plotter import plot_actual_vs_predicted\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 1: Create synthetic data for testing\n&gt;&gt;&gt; index = pd.date_range('2020-01-01', periods=24, freq='h')\n&gt;&gt;&gt; actual = pd.Series(range(100, 124), index=index, name='actual')\n&gt;&gt;&gt; baseline = pd.Series(range(101, 125), index=index, name='baseline')\n&gt;&gt;&gt; covariates = pd.Series(range(99, 123), index=index, name='covariates')\n&gt;&gt;&gt; custom = pd.Series(range(100, 124), index=index, name='custom')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Verify data properties\n&gt;&gt;&gt; print(f\"Data length: {len(actual)}\")\nData length: 24\n&gt;&gt;&gt; print(f\"Index type: {type(actual.index).__name__}\")\nIndex type: DatetimeIndex\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 2: Comparing models with different accuracies\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; index = pd.date_range('2020-01-01 00:00:00', periods=48, freq='h')\n&gt;&gt;&gt; actual = pd.Series(\n...     100 + 10 * np.sin(np.arange(48) * 2 * np.pi / 24),\n...     index=index\n... )\n&gt;&gt;&gt; baseline = actual + np.random.normal(0, 2, 48)\n&gt;&gt;&gt; covariates = actual + np.random.normal(0, 1, 48)\n&gt;&gt;&gt; custom = actual + np.random.normal(0, 0.5, 48)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Verify series properties before plotting\n&gt;&gt;&gt; print(f\"Actual range: [{actual.min():.1f}, {actual.max():.1f}]\")\nActual range: [90.0, 110.0]\n&gt;&gt;&gt; print(f\"All indices aligned: {(actual.index == baseline.index).all()}\")\nAll indices aligned: True\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 3: Production workflow with actual forecast data\n&gt;&gt;&gt; index = pd.date_range('2020-01-01', periods=24, freq='h')\n&gt;&gt;&gt; ground_truth = pd.Series([100 + i for i in range(24)], index=index)\n&gt;&gt;&gt; model1_pred = pd.Series([101 + i for i in range(24)], index=index)\n&gt;&gt;&gt; model2_pred = pd.Series([99 + i for i in range(24)], index=index)\n&gt;&gt;&gt; model3_pred = pd.Series([100 + i for i in range(24)], index=index)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Calculate errors\n&gt;&gt;&gt; mae_baseline = abs(ground_truth - model1_pred).mean()\n&gt;&gt;&gt; mae_covariates = abs(ground_truth - model2_pred).mean()\n&gt;&gt;&gt; mae_custom = abs(ground_truth - model3_pred).mean()\n&gt;&gt;&gt; print(f\"Baseline MAE: {mae_baseline:.2f}\")\nBaseline MAE: 1.00\n&gt;&gt;&gt; print(f\"Covariates MAE: {mae_covariates:.2f}\")\nCovariates MAE: 1.00\n&gt;&gt;&gt; print(f\"Custom MAE: {mae_custom:.2f}\")\nCustom MAE: 0.00\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 4: Verify data alignment before plotting\n&gt;&gt;&gt; index1 = pd.date_range('2020-01-01', periods=24, freq='h')\n&gt;&gt;&gt; index2 = pd.date_range('2020-01-02', periods=24, freq='h')\n&gt;&gt;&gt; series1 = pd.Series(range(24), index=index1)\n&gt;&gt;&gt; series2 = pd.Series(range(24), index=index2)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check alignment\n&gt;&gt;&gt; indices_match = (series1.index == series2.index).all()\n&gt;&gt;&gt; print(f\"Indices aligned: {indices_match}\")\nIndices aligned: False\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Reindex to align\n&gt;&gt;&gt; series2_aligned = series2.reindex(series1.index)\n&gt;&gt;&gt; print(f\"After reindex: {(series1.index == series2_aligned.index).all()}\")\nAfter reindex: True\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 5: Verify all series have correct properties\n&gt;&gt;&gt; index = pd.date_range('2020-01-01', periods=10, freq='h')\n&gt;&gt;&gt; actual = pd.Series(range(10), index=index)\n&gt;&gt;&gt; pred1 = pd.Series(range(1, 11), index=index)\n&gt;&gt;&gt; pred2 = pd.Series(range(10), index=index)\n&gt;&gt;&gt; pred3 = pd.Series(range(10), index=index)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Safety checks\n&gt;&gt;&gt; assert isinstance(actual.index, pd.DatetimeIndex), \"Index must be DatetimeIndex\"\n&gt;&gt;&gt; assert len(actual) == len(pred1) == len(pred2) == len(pred3), \"All series must have same length\"\n&gt;&gt;&gt; assert (actual.index == pred1.index).all(), \"Indices must align\"\n&gt;&gt;&gt; print(\"All safety checks passed\")\nAll safety checks passed\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 6: Calculate metrics for model comparison\n&gt;&gt;&gt; index = pd.date_range('2020-01-01', periods=100, freq='h')\n&gt;&gt;&gt; actual = pd.Series(100 + np.random.randn(100) * 5, index=index)\n&gt;&gt;&gt; pred1 = actual + np.random.randn(100) * 2\n&gt;&gt;&gt; pred2 = actual + np.random.randn(100) * 1.5\n&gt;&gt;&gt; pred3 = actual + np.random.randn(100) * 1\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Calculate MAE for each model\n&gt;&gt;&gt; mae1 = abs(actual - pred1).mean()\n&gt;&gt;&gt; mae2 = abs(actual - pred2).mean()\n&gt;&gt;&gt; mae3 = abs(actual - pred3).mean()\n&gt;&gt;&gt; print(f\"Model 1 MAE: {mae1:.2f}\")\nModel 1 MAE: ...\n&gt;&gt;&gt; print(f\"Model 2 MAE: {mae2:.2f}\")\nModel 2 MAE: ...\n&gt;&gt;&gt; print(f\"Model 3 MAE: {mae3:.2f}\")\nModel 3 MAE: ...",
    "crumbs": [
      "API Reference",
      "Manager",
      "plotter"
    ]
  },
  {
    "objectID": "docs/reference/manager.plotter.html#classes",
    "href": "docs/reference/manager.plotter.html#classes",
    "title": "manager.plotter",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nPredictionFigure\nEncapsulates the generation of an interactive Plotly figure for predictions.\n\n\n\n\n\nmanager.plotter.PredictionFigure(prediction_package)\nEncapsulates the generation of an interactive Plotly figure for predictions.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprediction_package\nDict[str, Any]\nA dictionary containing prediction data and metrics. Expected keys include: - ‘train_actual’: pd.Series - ‘future_actual’: pd.Series - ‘train_pred’: pd.Series - ‘future_pred’: pd.Series - ‘future_forecast’: pd.Series (e.g., benchmark/ENTSOE) - ‘metrics_train’: Dict[str, float] - ‘metrics_future’: Dict[str, float] - ‘metrics_future_one_day’: Dict[str, float] - ‘metrics_forecast’: Dict[str, float] - ‘metrics_forecast_one_day’: Dict[str, float]\nrequired\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nmake_plot\nGenerate the Plotly figure with traces and annotations. The following traces are added:\n\n\n\n\n\nmanager.plotter.PredictionFigure.make_plot()\nGenerate the Plotly figure with traces and annotations. The following traces are added: - Actual values - Predicted values - Future forecast values - Actual values of the last week\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.manager.plotter import PredictionFigure\n&gt;&gt;&gt; # Create synthetic data\n&gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=100, freq=\"h\", tz=\"UTC\")\n&gt;&gt;&gt; train_end = dates[70]\n&gt;&gt;&gt; y = pd.Series(np.random.rand(100) * 100, index=dates, name=\"load\")\n&gt;&gt;&gt; p = y + np.random.normal(0, 5, 100)\n&gt;&gt;&gt; pkg = {\n...     \"train_actual\": y.loc[:train_end],\n...     \"future_actual\": y.loc[train_end:],\n...     \"train_pred\": p.loc[:train_end],\n...     \"future_pred\": p.loc[train_end:],\n...     \"metrics_train\": {\"mae\": 5.0, \"mape\": 0.1},\n...     \"metrics_future\": {\"mae\": 6.0, \"mape\": 0.12},\n...     \"metrics_future_one_day\": {\"mae\": 4.5, \"mape\": 0.08},\n... }\n&gt;&gt;&gt; fig = PredictionFigure(pkg).make_plot()\n&gt;&gt;&gt; isinstance(fig.data, tuple)\nTrue",
    "crumbs": [
      "API Reference",
      "Manager",
      "plotter"
    ]
  },
  {
    "objectID": "docs/reference/manager.plotter.html#functions",
    "href": "docs/reference/manager.plotter.html#functions",
    "title": "manager.plotter",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nmake_plot\nGenerate and optionally save an interactive prediction plot.\n\n\nplot_actual_vs_predicted\nPlot actual vs predicted combined values for model comparison.\n\n\n\n\n\nmanager.plotter.make_plot(prediction_package, output_path=None)\nGenerate and optionally save an interactive prediction plot.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprediction_package\nDict[str, Any]\nDictionary of results (actuals, preds, metrics).\nrequired\n\n\noutput_path\nOptional[Union[str, Path]]\nPath to save the HTML file. If None, it defaults to ‘index.html’ in the package’s data home directory.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ngo.Figure\nThe generated Plotly Figure object.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotforecast2.manager.plotter import make_plot\n&gt;&gt;&gt; # fig = make_plot(results)\n\n\n\n\nmanager.plotter.plot_actual_vs_predicted(\n    actual_combined,\n    baseline_combined,\n    covariates_combined,\n    custom_lgbm_combined,\n    html_path=None,\n)\nPlot actual vs predicted combined values for model comparison.\nThis function creates an interactive Plotly figure comparing ground truth with predictions from three different forecasting models: baseline, covariate-enhanced, and custom LightGBM. The plot includes interactive hover information and can be saved as a standalone HTML file.\n\n\n\nInteractive visualization for model validation\nSupports HTML export for audit trails\nShows all models simultaneously for easy comparison\nUses consistent color scheme and line styles\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nactual_combined\npd.Series\nGround truth combined series with datetime index.\nrequired\n\n\nbaseline_combined\npd.Series\nBaseline combined prediction series. Must have same index as actual_combined.\nrequired\n\n\ncovariates_combined\npd.Series\nCovariate-enhanced combined prediction series. Must have same index as actual_combined.\nrequired\n\n\ncustom_lgbm_combined\npd.Series\nCustom LightGBM (optimized params) combined prediction series. Must have same index as actual_combined.\nrequired\n\n\nhtml_path\nOptional[str]\nIf set, save the plot as a single self-contained HTML file to this path. If None, displays plot interactively only.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone. Displays plot and optionally saves to HTML file.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf series indices don’t align or are empty.\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from spotforecast2.manager.plotter import plot_actual_vs_predicted\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 1: Create synthetic data for testing\n&gt;&gt;&gt; index = pd.date_range('2020-01-01', periods=24, freq='h')\n&gt;&gt;&gt; actual = pd.Series(range(100, 124), index=index, name='actual')\n&gt;&gt;&gt; baseline = pd.Series(range(101, 125), index=index, name='baseline')\n&gt;&gt;&gt; covariates = pd.Series(range(99, 123), index=index, name='covariates')\n&gt;&gt;&gt; custom = pd.Series(range(100, 124), index=index, name='custom')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Verify data properties\n&gt;&gt;&gt; print(f\"Data length: {len(actual)}\")\nData length: 24\n&gt;&gt;&gt; print(f\"Index type: {type(actual.index).__name__}\")\nIndex type: DatetimeIndex\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 2: Comparing models with different accuracies\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; index = pd.date_range('2020-01-01 00:00:00', periods=48, freq='h')\n&gt;&gt;&gt; actual = pd.Series(\n...     100 + 10 * np.sin(np.arange(48) * 2 * np.pi / 24),\n...     index=index\n... )\n&gt;&gt;&gt; baseline = actual + np.random.normal(0, 2, 48)\n&gt;&gt;&gt; covariates = actual + np.random.normal(0, 1, 48)\n&gt;&gt;&gt; custom = actual + np.random.normal(0, 0.5, 48)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Verify series properties before plotting\n&gt;&gt;&gt; print(f\"Actual range: [{actual.min():.1f}, {actual.max():.1f}]\")\nActual range: [90.0, 110.0]\n&gt;&gt;&gt; print(f\"All indices aligned: {(actual.index == baseline.index).all()}\")\nAll indices aligned: True\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 3: Production workflow with actual forecast data\n&gt;&gt;&gt; index = pd.date_range('2020-01-01', periods=24, freq='h')\n&gt;&gt;&gt; ground_truth = pd.Series([100 + i for i in range(24)], index=index)\n&gt;&gt;&gt; model1_pred = pd.Series([101 + i for i in range(24)], index=index)\n&gt;&gt;&gt; model2_pred = pd.Series([99 + i for i in range(24)], index=index)\n&gt;&gt;&gt; model3_pred = pd.Series([100 + i for i in range(24)], index=index)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Calculate errors\n&gt;&gt;&gt; mae_baseline = abs(ground_truth - model1_pred).mean()\n&gt;&gt;&gt; mae_covariates = abs(ground_truth - model2_pred).mean()\n&gt;&gt;&gt; mae_custom = abs(ground_truth - model3_pred).mean()\n&gt;&gt;&gt; print(f\"Baseline MAE: {mae_baseline:.2f}\")\nBaseline MAE: 1.00\n&gt;&gt;&gt; print(f\"Covariates MAE: {mae_covariates:.2f}\")\nCovariates MAE: 1.00\n&gt;&gt;&gt; print(f\"Custom MAE: {mae_custom:.2f}\")\nCustom MAE: 0.00\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 4: Verify data alignment before plotting\n&gt;&gt;&gt; index1 = pd.date_range('2020-01-01', periods=24, freq='h')\n&gt;&gt;&gt; index2 = pd.date_range('2020-01-02', periods=24, freq='h')\n&gt;&gt;&gt; series1 = pd.Series(range(24), index=index1)\n&gt;&gt;&gt; series2 = pd.Series(range(24), index=index2)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check alignment\n&gt;&gt;&gt; indices_match = (series1.index == series2.index).all()\n&gt;&gt;&gt; print(f\"Indices aligned: {indices_match}\")\nIndices aligned: False\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Reindex to align\n&gt;&gt;&gt; series2_aligned = series2.reindex(series1.index)\n&gt;&gt;&gt; print(f\"After reindex: {(series1.index == series2_aligned.index).all()}\")\nAfter reindex: True\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 5: Verify all series have correct properties\n&gt;&gt;&gt; index = pd.date_range('2020-01-01', periods=10, freq='h')\n&gt;&gt;&gt; actual = pd.Series(range(10), index=index)\n&gt;&gt;&gt; pred1 = pd.Series(range(1, 11), index=index)\n&gt;&gt;&gt; pred2 = pd.Series(range(10), index=index)\n&gt;&gt;&gt; pred3 = pd.Series(range(10), index=index)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Safety checks\n&gt;&gt;&gt; assert isinstance(actual.index, pd.DatetimeIndex), \"Index must be DatetimeIndex\"\n&gt;&gt;&gt; assert len(actual) == len(pred1) == len(pred2) == len(pred3), \"All series must have same length\"\n&gt;&gt;&gt; assert (actual.index == pred1.index).all(), \"Indices must align\"\n&gt;&gt;&gt; print(\"All safety checks passed\")\nAll safety checks passed\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 6: Calculate metrics for model comparison\n&gt;&gt;&gt; index = pd.date_range('2020-01-01', periods=100, freq='h')\n&gt;&gt;&gt; actual = pd.Series(100 + np.random.randn(100) * 5, index=index)\n&gt;&gt;&gt; pred1 = actual + np.random.randn(100) * 2\n&gt;&gt;&gt; pred2 = actual + np.random.randn(100) * 1.5\n&gt;&gt;&gt; pred3 = actual + np.random.randn(100) * 1\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Calculate MAE for each model\n&gt;&gt;&gt; mae1 = abs(actual - pred1).mean()\n&gt;&gt;&gt; mae2 = abs(actual - pred2).mean()\n&gt;&gt;&gt; mae3 = abs(actual - pred3).mean()\n&gt;&gt;&gt; print(f\"Model 1 MAE: {mae1:.2f}\")\nModel 1 MAE: ...\n&gt;&gt;&gt; print(f\"Model 2 MAE: {mae2:.2f}\")\nModel 2 MAE: ...\n&gt;&gt;&gt; print(f\"Model 3 MAE: {mae3:.2f}\")\nModel 3 MAE: ...",
    "crumbs": [
      "API Reference",
      "Manager",
      "plotter"
    ]
  },
  {
    "objectID": "docs/reference/index.html",
    "href": "docs/reference/index.html",
    "title": "Function reference",
    "section": "",
    "text": "Data curation, outlier detection, and split tools.\n\n\n\npreprocessing._binner\nQuantileBinner class for binning data into quantile-based bins.\n\n\npreprocessing._common\nCommon preprocessing functions and utilities.\n\n\npreprocessing._differentiator\nTime series differentiator transformer.\n\n\npreprocessing._rolling\n\n\n\npreprocessing.curate_data\n\n\n\npreprocessing.imputation\n\n\n\npreprocessing.outlier\nOutlier detection utilities (legacy wrapper for spotforecast2_safe).\n\n\npreprocessing.outlier_plots\n\n\n\npreprocessing.split\n\n\n\npreprocessing.time_series_visualization\nTime series visualization.\n\n\n\n\n\n\nSearch algorithms and cross-validation tools.\n\n\n\nmodel_selection.bayesian_search\nBayesian hyperparameter search functions for forecasters using Optuna.\n\n\nmodel_selection.grid_search\n\n\n\nmodel_selection.random_search\nRandom search hyperparameter optimization for forecasters.\n\n\nmodel_selection.split_base\nBase class for time series cross-validation splitting.\n\n\nmodel_selection.split_ts_cv\nTime series cross-validation splitting.\n\n\nmodel_selection.spotoptim_search\nHyperparameter search functions for forecasters using SpotOptim.\n\n\nmodel_selection.utils_common\nCommon validation and initialization utilities for model selection.\n\n\nmodel_selection.utils_metrics\nMetrics calculation utilities for model selection.\n\n\n\n\n\n\nModel management and configurators.\n\n\n\nmanager.models\nFull-featured forecasting model classes with Bayesian tuning and SHAP.\n\n\nmanager.plotter\nModule for generating interactive prediction plots.\n\n\nmanager.trainer_full\nModule for managing full model training.\n\n\n\n\n\n\nForecasting utilities and metrics.\n\n\n\nforecaster.metrics\nMetrics for evaluating forecasting models.\n\n\nforecaster.utils\n\n\n\nforecaster.recursive._warnings\n\n\n\n\n\n\n\nStatistical analysis tools.\n\n\n\nstats.autocorrelation\n\n\n\n\n\n\n\nDemonstration and predefined forecasting tasks.\n\n\n\ntasks.task_demo\nTask demo: compare baseline, covariate, and custom LightGBM forecasts against ground truth.\n\n\ntasks.task_entsoe\nUnified CLI task script for ENTSO-E data downloading, model training, and prediction.\n\n\ntasks.task_n_to_1\n\n\n\ntasks.task_n_to_1_dataframe\n\n\n\ntasks.task_n_to_1_with_covariates\nN-to-1 Forecasting with Exogenous Covariates and Prediction Aggregation.\n\n\ntasks.task_n_to_1_with_covariates_and_dataframe\nN-to-1 Forecasting with Exogenous Covariates and Prediction Aggregation.\n\n\n\n\n\n\nWeather client utilities.\n\n\n\nweather.weather_client\nWeather data fetching and processing using Open-Meteo API.\n\n\n\n\n\n\nValidation and data transformation utilities.\n\n\n\nutils.data_transform\nData transformation utilities for time series forecasting.\n\n\nutils.forecaster_config\nForecaster configuration utilities.\n\n\nutils.generate_holiday\nUtilities for generating holiday dataframe as covariate.\n\n\nutils.validation\nValidation utilities for time series forecasting.\n\n\n\n\n\n\nSpotforecast exception classes.\n\n\n\nexceptions\nCustom exceptions and warnings for spotforecast2.",
    "crumbs": [
      "API Reference",
      "Overview"
    ]
  },
  {
    "objectID": "docs/reference/index.html#preprocessing",
    "href": "docs/reference/index.html#preprocessing",
    "title": "Function reference",
    "section": "",
    "text": "Data curation, outlier detection, and split tools.\n\n\n\npreprocessing._binner\nQuantileBinner class for binning data into quantile-based bins.\n\n\npreprocessing._common\nCommon preprocessing functions and utilities.\n\n\npreprocessing._differentiator\nTime series differentiator transformer.\n\n\npreprocessing._rolling\n\n\n\npreprocessing.curate_data\n\n\n\npreprocessing.imputation\n\n\n\npreprocessing.outlier\nOutlier detection utilities (legacy wrapper for spotforecast2_safe).\n\n\npreprocessing.outlier_plots\n\n\n\npreprocessing.split\n\n\n\npreprocessing.time_series_visualization\nTime series visualization.",
    "crumbs": [
      "API Reference",
      "Overview"
    ]
  },
  {
    "objectID": "docs/reference/index.html#model-selection",
    "href": "docs/reference/index.html#model-selection",
    "title": "Function reference",
    "section": "",
    "text": "Search algorithms and cross-validation tools.\n\n\n\nmodel_selection.bayesian_search\nBayesian hyperparameter search functions for forecasters using Optuna.\n\n\nmodel_selection.grid_search\n\n\n\nmodel_selection.random_search\nRandom search hyperparameter optimization for forecasters.\n\n\nmodel_selection.split_base\nBase class for time series cross-validation splitting.\n\n\nmodel_selection.split_ts_cv\nTime series cross-validation splitting.\n\n\nmodel_selection.spotoptim_search\nHyperparameter search functions for forecasters using SpotOptim.\n\n\nmodel_selection.utils_common\nCommon validation and initialization utilities for model selection.\n\n\nmodel_selection.utils_metrics\nMetrics calculation utilities for model selection.",
    "crumbs": [
      "API Reference",
      "Overview"
    ]
  },
  {
    "objectID": "docs/reference/index.html#manager",
    "href": "docs/reference/index.html#manager",
    "title": "Function reference",
    "section": "",
    "text": "Model management and configurators.\n\n\n\nmanager.models\nFull-featured forecasting model classes with Bayesian tuning and SHAP.\n\n\nmanager.plotter\nModule for generating interactive prediction plots.\n\n\nmanager.trainer_full\nModule for managing full model training.",
    "crumbs": [
      "API Reference",
      "Overview"
    ]
  },
  {
    "objectID": "docs/reference/index.html#forecaster",
    "href": "docs/reference/index.html#forecaster",
    "title": "Function reference",
    "section": "",
    "text": "Forecasting utilities and metrics.\n\n\n\nforecaster.metrics\nMetrics for evaluating forecasting models.\n\n\nforecaster.utils\n\n\n\nforecaster.recursive._warnings",
    "crumbs": [
      "API Reference",
      "Overview"
    ]
  },
  {
    "objectID": "docs/reference/index.html#stats",
    "href": "docs/reference/index.html#stats",
    "title": "Function reference",
    "section": "",
    "text": "Statistical analysis tools.\n\n\n\nstats.autocorrelation",
    "crumbs": [
      "API Reference",
      "Overview"
    ]
  },
  {
    "objectID": "docs/reference/index.html#tasks",
    "href": "docs/reference/index.html#tasks",
    "title": "Function reference",
    "section": "",
    "text": "Demonstration and predefined forecasting tasks.\n\n\n\ntasks.task_demo\nTask demo: compare baseline, covariate, and custom LightGBM forecasts against ground truth.\n\n\ntasks.task_entsoe\nUnified CLI task script for ENTSO-E data downloading, model training, and prediction.\n\n\ntasks.task_n_to_1\n\n\n\ntasks.task_n_to_1_dataframe\n\n\n\ntasks.task_n_to_1_with_covariates\nN-to-1 Forecasting with Exogenous Covariates and Prediction Aggregation.\n\n\ntasks.task_n_to_1_with_covariates_and_dataframe\nN-to-1 Forecasting with Exogenous Covariates and Prediction Aggregation.",
    "crumbs": [
      "API Reference",
      "Overview"
    ]
  },
  {
    "objectID": "docs/reference/index.html#weather",
    "href": "docs/reference/index.html#weather",
    "title": "Function reference",
    "section": "",
    "text": "Weather client utilities.\n\n\n\nweather.weather_client\nWeather data fetching and processing using Open-Meteo API.",
    "crumbs": [
      "API Reference",
      "Overview"
    ]
  },
  {
    "objectID": "docs/reference/index.html#utils",
    "href": "docs/reference/index.html#utils",
    "title": "Function reference",
    "section": "",
    "text": "Validation and data transformation utilities.\n\n\n\nutils.data_transform\nData transformation utilities for time series forecasting.\n\n\nutils.forecaster_config\nForecaster configuration utilities.\n\n\nutils.generate_holiday\nUtilities for generating holiday dataframe as covariate.\n\n\nutils.validation\nValidation utilities for time series forecasting.",
    "crumbs": [
      "API Reference",
      "Overview"
    ]
  },
  {
    "objectID": "docs/reference/index.html#exceptions",
    "href": "docs/reference/index.html#exceptions",
    "title": "Function reference",
    "section": "",
    "text": "Spotforecast exception classes.\n\n\n\nexceptions\nCustom exceptions and warnings for spotforecast2.",
    "crumbs": [
      "API Reference",
      "Overview"
    ]
  },
  {
    "objectID": "docs/reference/forecaster.recursive._warnings.html",
    "href": "docs/reference/forecaster.recursive._warnings.html",
    "title": "forecaster.recursive._warnings",
    "section": "",
    "text": "forecaster.recursive._warnings\nforecaster.recursive._warnings",
    "crumbs": [
      "API Reference",
      "Forecaster",
      "recursive._warnings"
    ]
  },
  {
    "objectID": "docs/reference/exceptions.html",
    "href": "docs/reference/exceptions.html",
    "title": "exceptions",
    "section": "",
    "text": "exceptions\nCustom exceptions and warnings for spotforecast2.\nThis module contains all the custom warnings and error classes used across spotforecast2.\n\n\nUsing custom warnings::\nimport warnings\nfrom spotforecast2.exceptions import MissingValuesWarning\n\n# Raise a warning\nwarnings.warn(\n    \"Missing values detected in input data.\",\n    MissingValuesWarning\n)\n\n# Suppress a specific warning\nwarnings.simplefilter('ignore', category=MissingValuesWarning)",
    "crumbs": [
      "API Reference",
      "Exceptions",
      "exceptions"
    ]
  },
  {
    "objectID": "docs/reference/exceptions.html#examples",
    "href": "docs/reference/exceptions.html#examples",
    "title": "exceptions",
    "section": "",
    "text": "Using custom warnings::\nimport warnings\nfrom spotforecast2.exceptions import MissingValuesWarning\n\n# Raise a warning\nwarnings.warn(\n    \"Missing values detected in input data.\",\n    MissingValuesWarning\n)\n\n# Suppress a specific warning\nwarnings.simplefilter('ignore', category=MissingValuesWarning)",
    "crumbs": [
      "API Reference",
      "Exceptions",
      "exceptions"
    ]
  },
  {
    "objectID": "docs/model_selection/spotoptim_intro.html",
    "href": "docs/model_selection/spotoptim_intro.html",
    "title": "Introduction to Hyperparameter Tuning with SpotOptim",
    "section": "",
    "text": "SpotOptim provides an advanced surrogate-model-based optimization engine integrated directly into spotforecast2. It acts as a powerful, drop-in alternative to bayesian_search_forecaster, leveraging surrogate modeling (such as Kriging/Gaussian Processes, Random Forests, etc.) rather than Optuna’s TPE sampler, often discovering better hyperparameter configurations with fewer evaluations.\nThis guide will walk you through the core function spotoptim_search_forecaster(), from a simple baseline example up to an advanced configuration, explaining every argument available.",
    "crumbs": [
      "Model Selection Guides",
      "Intro to SpotOptim"
    ]
  },
  {
    "objectID": "docs/model_selection/spotoptim_intro.html#core-arguments-overview",
    "href": "docs/model_selection/spotoptim_intro.html#core-arguments-overview",
    "title": "Introduction to Hyperparameter Tuning with SpotOptim",
    "section": "Core Arguments Overview",
    "text": "Core Arguments Overview\nWhen calling spotoptim_search_forecaster(), you can comprehensively control the optimization environment. Table 1 shows a breakdown of every available argument:\n\n\n\nTable 1: Available arguments for spotoptim_search_forecaster().\n\n\n\n\n\n\n\n\n\n\nArgument\nType\nDescription\n\n\n\n\nforecaster\nobject\nThe base forecaster model to be tuned (e.g., ForecasterRecursive).\n\n\ny\npd.Series\nThe target training time series. Must have a datetime or numeric index.\n\n\ncv\nTimeSeriesFold | OneStepAheadFold\nThe cross-validation strategy used to evaluate configurations during the search.\n\n\nsearch_space\nParameterSet | dict\nThe boundaries and categories of the hyperparameters to explore.\n\n\nmetric\nstr | Callable | list\nThe metric(s) used to evaluate forecaster performance. First metric dictates sorting.\n\n\nexog\npd.Series | pd.DataFrame | None\nOptional exogenous variables to include during modeling.\n\n\nn_trials\nint\nThe total number of evaluations to perform throughout the entire search.\n\n\nn_initial\nint\nThe number of random initial design points before the surrogate model takes over.\n\n\nrandom_state\nint\nRNG seed ensuring reproducible designs and sampling.\n\n\nreturn_best\nbool\nIf True, the passed forecaster is automatically refit on the entire y using the best found configuration.\n\n\nn_jobs\nint | str\nNumber of parallel jobs for backtesting parallelization. Use \"auto\" for automatic CPU detection.\n\n\nverbose\nbool\nPrint detailed SpotOptim engine optimization logs to stdout.\n\n\nshow_progress\nbool\nDisplay a tqdm progress bar for evaluation progression.\n\n\nsuppress_warnings\nbool\nTemporarily suppress spotforecast2 warnings during rapid evaluation cycles.\n\n\noutput_file\nstr | None\nPath to save the trial results incrementally as a TSV file.\n\n\nkwargs_spotoptim\ndict | None\nAdvanced dictionary passed directly to the underlying SpotOptim engine (e.g., custom surrogate configurations, acquisition functions).",
    "crumbs": [
      "Model Selection Guides",
      "Intro to SpotOptim"
    ]
  },
  {
    "objectID": "docs/model_selection/spotoptim_intro.html#simple-tuning-example",
    "href": "docs/model_selection/spotoptim_intro.html#simple-tuning-example",
    "title": "Introduction to Hyperparameter Tuning with SpotOptim",
    "section": "Simple Tuning Example",
    "text": "Simple Tuning Example\nIn this first example, we want to tune a basic Ridge regression forecaster. We will evaluate configurations using a simple cross-validation fold, passing the search_space as a standard dictionary.\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import Ridge\nfrom spotforecast2_safe.forecaster.recursive import ForecasterRecursive\nfrom spotforecast2.model_selection import (\n    TimeSeriesFold,\n    spotoptim_search_forecaster,\n)\n\n# 1. Generate realistic target data\nnp.random.seed(42)\ny = pd.Series(\n    np.random.randn(200).cumsum(),\n    index=pd.date_range(\"2022-01-01\", periods=200, freq=\"h\"),\n    name=\"load\",\n)\n\n# 2. Define Forecaster and Validation\nforecaster = ForecasterRecursive(estimator=Ridge(), lags=5)\ncv = TimeSeriesFold(\n    steps=5,\n    initial_train_size=150,\n    refit=False,\n)\n\n# 3. Define a simple search space using a dictionary\nsearch_space = {\"alpha\": (0.01, 10.0)}\n\n# 4. execute SpotOptim Search\nresults, optimizer = spotoptim_search_forecaster(\n    forecaster=forecaster,\n    y=y,\n    cv=cv,\n    search_space=search_space,\n    metric=\"mean_absolute_error\",\n    n_trials=5,      # 5 total trials\n    n_initial=3,     # 3 random, 2 surrogate-guided\n    random_state=42,\n    return_best=True, # Automatically refits the `forecaster` object\n    show_progress=False,\n)\n\nprint(results.head(3))\nprint(f\"Best found alpha: {results.loc[0, 'alpha']}\")\n\n`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [1 2 3 4 5] \n  Parameters: {'alpha': 7.422726358308641}\n  Backtesting metric: 1.019885673261824\n              lags                         params  mean_absolute_error  \\\n0  [1, 2, 3, 4, 5]   {'alpha': 7.422726358308641}             1.019886   \n1  [1, 2, 3, 4, 5]   {'alpha': 1.878534795625666}             1.019886   \n2  [1, 2, 3, 4, 5]  {'alpha': 3.8108689266950964}             1.019886   \n\n      alpha  \n0  7.422726  \n1  1.878535  \n2  3.810869  \nBest found alpha: 7.422726358308641\n\n\nBecause return_best=True, the forecaster object is now perfectly equipped to make predictions immediately, using alpha configuration results.loc[0, \"alpha\"].",
    "crumbs": [
      "Model Selection Guides",
      "Intro to SpotOptim"
    ]
  },
  {
    "objectID": "docs/model_selection/spotoptim_intro.html#advanced-tuning-example",
    "href": "docs/model_selection/spotoptim_intro.html#advanced-tuning-example",
    "title": "Introduction to Hyperparameter Tuning with SpotOptim",
    "section": "Advanced Tuning Example",
    "text": "Advanced Tuning Example\nNow, let’s explore an advanced configuration. We’ll use a ParameterSet object (from the spotoptim library) for more granular control over numerical types and transformations. We will also inject dictionary arguments explicitly to the SpotOptim engine.\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom spotforecast2.model_selection import OneStepAheadFold\nfrom spotoptim.hyperparameters import ParameterSet\n\n# Data generation\ny_adv = pd.Series(\n    np.random.randn(300).cumsum(),\n    index=pd.date_range(\"2023-01-01\", periods=300, freq=\"h\"),\n    name=\"advanced_load\",\n)\nexog_adv = pd.Series(\n    np.random.uniform(10, 20, 300),\n    index=y_adv.index,\n    name=\"temp\",\n)\n\n# 1. Advanced setup\nforecaster_rf = ForecasterRecursive(\n    estimator=RandomForestRegressor(random_state=123), \n    lags=12\n)\n\n# Fast screening validation approach\ncv_fast = OneStepAheadFold(initial_train_size=100)\n\n# 2. Granular Search Space using ParameterSet\nps = ParameterSet()\nps.add_int(\"n_estimators\", low=10, high=50)\nps.add_int(\"max_depth\", low=2, high=10)\nps.add_float(\"max_features\", low=0.1, high=1.0)\n# We can also tune `lags` systematically\nps.add_factor(\"lags\", [\"[1, 2, 3]\", \"[1, 2, 3, 4, 5, 6]\"])\n\n# 3. SpotOptim execution with advanced Engine Control\nresults_adv, optimizer_adv = spotoptim_search_forecaster(\n    forecaster=forecaster_rf,\n    y=y_adv,\n    exog=exog_adv,\n    cv=cv_fast,\n    search_space=ps,\n    metric=[\"mean_absolute_error\", \"mean_squared_error\"], # Multiple metrics tracked\n    n_trials=10,\n    n_initial=5,\n    random_state=99,\n    return_best=False,\n    n_jobs=1,\n    verbose=False,\n    show_progress=False,\n    suppress_warnings=True,\n    kwargs_spotoptim={\n        \"surrogate\": RandomForestRegressor(random_state=42), # Custom surrogate model\n    }\n)\n\nprint(results_adv[[\"n_estimators\", \"max_depth\", \"lags\", \"mean_absolute_error\"]].head(3))\n\n   n_estimators  max_depth       lags  mean_absolute_error\n0          24.0        9.0  [1, 2, 3]             1.091275\n1          49.0        9.0  [1, 2, 3]             1.091275\n2          46.0        9.0  [1, 2, 3]             1.091275\n\n\nThis advanced setup illustrates how spotoptim_search_forecaster acts as an extremely flexible orchestration bridge, allowing you to efficiently explore your state space while capturing comprehensive performance metadata.",
    "crumbs": [
      "Model Selection Guides",
      "Intro to SpotOptim"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to spotforecast2",
    "section": "",
    "text": "spotforecast2 is a Python package for forecasting, combining the power of sklearn, spotoptim and skforecast with specialized utilities for “spot” forecasting.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#quick-links",
    "href": "index.html#quick-links",
    "title": "Welcome to spotforecast2",
    "section": "Quick Links",
    "text": "Quick Links\n\n📦 GitHub Repository\n📚 API Reference\n🚀 Current Version: 0.9.1",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Welcome to spotforecast2",
    "section": "Installation",
    "text": "Installation\n\nDownload from GitHub\n\ngit clone https://github.com/sequential-parameter-optimization/spotforecast2.git\ncd spotforecast2\n\nSync using uv\n\nuv sync",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "Welcome to spotforecast2",
    "section": "Features",
    "text": "Features\n\nData Fetching: Easy access to time series data.\nPreprocessing: Robust tools for curating, cleaning, and splitting data.\nForecasting: A rich set of forecasting strategies (constantly extended).\nModel Selection: spotoptim and optuna search for hyperparameter tuning.\nWeather Integration: Utilities for fetching and using weather data in forecasts.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#attributions",
    "href": "index.html#attributions",
    "title": "Welcome to spotforecast2",
    "section": "Attributions",
    "text": "Attributions\nParts of the code are ported from skforecast to reduce external dependencies. Many thanks to the skforecast team for their great work!",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "docs/model_selection/model_training_intro.html",
    "href": "docs/model_selection/model_training_intro.html",
    "title": "Introduction to Model Training",
    "section": "",
    "text": "The train_new_model() function in spotforecast2.manager.trainer_full serves as the primary gateway for bootstrapping a forecasting context. It is designed to cleanly separate the complex realities of data ingestion, training window creation (cutoffs), hyperparameter tuning, and cross-platform model persistence.\nThis guide explores the standard approach to training models with train_new_model(), evaluating argument utility, and demonstrating both basic and advanced scenarios.",
    "crumbs": [
      "Model Selection Guides",
      "Intro to Model Training"
    ]
  },
  {
    "objectID": "docs/model_selection/model_training_intro.html#core-arguments-overview",
    "href": "docs/model_selection/model_training_intro.html#core-arguments-overview",
    "title": "Introduction to Model Training",
    "section": "Core Arguments Overview",
    "text": "Core Arguments Overview\nWhen interacting with train_new_model(), you supply necessary context parameters that define what model is built and what data it learns from.\n\n\n\nTable 1: Available arguments for train_new_model().\n\n\n\n\n\n\n\n\n\n\nArgument\nType\nDescription\n\n\n\n\nmodel_class\ntype\nReference to the python class representing the forecaster. It must accept iteration, end_dev, and train_size, and expose a tune() method.\n\n\nn_iteration\nint\nIncremental version number distinguishing this training cycle from predecessors. Strongly recommended for lineage tagging.\n\n\nmodel_name\nstr | None\nBase tracking tag. The final saved filename follows the format: &lt;model_name&gt;_forecaster_&lt;n_iteration&gt;.joblib.\n\n\ntrain_size\npd.Timedelta | None\nTotal duration of the time series window extracted backwards from the end_dev cutoff.\n\n\nsave_to_file\nbool\nAutomatically compress and serialize the fully tuned model to a .joblib component on disk.\n\n\nmodel_dir\nstr | Path | None\nOutput directory for the joblib. Defaults to the framework’s canonical cache home if left unspecified.\n\n\nend_dev\nstr | pd.Timestamp | None\nHard cutoff timestamp. Data strictly chronologically after this timestamp is masked from the training pipeline. If None, it automatically infers this as one day before the most recent data point.\n\n\ndata_filename\nstr | None\nTarget csv path inside the dataset dir to load. Defers to fetch_data() logic if omitted.\n\n\n**kwargs\nAny\nKey-value arguments streamed dynamically right into the model_class initialization lifecycle.",
    "crumbs": [
      "Model Selection Guides",
      "Intro to Model Training"
    ]
  },
  {
    "objectID": "docs/model_selection/model_training_intro.html#simple-training-example",
    "href": "docs/model_selection/model_training_intro.html#simple-training-example",
    "title": "Introduction to Model Training",
    "section": "Simple Training Example",
    "text": "Simple Training Example\nLet’s look at the most basic way to initialize and launch tuning for a new model pipeline. For the purpose of these examples, we will define a MockForecaster class representing our forecaster model, similar to how internal tracking elements act.\n\nimport pandas as pd\nfrom spotforecast2.manager.trainer_full import train_new_model\n\n# 1. Define a Mock Model Class meeting the API requirements\nclass MockForecaster:\n    def __init__(self, iteration, end_dev, train_size, **kwargs):\n        self.iteration = iteration\n        self.end_dev = end_dev\n        self.train_size = train_size\n        self.config = kwargs\n    \n    def tune(self):\n        # In actual usage, this acts as the gateway to spotoptim_search\n        print(f\"Executing tune() for iteration {self.iteration}\")\n        print(f\"Focus window cuts off at: {self.end_dev}\")\n\n    def get_params(self):\n        return {\"stub\": \"mock\"}\n\n# 2. Start a basic training run explicitly overriding the cutoff\n# Note: we disable saving to prevent dumping a joblib locally during the example\nfrom spotforecast2_safe.data.fetch_data import get_package_data_home\ndemo_file = get_package_data_home() / \"demo01.csv\"\n\nmodel_basic = train_new_model(\n    model_class=MockForecaster,\n    n_iteration=1,\n    model_name=\"baseline_mock\",\n    end_dev=\"2023-01-01 00:00+00:00\",\n    train_size=None, # Use the entire history\n    save_to_file=False,\n    data_filename=str(demo_file)\n)\n\nprint(f\"Constructed class type: {type(model_basic).__name__}\")\nprint(f\"Model internal cutoff limit: {model_basic.end_dev}\")\n\nExecuting tune() for iteration 1\nFocus window cuts off at: 2023-01-01 00:00:00+00:00\nConstructed class type: MockForecaster\nModel internal cutoff limit: 2023-01-01 00:00:00+00:00",
    "crumbs": [
      "Model Selection Guides",
      "Intro to Model Training"
    ]
  },
  {
    "objectID": "docs/model_selection/model_training_intro.html#advanced-training-scenarios",
    "href": "docs/model_selection/model_training_intro.html#advanced-training-scenarios",
    "title": "Introduction to Model Training",
    "section": "Advanced Training Scenarios",
    "text": "Advanced Training Scenarios\nIn production systems, train_new_model handles rolling window progression safely via argument parameters. You will rarely want to default to train_size=None (complete history) as this risks severe memory allocation and concept drift over time. Instead, utilizing fixed continuous windows mapped against hard checkpoints handles edge cases effectively.\nWe can combine train_size constraint generation dynamically with extra parameter streaming (**kwargs):\n\n# 1. Start an advanced tuning workflow\nfrom spotforecast2_safe.data.fetch_data import get_package_data_home\ndemo_file = get_package_data_home() / \"demo01.csv\"\n\nmodel_advanced = train_new_model(\n    model_class=MockForecaster,\n    n_iteration=3,\n    model_name=\"production_mock\",\n    train_size=pd.Timedelta(days=365), # Force exactly 1 year backward logic\n    end_dev=\"2024-03-15 00:00+00:00\",\n    save_to_file=False,\n    data_filename=str(demo_file),\n    # Inject specific kwargs dynamically\n    lags=48,\n    advanced_regularization=True,\n    surrogate_seed=1214\n)\n\nprint(f\"Validation bounded train_size setting: {model_advanced.train_size.days} days\")\nprint(f\"Injected **kwargs parameters -&gt; lags: {model_advanced.config.get('lags')}\")\nprint(f\"Injected **kwargs parameters -&gt; regularization: {model_advanced.config.get('advanced_regularization')}\")\n\nExecuting tune() for iteration 3\nFocus window cuts off at: 2024-03-15 00:00:00+00:00\nValidation bounded train_size setting: 365 days\nInjected **kwargs parameters -&gt; lags: 48\nInjected **kwargs parameters -&gt; regularization: True\n\n\nBecause of its generalized class hook mechanism, any ForecasterRecursive wrap, complex pipeline, or hybrid system that matches the initialization signature and tune() command standard can be successfully optimized and routed through this framework entrypoint.",
    "crumbs": [
      "Model Selection Guides",
      "Intro to Model Training"
    ]
  },
  {
    "objectID": "docs/model_selection/model_training_intro.html#fully-functional-end-to-end-example",
    "href": "docs/model_selection/model_training_intro.html#fully-functional-end-to-end-example",
    "title": "Introduction to Model Training",
    "section": "Fully Functional End-to-End Example",
    "text": "Fully Functional End-to-End Example\nTo bridge theory into a real application, the following completely functional example demonstrates loading the packaged demo01.csv historical dataset. We construct a minimal implementation of the model_class, utilizing fetch_data to load the history inside the tune method and performing a genuine ForecasterRecursive fit.\n\nimport pandas as pd\nfrom sklearn.linear_model import Ridge\nfrom spotforecast2_safe.forecaster.recursive import ForecasterRecursive\nfrom spotforecast2_safe.data.fetch_data import fetch_data, get_package_data_home\nfrom spotforecast2.manager.trainer_full import train_new_model\n\nclass FunctionalForecaster:\n    # Notice we capture `dataset_path` from dynamic **kwargs\n    def __init__(self, iteration, end_dev, train_size, dataset_path=None, **kwargs):\n        self.iteration = iteration\n        self.end_dev = end_dev\n        self.train_size = train_size\n        self.dataset_path = dataset_path\n        \n        # A simple internal forecaster to be trained\n        self.forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        self.name = \"demo01_model\"\n\n    def tune(self):\n        # 1. Fetch the data inside the model\n        df = fetch_data(filename=self.dataset_path)\n        y = df[\"Actual Load\"]\n        \n        # 2. Slice the historical data strictly up to end_dev according to train_size\n        if self.train_size is not None:\n            start_date = self.end_dev - self.train_size\n            y_train = y.loc[start_date:self.end_dev]\n        else:\n            y_train = y.loc[:self.end_dev]\n\n        # 3. Fit the model genuinely\n        print(f\"Fitting model strictly on data until {self.end_dev}\")\n        print(f\"Training window length: {len(y_train)} hours\")\n        self.forecaster.fit(y=y_train)\n        \n    def get_params(self):\n        return {}\n\n# 1. Define path to the demo dataset packaged dynamically with spotforecast2_safe\ndemo_file = get_package_data_home() / \"demo01.csv\"\n\n# 2. Execute the training pipeline\n# By setting end_dev=None, train_new_model checks the CSV implicitly \n# to calculate the cutoff boundary to be exactly 1 day before the final record.\nmodel_functional = train_new_model(\n    model_class=FunctionalForecaster,\n    n_iteration=1,\n    train_size=pd.Timedelta(days=7), # Only use the last 7 days of data for training\n    end_dev=None, \n    data_filename=str(demo_file), # Passed to train_new_model to compute cutoff\n    save_to_file=False, # Disable file writes for the example\n    dataset_path=str(demo_file) # Stored in kwargs and passed to __init__\n)\n\nassert model_functional.forecaster.is_fitted is True\nprint(\"Model pipeline successfully fitted!\")\n\nFitting model strictly on data until 2026-02-13 22:45:00+00:00\nTraining window length: 673 hours\nModel pipeline successfully fitted!\n\n\n/opt/hostedtoolcache/Python/3.13.11/x64/lib/python3.13/site-packages/spotforecast2_safe/forecaster/utils.py:792: UserWarning:\n\n`y` has a DatetimeIndex but no frequency. The frequency has been inferred from the index.",
    "crumbs": [
      "Model Selection Guides",
      "Intro to Model Training"
    ]
  },
  {
    "objectID": "docs/model_selection/model_training_intro.html#visualizing-prediction-quality",
    "href": "docs/model_selection/model_training_intro.html#visualizing-prediction-quality",
    "title": "Introduction to Model Training",
    "section": "Visualizing Prediction Quality",
    "text": "Visualizing Prediction Quality\nIn safety-critical workflows, evaluating multi-step out-of-sample performance is critical. We can leverage the framework to explicitly constrain the end_dev boundary, allowing us to withhold future data. Once train_new_model completes training, we use the returned pipeline to project predictions across the held-out window and visualize the model’s reliability in distinguishing structural patterns in the demo02.csv dataset.\n\nimport pandas as pd\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_absolute_error\nfrom spotforecast2_safe.forecaster.recursive import ForecasterRecursive\nfrom spotforecast2_safe.data.fetch_data import fetch_data, get_package_data_home\nfrom spotforecast2.manager.trainer_full import train_new_model\nimport plotly.graph_objects as go\n\nclass VisualizingForecaster:\n    def __init__(self, iteration, end_dev, train_size, dataset_path=None, **kwargs):\n        self.iteration = iteration\n        self.end_dev = end_dev\n        self.train_size = train_size\n        self.dataset_path = dataset_path\n        \n        # Using a deeper lag window for more predictive capability\n        self.forecaster = ForecasterRecursive(estimator=Ridge(), lags=24)\n        self.name = \"demo02_model\"\n\n    def tune(self):\n        df = fetch_data(filename=self.dataset_path)\n        y = df[\"A\"].groupby(level=0).mean().asfreq(\"h\").ffill() # Safely handle duplicates and NA gaps\n        \n        # Enforce hard upper cutoff\n        y_train = y.loc[:self.end_dev]\n        \n        # Enforce lower boundary\n        if self.train_size is not None:\n            start_date = pd.to_datetime(self.end_dev, utc=True) - self.train_size\n            y_train = y_train.loc[start_date:]\n            \n        print(f\"Fitting model locally on {len(y_train)} points until {self.end_dev}\")\n        self.forecaster.fit(y=y_train)\n\n    def get_params(self): return {}\n\n# 1. Fetch the multi-variate continuous integration dataset \"demo02.csv\"\ndemo_file = get_package_data_home() / \"demo02.csv\"\ndf_full = fetch_data(filename=str(demo_file))\ny_full = df_full[\"A\"].groupby(level=0).mean().asfreq(\"h\").ffill()\n\n# 2. Establish chronological boundaries (e.g., test on the final 7 days)\ntest_duration = pd.Timedelta(days=7)\ncutoff_date = y_full.index.max() - test_duration\n\n# 3. Train isolated pipeline matching precise boundaries\nmodel_vis = train_new_model(\n    model_class=VisualizingForecaster,\n    n_iteration=1,\n    train_size=pd.Timedelta(days=60), # 60-day historical perspective\n    end_dev=cutoff_date, \n    data_filename=str(demo_file), \n    save_to_file=False, \n    dataset_path=str(demo_file)\n)\n\n# 4. Extract ground truth testing window (exclusive of the cutoff)\ny_test = y_full.loc[cutoff_date + pd.Timedelta(hours=1):]\n\n# 5. Execute N-step recursive predictions\npreds = model_vis.forecaster.predict(steps=len(y_test))\npreds.index = y_test.index  # Align axes\n\n# 6. Measure mathematical accuracy\nmae = mean_absolute_error(y_test, preds)\nprint(f\"Validation MAE: {mae:.3f}\")\n\n# 7. Generate interactive verification layer (plotly native view)\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=y_test.index, y=y_test, mode=\"lines\", name=\"Actual Truth\"))\nfig.add_trace(go.Scatter(\n    x=preds.index, y=preds, mode=\"lines\", \n    name=\"Forecaster Projection\", \n    line=dict(dash=\"dash\", color=\"orange\")\n))\n\nfig.update_layout(\n    title=f\"demo02.csv Prediction Quality Appraisal (MAE: {mae:.3f})\",\n    xaxis_title=\"Time (UTC)\",\n    yaxis_title=\"Target Sensor: A\",\n    template=\"plotly_white\",\n    hovermode=\"x unified\"\n)\n# fig.show() # Automatically evaluates inside Quarto output blocks\n\nFitting model locally on 1441 points until 1975-06-11 18:00:00+00:00\nValidation MAE: 0.093",
    "crumbs": [
      "Model Selection Guides",
      "Intro to Model Training"
    ]
  },
  {
    "objectID": "docs/model_selection/model_training_intro.html#advanced-modeling-with-lightgbm",
    "href": "docs/model_selection/model_training_intro.html#advanced-modeling-with-lightgbm",
    "title": "Introduction to Model Training",
    "section": "Advanced Modeling with LightGBM",
    "text": "Advanced Modeling with LightGBM\nWhile simple linear models provide a solid baseline, modern production pipelines often utilize Gradient Boosted Trees to capture non-linear relationships and complex interactions. The framework natively supports interchangeable scikit-learn compatible estimators.\nHere is the exact same pipeline from above, but upgraded to use LGBMRegressor from lightgbm. Notice that the architecture of VisualizingForecaster remains entirely modular, demonstrating the flexibility of the ForecasterRecursive wrapper.\n\nimport pandas as pd\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom spotforecast2_safe.forecaster.recursive import ForecasterRecursive\nfrom spotforecast2_safe.data.fetch_data import fetch_data, get_package_data_home\nfrom spotforecast2.manager.trainer_full import train_new_model\nimport plotly.graph_objects as go\n\nclass LGBMVisualizingForecaster:\n    def __init__(self, iteration, end_dev, train_size, dataset_path=None, **kwargs):\n        self.iteration = iteration\n        self.end_dev = end_dev\n        self.train_size = train_size\n        self.dataset_path = dataset_path\n        \n        # Inject LightGBM instead of Ridge\n        self.forecaster = ForecasterRecursive(\n            estimator=LGBMRegressor(n_estimators=100, learning_rate=0.05, random_state=42, verbose=-1), \n            lags=24\n        )\n        self.name = \"demo02_lgbm_model\"\n\n    def tune(self):\n        df = fetch_data(filename=self.dataset_path)\n        y = df[\"A\"].groupby(level=0).mean().asfreq(\"h\").ffill()\n        \n        y_train = y.loc[:self.end_dev]\n        if self.train_size is not None:\n            start_date = pd.to_datetime(self.end_dev, utc=True) - self.train_size\n            y_train = y_train.loc[start_date:]\n            \n        print(f\"Fitting LGBM locally on {len(y_train)} points until {self.end_dev}\")\n        self.forecaster.fit(y=y_train)\n\n    def get_params(self): return {}\n\ndemo_file = get_package_data_home() / \"demo02.csv\"\ndf_full = fetch_data(filename=str(demo_file))\ny_full = df_full[\"A\"].groupby(level=0).mean().asfreq(\"h\").ffill()\n\ntest_duration = pd.Timedelta(days=7)\ncutoff_date = y_full.index.max() - test_duration\n\nmodel_lgbm = train_new_model(\n    model_class=LGBMVisualizingForecaster,\n    n_iteration=1,\n    train_size=pd.Timedelta(days=60), \n    end_dev=cutoff_date, \n    data_filename=str(demo_file), \n    save_to_file=False, \n    dataset_path=str(demo_file)\n)\n\ny_test = y_full.loc[cutoff_date + pd.Timedelta(hours=1):]\npreds = model_lgbm.forecaster.predict(steps=len(y_test))\npreds.index = y_test.index  \n\nmae = mean_absolute_error(y_test, preds)\nprint(f\"LGBM Validation MAE: {mae:.3f}\")\n\n# Generate interactive verification layer\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=y_test.index, y=y_test, mode=\"lines\", name=\"Actual Truth\"))\nfig.add_trace(go.Scatter(\n    x=preds.index, y=preds, mode=\"lines\", \n    name=\"LGBM Projection\", \n    line=dict(dash=\"dash\", color=\"purple\")\n))\n\nfig.update_layout(\n    title=f\"demo02.csv LightGBM Quality Appraisal (MAE: {mae:.3f})\",\n    xaxis_title=\"Time (UTC)\",\n    yaxis_title=\"Target Sensor: A\",\n    template=\"plotly_white\",\n    hovermode=\"x unified\"\n)\n# fig.show() \n\nFitting LGBM locally on 1441 points until 1975-06-11 18:00:00+00:00\nLGBM Validation MAE: 0.098",
    "crumbs": [
      "Model Selection Guides",
      "Intro to Model Training"
    ]
  },
  {
    "objectID": "docs/model_selection/spotoptim_lags.html",
    "href": "docs/model_selection/spotoptim_lags.html",
    "title": "Lag Handling in SpotOptim Search",
    "section": "",
    "text": "When performing hyperparameter optimization with spotoptim_search_forecaster, the lags parameter can be tuned just like any other model parameter. This guide explains the two primary ways to specify lag search spaces and how they are handled internally.",
    "crumbs": [
      "Model Selection Guides",
      "SpotOptim Lag Handling"
    ]
  },
  {
    "objectID": "docs/model_selection/spotoptim_lags.html#specifying-search-spaces-for-lags",
    "href": "docs/model_selection/spotoptim_lags.html#specifying-search-spaces-for-lags",
    "title": "Lag Handling in SpotOptim Search",
    "section": "Specifying Search Spaces for Lags",
    "text": "Specifying Search Spaces for Lags\nIn spotforecast2, lags can be specified either as a numeric range (for searching the number of recent observations) or as a discrete set of configurations (for searching specific offsets).\n\n1. Numeric Range (Integer Search)\nIf you want to search for the optimal number of lags (e.g., between 2 and 24), you can provide a tuple of integers.\nsearch_space = {\n    \"lags\": (2, 24),\n    \"alpha\": (0.01, 1.0)\n}\nInternal Mapping:\n\nThis is mapped to an integer variable type (\"int\") in SpotOptim.\nSpotOptim treats this as a bounded search space [2, 24].\nDuring evaluation, if SpotOptim selects a value like 5, spotforecast2 automatically expands this into a full lag array: [1, 2, 3, 4, 5].\n\n\n\n2. Discrete Configurations (Categorical Search)\nOften, specific lag patterns are more effective than a simple range (e.g., “only the same hour yesterday” vs “all 24 hours”). You can specify these as a list of strings representing the configurations.\nsearch_space = {\n    \"lags\": [\"24\", \"48\", \"[1, 2, 24, 48]\"],\n    \"max_depth\": (3, 10)\n}\nInternal Mapping:\n\nThis is mapped to a factor variable type (\"factor\") in SpotOptim.\nEach string in the list is treated as a discrete category.\nSpotOptim selects one of the strings (e.g., \"[1, 2, 24, 48]\").\nspotforecast2 parses this string back into a Python object (e.g., a list of integers) and applies it to the forecaster.",
    "crumbs": [
      "Model Selection Guides",
      "SpotOptim Lag Handling"
    ]
  },
  {
    "objectID": "docs/model_selection/spotoptim_lags.html#summary-of-mapping-logic",
    "href": "docs/model_selection/spotoptim_lags.html#summary-of-mapping-logic",
    "title": "Lag Handling in SpotOptim Search",
    "section": "Summary of Mapping Logic",
    "text": "Summary of Mapping Logic\nThe following table summarizes how different Python types in the search space dictionary are mapped to SpotOptim’s internal representations:\n\n\n\n\n\n\n\n\n\nUser Input Type\nExample\nSpotOptim Type\nInternal Handling\n\n\n\n\nTuple of ints\n(2, 24)\nint\nMaps to numeric range \\([LB, UB]\\).\n\n\nTuple of floats\n(0.1, 1.0)\nfloat\nMaps to numeric range \\([LB, UB]\\).\n\n\nList of strings\n[\"24\", \"48\"]\nfactor\nMaps to discrete categories.",
    "crumbs": [
      "Model Selection Guides",
      "SpotOptim Lag Handling"
    ]
  },
  {
    "objectID": "docs/model_selection/spotoptim_lags.html#why-strings-for-lag-lists",
    "href": "docs/model_selection/spotoptim_lags.html#why-strings-for-lag-lists",
    "title": "Lag Handling in SpotOptim Search",
    "section": "Why Strings for Lag Lists?",
    "text": "Why Strings for Lag Lists?\nIn the categorical search space, lists of lags are provided as strings (e.g., \"[1, 2, 24]\" instead of [1, 2, 24]). This is because:\n\nSpotOptim Factors: SpotOptim’s categorical interface expects strings to distinguish different configurations.\nAmbiguity Avoidance: Using a string \"[1, 12, 24]\" ensures that the entire configuration is treated as a single “choice” rather than a range of values.",
    "crumbs": [
      "Model Selection Guides",
      "SpotOptim Lag Handling"
    ]
  },
  {
    "objectID": "docs/model_selection/spotoptim_lags.html#implementation-details",
    "href": "docs/model_selection/spotoptim_lags.html#implementation-details",
    "title": "Lag Handling in SpotOptim Search",
    "section": "Implementation Details",
    "text": "Implementation Details\nThe mapping is handled via two internal utilities:\n\nconvert_search_space: Converts the dictionary into SpotOptim’s bounds, var_type, and var_name arrays.\nparse_lags_from_strings: Converts the selected choice back into a usable lag object (integer or list).\n\nThese utilities ensure that the optimization process is seamless, regardless of whether you are searching across a continuous range or a discrete set of domain-specific lag patterns.",
    "crumbs": [
      "Model Selection Guides",
      "SpotOptim Lag Handling"
    ]
  },
  {
    "objectID": "docs/reference/forecaster.metrics.html",
    "href": "docs/reference/forecaster.metrics.html",
    "title": "forecaster.metrics",
    "section": "",
    "text": "forecaster.metrics\nforecaster.metrics\nMetrics for evaluating forecasting models.\nThis module provides various metric functions for evaluating forecasting performance, including custom metrics like MASE, RMSSE, and probabilistic metrics like CRPS. These metrics are imported from the spotforecast2_safe package.",
    "crumbs": [
      "API Reference",
      "Forecaster",
      "metrics"
    ]
  },
  {
    "objectID": "docs/reference/forecaster.utils.html",
    "href": "docs/reference/forecaster.utils.html",
    "title": "forecaster.utils",
    "section": "",
    "text": "forecaster.utils\n\n\n\n\n\nName\nDescription\n\n\n\n\ncheck_residuals_input\nCheck residuals input arguments in Forecasters.\n\n\ndate_to_index_position\nTransform a datetime string or pandas Timestamp to an integer. The integer\n\n\nexog_to_direct_numpy\nTransforms exog to numpy ndarray with the shape needed for Direct\n\n\ninitialize_estimator\nHelper to handle the deprecation of ‘regressor’ in favor of ‘estimator’.\n\n\ninitialize_transformer_series\nInitialize transformer_series_ attribute for multivariate/multiseries forecasters.\n\n\ninitialize_window_features\nCheck window_features argument input and generate the corresponding list.\n\n\npredict_multivariate\nGenerate multi-output predictions using multiple baseline forecasters.\n\n\nprepare_steps_direct\nPrepare list of steps to be predicted in Direct Forecasters.\n\n\nselect_n_jobs_fit_forecaster\nSelect the number of jobs to run in parallel.\n\n\ntransform_numpy\nTransform raw values of a numpy ndarray with a scikit-learn alike\n\n\n\n\n\nforecaster.utils.check_residuals_input(\n    forecaster_name,\n    use_in_sample_residuals,\n    in_sample_residuals_,\n    out_sample_residuals_,\n    use_binned_residuals,\n    in_sample_residuals_by_bin_,\n    out_sample_residuals_by_bin_,\n    levels=None,\n    encoding=None,\n)\nCheck residuals input arguments in Forecasters.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nforecaster_name\nstr\nstr Forecaster name.\nrequired\n\n\nuse_in_sample_residuals\nbool\nbool Indicates if in sample or out sample residuals are used.\nrequired\n\n\nin_sample_residuals_\nnp.ndarray | dict[str, np.ndarray] | None\nnumpy ndarray, dict Residuals of the model when predicting training data.\nrequired\n\n\nout_sample_residuals_\nnp.ndarray | dict[str, np.ndarray] | None\nnumpy ndarray, dict Residuals of the model when predicting non training data.\nrequired\n\n\nuse_binned_residuals\nbool\nbool Indicates if residuals are binned.\nrequired\n\n\nin_sample_residuals_by_bin_\ndict[str | int, np.ndarray | dict[int, np.ndarray]] | None\ndict In sample residuals binned according to the predicted value each residual is associated with.\nrequired\n\n\nout_sample_residuals_by_bin_\ndict[str | int, np.ndarray | dict[int, np.ndarray]] | None\ndict Out of sample residuals binned according to the predicted value each residual is associated with.\nrequired\n\n\nlevels\nlist[str] | None\nlist, default None Names of the series (levels) to be predicted (Forecasters multiseries).\nNone\n\n\nencoding\nstr | None\nstr, default None Encoding used to identify the different series (ForecasterRecursiveMultiSeries).\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n\nforecaster.utils.date_to_index_position(\n    index,\n    date_input,\n    method='prediction',\n    date_literal='steps',\n    kwargs_pd_to_datetime=None,\n)\nTransform a datetime string or pandas Timestamp to an integer. The integer represents the position of the datetime in the index.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nindex\npd.Index\npandas Index Original datetime index (must be a pandas DatetimeIndex if date_input is not an int).\nrequired\n\n\ndate_input\nint | str | pd.Timestamp\nint, str, pandas Timestamp Datetime to transform to integer. - If int, returns the same integer. - If str or pandas Timestamp, it is converted and expanded into the index.\nrequired\n\n\nmethod\nstr\nstr, default ‘prediction’ Can be ‘prediction’ or ‘validation’. - If ‘prediction’, the date must be later than the last date in the index. - If ‘validation’, the date must be within the index range.\n'prediction'\n\n\ndate_literal\nstr\nstr, default ‘steps’ Variable name used in error messages.\n'steps'\n\n\nkwargs_pd_to_datetime\ndict | None\ndict, default {} Additional keyword arguments to pass to pd.to_datetime().\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nint\nint\ndate_input transformed to integer position in the index.\n\n\n\nint\n+ If date_input is an integer, it returns the same integer.\n\n\n\nint\n+ If method is ‘prediction’, number of steps to predict from the last\n\n\n\nint\ndate in the index.\n\n\n\nint\n+ If method is ‘validation’, position plus one of the date in the index,\n\n\n\nint\nthis is done to include the target date in the training set when using\n\n\n\nint\npandas iloc with slices.\n\n\n\n\n\n\n\nforecaster.utils.exog_to_direct_numpy(exog, steps)\nTransforms exog to numpy ndarray with the shape needed for Direct forecasting.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexog\nnp.ndarray | pd.Series | pd.DataFrame\nnumpy ndarray, pandas Series, pandas DataFrame Exogenous variables, shape(samples,). If exog is a pandas format, the direct exog names are created.\nrequired\n\n\nsteps\nint\nint Number of steps that will be predicted using exog.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple[np.ndarray, list[str] | None]\ntuple[np.ndarray, list[str] | None]: exog_direct: numpy ndarray Exogenous variables transformed. exog_direct_names: list, None Names of the columns of the exogenous variables transformed. Only created if exog is a pandas Series or DataFrame.\n\n\n\n\n\n\n\nforecaster.utils.initialize_estimator(estimator=None, regressor=None)\nHelper to handle the deprecation of ‘regressor’ in favor of ‘estimator’. Returns the valid estimator object.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nestimator\nobject | None\nestimator or pipeline compatible with the scikit-learn API, default None An instance of a estimator or pipeline compatible with the scikit-learn API.\nNone\n\n\nregressor\nobject | None\nestimator or pipeline compatible with the scikit-learn API, default None Deprecated. An instance of a estimator or pipeline compatible with the scikit-learn API.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nestimator or pipeline compatible with the scikit-learn API The valid estimator object.\n\n\n\n\n\n\n\nforecaster.utils.initialize_transformer_series(\n    forecaster_name,\n    series_names_in_,\n    encoding=None,\n    transformer_series=None,\n)\nInitialize transformer_series_ attribute for multivariate/multiseries forecasters.\nCreates a dictionary of transformers for each time series in multivariate or multiseries forecasting. Handles three cases: no transformation (None), same transformer for all series (single object), or different transformers per series (dictionary). Clones transformer objects to avoid overwriting.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nforecaster_name\nstr\nName of the forecaster using this function. Special handling is applied for ‘ForecasterRecursiveMultiSeries’.\nrequired\n\n\nseries_names_in_\nlist[str]\nNames of the time series (levels) used during training. These will be the keys in the returned transformer dictionary.\nrequired\n\n\nencoding\nstr | None\nEncoding used to identify different series. Only used for ForecasterRecursiveMultiSeries. If None, creates a single ’_unknown_level’ entry. Defaults to None.\nNone\n\n\ntransformer_series\nobject | dict[str, object | None] | None\nTransformer(s) to apply to series. Can be: - None: No transformation applied - Single transformer object: Same transformer cloned for all series - Dict mapping series names to transformers: Different transformer per series Defaults to None.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndict\ndict[str, object | None]\nDictionary with series names as keys and transformer objects (or None) as values. Transformers are cloned to prevent overwriting.\n\n\n\n\n\n\nIf transformer_series is a dict and some series_names_in_ are not present in the dict keys (those series get no transformation).\n\n\n\nNo transformation:\n&gt;&gt;&gt; from spotforecast2.forecaster.utils import initialize_transformer_series\n&gt;&gt;&gt; series = ['series1', 'series2', 'series3']\n&gt;&gt;&gt; result = initialize_transformer_series(\n...     forecaster_name='ForecasterDirectMultiVariate',\n...     series_names_in_=series,\n...     transformer_series=None\n... )\n&gt;&gt;&gt; print(result)\n{'series1': None, 'series2': None, 'series3': None}\nSame transformer for all series:\n&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n&gt;&gt;&gt; scaler = StandardScaler()\n&gt;&gt;&gt; result = initialize_transformer_series(\n...     forecaster_name='ForecasterDirectMultiVariate',\n...     series_names_in_=['series1', 'series2'],\n...     transformer_series=scaler\n... )\n&gt;&gt;&gt; len(result)\n2\n&gt;&gt;&gt; all(isinstance(v, StandardScaler) for v in result.values())\nTrue\n&gt;&gt;&gt; result['series1'] is result['series2']  # Different clones\nFalse\nDifferent transformer per series:\n&gt;&gt;&gt; from sklearn.preprocessing import MinMaxScaler\n&gt;&gt;&gt; transformers = {\n...     'series1': StandardScaler(),\n...     'series2': MinMaxScaler()\n... }\n&gt;&gt;&gt; result = initialize_transformer_series(\n...     forecaster_name='ForecasterDirectMultiVariate',\n...     series_names_in_=['series1', 'series2'],\n...     transformer_series=transformers\n... )\n&gt;&gt;&gt; isinstance(result['series1'], StandardScaler)\nTrue\n&gt;&gt;&gt; isinstance(result['series2'], MinMaxScaler)\nTrue\n\n\n\n\nforecaster.utils.initialize_window_features(window_features)\nCheck window_features argument input and generate the corresponding list.\nThis function validates window feature objects and extracts their metadata, ensuring they have the required attributes (window_sizes, features_names) and methods (transform_batch, transform) for proper forecasting operations.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwindow_features\nAny\nClasses used to create window features. Can be a single object or a list of objects. Each object must have window_sizes, features_names attributes and transform_batch, transform methods.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ntuple\nTuple[Optional[List[object]], Optional[List[str]], Optional[int]]\nA tuple containing: - window_features (list or None): List of classes used to create window features. - window_features_names (list or None): List with all the features names of the window features. - max_size_window_features (int or None): Maximum value of the window_sizes attribute of all classes.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf window_features is an empty list.\n\n\n\nValueError\nIf a window feature is missing required attributes or methods.\n\n\n\nTypeError\nIf window_sizes or features_names have incorrect types.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotforecast2.forecaster.preprocessing import RollingFeatures\n&gt;&gt;&gt; wf = RollingFeatures(stats=['mean', 'std'], window_sizes=[7, 14])\n&gt;&gt;&gt; wf_list, names, max_size = initialize_window_features(wf)\n&gt;&gt;&gt; print(f\"Max window size: {max_size}\")\nMax window size: 14\n&gt;&gt;&gt; print(f\"Number of features: {len(names)}\")\nNumber of features: 4\nMultiple window features:\n&gt;&gt;&gt; wf1 = RollingFeatures(stats=['mean'], window_sizes=7)\n&gt;&gt;&gt; wf2 = RollingFeatures(stats=['max', 'min'], window_sizes=3)\n&gt;&gt;&gt; wf_list, names, max_size = initialize_window_features([wf1, wf2])\n&gt;&gt;&gt; print(f\"Max window size: {max_size}\")\nMax window size: 7\n\n\n\n\nforecaster.utils.predict_multivariate(\n    forecasters,\n    steps_ahead,\n    exog=None,\n    show_progress=False,\n)\nGenerate multi-output predictions using multiple baseline forecasters.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nforecasters\ndict\nDictionary of fitted forecaster instances (one per target). Keys are target names, values are the fitted forecasters (e.g., ForecasterRecursive, ForecasterEquivalentDate).\nrequired\n\n\nsteps_ahead\nint\nNumber of steps to forecast.\nrequired\n\n\nexog\npd.DataFrame\nExogenous variables for prediction. If provided, will be passed to each forecaster’s predict method.\nNone\n\n\nshow_progress\nbool\nShow progress bar while predicting per target forecaster. Default: False.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: DataFrame with predictions for all targets.\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from spotforecast2.forecaster.utils import predict_multivariate\n&gt;&gt;&gt; y1 = pd.Series([1, 2, 3, 4, 5])\n&gt;&gt;&gt; y2 = pd.Series([2, 4, 6, 8, 10])\n&gt;&gt;&gt; f1 = ForecasterRecursive(estimator=LinearRegression(), lags=2)\n&gt;&gt;&gt; f2 = ForecasterRecursive(estimator=LinearRegression(), lags=2)\n&gt;&gt;&gt; f1.fit(y=y1)\n&gt;&gt;&gt; f2.fit(y=y2)\n&gt;&gt;&gt; forecasters = {'target1': f1, 'target2': f2}\n&gt;&gt;&gt; predictions = predict_multivariate(forecasters, steps_ahead=2)\n&gt;&gt;&gt; predictions\n   target1  target2\n5      6.0     12.0\n6      7.0     14.0\n\n\n\n\nforecaster.utils.prepare_steps_direct(max_step, steps=None)\nPrepare list of steps to be predicted in Direct Forecasters.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmax_step\nint | list[int] | np.ndarray\nint, list, numpy ndarray Maximum number of future steps the forecaster will predict when using predict methods.\nrequired\n\n\nsteps\nint | list[int] | None\nint, list, None, default None Predict n steps. The value of steps must be less than or equal to the value of steps defined when initializing the forecaster. Starts at 1. - If int: Only steps within the range of 1 to int are predicted. - If list: List of ints. Only the steps contained in the list are predicted. - If None: As many steps are predicted as were defined at initialization.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[int]\nlist[int]: Steps to be predicted.\n\n\n\n\n\n\n\nforecaster.utils.select_n_jobs_fit_forecaster(forecaster_name, estimator)\nSelect the number of jobs to run in parallel.\n\n\n\nforecaster.utils.transform_numpy(\n    array,\n    transformer,\n    fit=False,\n    inverse_transform=False,\n)\nTransform raw values of a numpy ndarray with a scikit-learn alike transformer, preprocessor or ColumnTransformer. The transformer used must have the following methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\narray\nnp.ndarray\nnumpy ndarray Array to be transformed.\nrequired\n\n\ntransformer\nobject | None\nscikit-learn alike transformer, preprocessor, or ColumnTransformer. Scikit-learn alike transformer (preprocessor) with methods: fit, transform, fit_transform and inverse_transform.\nrequired\n\n\n\nfit: bool, default False Train the transformer before applying it. inverse_transform: bool, default False Transform back the data to the original representation. This is not available when using transformers of class scikit-learn ColumnTransformers.\n\n\n\narray_transformed : numpy ndarray Transformed array.",
    "crumbs": [
      "API Reference",
      "Forecaster",
      "utils"
    ]
  },
  {
    "objectID": "docs/reference/forecaster.utils.html#functions",
    "href": "docs/reference/forecaster.utils.html#functions",
    "title": "forecaster.utils",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncheck_residuals_input\nCheck residuals input arguments in Forecasters.\n\n\ndate_to_index_position\nTransform a datetime string or pandas Timestamp to an integer. The integer\n\n\nexog_to_direct_numpy\nTransforms exog to numpy ndarray with the shape needed for Direct\n\n\ninitialize_estimator\nHelper to handle the deprecation of ‘regressor’ in favor of ‘estimator’.\n\n\ninitialize_transformer_series\nInitialize transformer_series_ attribute for multivariate/multiseries forecasters.\n\n\ninitialize_window_features\nCheck window_features argument input and generate the corresponding list.\n\n\npredict_multivariate\nGenerate multi-output predictions using multiple baseline forecasters.\n\n\nprepare_steps_direct\nPrepare list of steps to be predicted in Direct Forecasters.\n\n\nselect_n_jobs_fit_forecaster\nSelect the number of jobs to run in parallel.\n\n\ntransform_numpy\nTransform raw values of a numpy ndarray with a scikit-learn alike\n\n\n\n\n\nforecaster.utils.check_residuals_input(\n    forecaster_name,\n    use_in_sample_residuals,\n    in_sample_residuals_,\n    out_sample_residuals_,\n    use_binned_residuals,\n    in_sample_residuals_by_bin_,\n    out_sample_residuals_by_bin_,\n    levels=None,\n    encoding=None,\n)\nCheck residuals input arguments in Forecasters.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nforecaster_name\nstr\nstr Forecaster name.\nrequired\n\n\nuse_in_sample_residuals\nbool\nbool Indicates if in sample or out sample residuals are used.\nrequired\n\n\nin_sample_residuals_\nnp.ndarray | dict[str, np.ndarray] | None\nnumpy ndarray, dict Residuals of the model when predicting training data.\nrequired\n\n\nout_sample_residuals_\nnp.ndarray | dict[str, np.ndarray] | None\nnumpy ndarray, dict Residuals of the model when predicting non training data.\nrequired\n\n\nuse_binned_residuals\nbool\nbool Indicates if residuals are binned.\nrequired\n\n\nin_sample_residuals_by_bin_\ndict[str | int, np.ndarray | dict[int, np.ndarray]] | None\ndict In sample residuals binned according to the predicted value each residual is associated with.\nrequired\n\n\nout_sample_residuals_by_bin_\ndict[str | int, np.ndarray | dict[int, np.ndarray]] | None\ndict Out of sample residuals binned according to the predicted value each residual is associated with.\nrequired\n\n\nlevels\nlist[str] | None\nlist, default None Names of the series (levels) to be predicted (Forecasters multiseries).\nNone\n\n\nencoding\nstr | None\nstr, default None Encoding used to identify the different series (ForecasterRecursiveMultiSeries).\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n\nforecaster.utils.date_to_index_position(\n    index,\n    date_input,\n    method='prediction',\n    date_literal='steps',\n    kwargs_pd_to_datetime=None,\n)\nTransform a datetime string or pandas Timestamp to an integer. The integer represents the position of the datetime in the index.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nindex\npd.Index\npandas Index Original datetime index (must be a pandas DatetimeIndex if date_input is not an int).\nrequired\n\n\ndate_input\nint | str | pd.Timestamp\nint, str, pandas Timestamp Datetime to transform to integer. - If int, returns the same integer. - If str or pandas Timestamp, it is converted and expanded into the index.\nrequired\n\n\nmethod\nstr\nstr, default ‘prediction’ Can be ‘prediction’ or ‘validation’. - If ‘prediction’, the date must be later than the last date in the index. - If ‘validation’, the date must be within the index range.\n'prediction'\n\n\ndate_literal\nstr\nstr, default ‘steps’ Variable name used in error messages.\n'steps'\n\n\nkwargs_pd_to_datetime\ndict | None\ndict, default {} Additional keyword arguments to pass to pd.to_datetime().\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nint\nint\ndate_input transformed to integer position in the index.\n\n\n\nint\n+ If date_input is an integer, it returns the same integer.\n\n\n\nint\n+ If method is ‘prediction’, number of steps to predict from the last\n\n\n\nint\ndate in the index.\n\n\n\nint\n+ If method is ‘validation’, position plus one of the date in the index,\n\n\n\nint\nthis is done to include the target date in the training set when using\n\n\n\nint\npandas iloc with slices.\n\n\n\n\n\n\n\nforecaster.utils.exog_to_direct_numpy(exog, steps)\nTransforms exog to numpy ndarray with the shape needed for Direct forecasting.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexog\nnp.ndarray | pd.Series | pd.DataFrame\nnumpy ndarray, pandas Series, pandas DataFrame Exogenous variables, shape(samples,). If exog is a pandas format, the direct exog names are created.\nrequired\n\n\nsteps\nint\nint Number of steps that will be predicted using exog.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple[np.ndarray, list[str] | None]\ntuple[np.ndarray, list[str] | None]: exog_direct: numpy ndarray Exogenous variables transformed. exog_direct_names: list, None Names of the columns of the exogenous variables transformed. Only created if exog is a pandas Series or DataFrame.\n\n\n\n\n\n\n\nforecaster.utils.initialize_estimator(estimator=None, regressor=None)\nHelper to handle the deprecation of ‘regressor’ in favor of ‘estimator’. Returns the valid estimator object.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nestimator\nobject | None\nestimator or pipeline compatible with the scikit-learn API, default None An instance of a estimator or pipeline compatible with the scikit-learn API.\nNone\n\n\nregressor\nobject | None\nestimator or pipeline compatible with the scikit-learn API, default None Deprecated. An instance of a estimator or pipeline compatible with the scikit-learn API.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nestimator or pipeline compatible with the scikit-learn API The valid estimator object.\n\n\n\n\n\n\n\nforecaster.utils.initialize_transformer_series(\n    forecaster_name,\n    series_names_in_,\n    encoding=None,\n    transformer_series=None,\n)\nInitialize transformer_series_ attribute for multivariate/multiseries forecasters.\nCreates a dictionary of transformers for each time series in multivariate or multiseries forecasting. Handles three cases: no transformation (None), same transformer for all series (single object), or different transformers per series (dictionary). Clones transformer objects to avoid overwriting.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nforecaster_name\nstr\nName of the forecaster using this function. Special handling is applied for ‘ForecasterRecursiveMultiSeries’.\nrequired\n\n\nseries_names_in_\nlist[str]\nNames of the time series (levels) used during training. These will be the keys in the returned transformer dictionary.\nrequired\n\n\nencoding\nstr | None\nEncoding used to identify different series. Only used for ForecasterRecursiveMultiSeries. If None, creates a single ’_unknown_level’ entry. Defaults to None.\nNone\n\n\ntransformer_series\nobject | dict[str, object | None] | None\nTransformer(s) to apply to series. Can be: - None: No transformation applied - Single transformer object: Same transformer cloned for all series - Dict mapping series names to transformers: Different transformer per series Defaults to None.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndict\ndict[str, object | None]\nDictionary with series names as keys and transformer objects (or None) as values. Transformers are cloned to prevent overwriting.\n\n\n\n\n\n\nIf transformer_series is a dict and some series_names_in_ are not present in the dict keys (those series get no transformation).\n\n\n\nNo transformation:\n&gt;&gt;&gt; from spotforecast2.forecaster.utils import initialize_transformer_series\n&gt;&gt;&gt; series = ['series1', 'series2', 'series3']\n&gt;&gt;&gt; result = initialize_transformer_series(\n...     forecaster_name='ForecasterDirectMultiVariate',\n...     series_names_in_=series,\n...     transformer_series=None\n... )\n&gt;&gt;&gt; print(result)\n{'series1': None, 'series2': None, 'series3': None}\nSame transformer for all series:\n&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n&gt;&gt;&gt; scaler = StandardScaler()\n&gt;&gt;&gt; result = initialize_transformer_series(\n...     forecaster_name='ForecasterDirectMultiVariate',\n...     series_names_in_=['series1', 'series2'],\n...     transformer_series=scaler\n... )\n&gt;&gt;&gt; len(result)\n2\n&gt;&gt;&gt; all(isinstance(v, StandardScaler) for v in result.values())\nTrue\n&gt;&gt;&gt; result['series1'] is result['series2']  # Different clones\nFalse\nDifferent transformer per series:\n&gt;&gt;&gt; from sklearn.preprocessing import MinMaxScaler\n&gt;&gt;&gt; transformers = {\n...     'series1': StandardScaler(),\n...     'series2': MinMaxScaler()\n... }\n&gt;&gt;&gt; result = initialize_transformer_series(\n...     forecaster_name='ForecasterDirectMultiVariate',\n...     series_names_in_=['series1', 'series2'],\n...     transformer_series=transformers\n... )\n&gt;&gt;&gt; isinstance(result['series1'], StandardScaler)\nTrue\n&gt;&gt;&gt; isinstance(result['series2'], MinMaxScaler)\nTrue\n\n\n\n\nforecaster.utils.initialize_window_features(window_features)\nCheck window_features argument input and generate the corresponding list.\nThis function validates window feature objects and extracts their metadata, ensuring they have the required attributes (window_sizes, features_names) and methods (transform_batch, transform) for proper forecasting operations.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwindow_features\nAny\nClasses used to create window features. Can be a single object or a list of objects. Each object must have window_sizes, features_names attributes and transform_batch, transform methods.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ntuple\nTuple[Optional[List[object]], Optional[List[str]], Optional[int]]\nA tuple containing: - window_features (list or None): List of classes used to create window features. - window_features_names (list or None): List with all the features names of the window features. - max_size_window_features (int or None): Maximum value of the window_sizes attribute of all classes.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf window_features is an empty list.\n\n\n\nValueError\nIf a window feature is missing required attributes or methods.\n\n\n\nTypeError\nIf window_sizes or features_names have incorrect types.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotforecast2.forecaster.preprocessing import RollingFeatures\n&gt;&gt;&gt; wf = RollingFeatures(stats=['mean', 'std'], window_sizes=[7, 14])\n&gt;&gt;&gt; wf_list, names, max_size = initialize_window_features(wf)\n&gt;&gt;&gt; print(f\"Max window size: {max_size}\")\nMax window size: 14\n&gt;&gt;&gt; print(f\"Number of features: {len(names)}\")\nNumber of features: 4\nMultiple window features:\n&gt;&gt;&gt; wf1 = RollingFeatures(stats=['mean'], window_sizes=7)\n&gt;&gt;&gt; wf2 = RollingFeatures(stats=['max', 'min'], window_sizes=3)\n&gt;&gt;&gt; wf_list, names, max_size = initialize_window_features([wf1, wf2])\n&gt;&gt;&gt; print(f\"Max window size: {max_size}\")\nMax window size: 7\n\n\n\n\nforecaster.utils.predict_multivariate(\n    forecasters,\n    steps_ahead,\n    exog=None,\n    show_progress=False,\n)\nGenerate multi-output predictions using multiple baseline forecasters.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nforecasters\ndict\nDictionary of fitted forecaster instances (one per target). Keys are target names, values are the fitted forecasters (e.g., ForecasterRecursive, ForecasterEquivalentDate).\nrequired\n\n\nsteps_ahead\nint\nNumber of steps to forecast.\nrequired\n\n\nexog\npd.DataFrame\nExogenous variables for prediction. If provided, will be passed to each forecaster’s predict method.\nNone\n\n\nshow_progress\nbool\nShow progress bar while predicting per target forecaster. Default: False.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: DataFrame with predictions for all targets.\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from spotforecast2.forecaster.utils import predict_multivariate\n&gt;&gt;&gt; y1 = pd.Series([1, 2, 3, 4, 5])\n&gt;&gt;&gt; y2 = pd.Series([2, 4, 6, 8, 10])\n&gt;&gt;&gt; f1 = ForecasterRecursive(estimator=LinearRegression(), lags=2)\n&gt;&gt;&gt; f2 = ForecasterRecursive(estimator=LinearRegression(), lags=2)\n&gt;&gt;&gt; f1.fit(y=y1)\n&gt;&gt;&gt; f2.fit(y=y2)\n&gt;&gt;&gt; forecasters = {'target1': f1, 'target2': f2}\n&gt;&gt;&gt; predictions = predict_multivariate(forecasters, steps_ahead=2)\n&gt;&gt;&gt; predictions\n   target1  target2\n5      6.0     12.0\n6      7.0     14.0\n\n\n\n\nforecaster.utils.prepare_steps_direct(max_step, steps=None)\nPrepare list of steps to be predicted in Direct Forecasters.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmax_step\nint | list[int] | np.ndarray\nint, list, numpy ndarray Maximum number of future steps the forecaster will predict when using predict methods.\nrequired\n\n\nsteps\nint | list[int] | None\nint, list, None, default None Predict n steps. The value of steps must be less than or equal to the value of steps defined when initializing the forecaster. Starts at 1. - If int: Only steps within the range of 1 to int are predicted. - If list: List of ints. Only the steps contained in the list are predicted. - If None: As many steps are predicted as were defined at initialization.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[int]\nlist[int]: Steps to be predicted.\n\n\n\n\n\n\n\nforecaster.utils.select_n_jobs_fit_forecaster(forecaster_name, estimator)\nSelect the number of jobs to run in parallel.\n\n\n\nforecaster.utils.transform_numpy(\n    array,\n    transformer,\n    fit=False,\n    inverse_transform=False,\n)\nTransform raw values of a numpy ndarray with a scikit-learn alike transformer, preprocessor or ColumnTransformer. The transformer used must have the following methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\narray\nnp.ndarray\nnumpy ndarray Array to be transformed.\nrequired\n\n\ntransformer\nobject | None\nscikit-learn alike transformer, preprocessor, or ColumnTransformer. Scikit-learn alike transformer (preprocessor) with methods: fit, transform, fit_transform and inverse_transform.\nrequired\n\n\n\nfit: bool, default False Train the transformer before applying it. inverse_transform: bool, default False Transform back the data to the original representation. This is not available when using transformers of class scikit-learn ColumnTransformers.\n\n\n\narray_transformed : numpy ndarray Transformed array.",
    "crumbs": [
      "API Reference",
      "Forecaster",
      "utils"
    ]
  },
  {
    "objectID": "docs/reference/manager.models.html",
    "href": "docs/reference/manager.models.html",
    "title": "manager.models",
    "section": "",
    "text": "manager.models\nFull-featured forecasting model classes with Bayesian tuning and SHAP.\nThis module extends the stub implementations in spotforecast2-safe with real Bayesian hyperparameter search (Optuna) and SHAP-based feature importance (shap.TreeExplainer).\n\n\n\n\n\nName\nDescription\n\n\n\n\nForecasterRecursiveLGBMFull\nLGBM forecaster with real Bayesian tuning and SHAP.\n\n\nForecasterRecursiveModelFull\nForecasterRecursiveModel with real Bayesian tuning and SHAP.\n\n\nForecasterRecursiveXGBFull\nXGBoost forecaster with real Bayesian tuning and SHAP.\n\n\n\n\n\nmanager.models.ForecasterRecursiveLGBMFull(iteration, lags=12, **kwargs)\nLGBM forecaster with real Bayesian tuning and SHAP.\nInherits the LGBM forecaster initialisation from ForecasterRecursiveLGBM (spotforecast2-safe) and adds the real tune() and get_global_shap_feature_importance() from ForecasterRecursiveModelFull.\n\n\n&gt;&gt;&gt; from spotforecast2.manager.models import ForecasterRecursiveLGBMFull\n&gt;&gt;&gt; model = ForecasterRecursiveLGBMFull(iteration=0)\n&gt;&gt;&gt; model.name\n'lgbm'\n&gt;&gt;&gt; model.forecaster is not None\nTrue\n\n\n\n\nmanager.models.ForecasterRecursiveModelFull(\n    iteration,\n    n_trials=_DEFAULT_N_TRIALS,\n    **kwargs,\n)\nForecasterRecursiveModel with real Bayesian tuning and SHAP.\nThis class overrides the two stubs in spotforecast2-safe:\n\n:meth:tune — performs a full Bayesian hyperparameter search using bayesian_search_forecaster (Optuna).\n:meth:get_global_shap_feature_importance — computes global SHAP values using shap.TreeExplainer.\n\n\n\n&gt;&gt;&gt; from spotforecast2.manager.models import ForecasterRecursiveModelFull\n&gt;&gt;&gt; model = ForecasterRecursiveModelFull(iteration=0)\n&gt;&gt;&gt; hasattr(model, 'tune')\nTrue\n&gt;&gt;&gt; hasattr(model, 'get_global_shap_feature_importance')\nTrue\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_global_shap_feature_importance\nReturn global SHAP-based feature importances.\n\n\ntune\nTune the forecaster via Bayesian search (Optuna).\n\n\n\n\n\nmanager.models.ForecasterRecursiveModelFull.get_global_shap_feature_importance(\n    frac=0.1,\n)\nReturn global SHAP-based feature importances.\nUses shap.TreeExplainer on the underlying estimator to compute mean absolute SHAP values across a random sample of the training data.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfrac\nfloat\nFraction of training data to sample (0 &lt; frac &lt;= 1).\n0.1\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: Feature importances sorted descending. Empty\n\n\n\npd.Series\nif the model has not been tuned.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the forecaster has not been initialized.\n\n\n\n\n\n\n\nmanager.models.ForecasterRecursiveModelFull.tune()\nTune the forecaster via Bayesian search (Optuna).\nLoads time-series data, builds exogenous features, and runs bayesian_search_forecaster over the search space registered for self.name in SEARCH_SPACES.\nAfter tuning the model is fitted with the best parameters and, if self.save_model_to_file is True, persisted to disk.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf self.name is not in SEARCH_SPACES.\n\n\n\n\n\n\n\n\n\nmanager.models.ForecasterRecursiveXGBFull(iteration, lags=12, **kwargs)\nXGBoost forecaster with real Bayesian tuning and SHAP.\nInherits the XGBoost forecaster initialisation from ForecasterRecursiveXGB (spotforecast2-safe) and adds the real tune() and get_global_shap_feature_importance() from ForecasterRecursiveModelFull.\n\n\n&gt;&gt;&gt; from spotforecast2.manager.models import ForecasterRecursiveXGBFull\n&gt;&gt;&gt; model = ForecasterRecursiveXGBFull(iteration=0)\n&gt;&gt;&gt; model.name\n'xgb'",
    "crumbs": [
      "API Reference",
      "Manager",
      "models"
    ]
  },
  {
    "objectID": "docs/reference/manager.models.html#classes",
    "href": "docs/reference/manager.models.html#classes",
    "title": "manager.models",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nForecasterRecursiveLGBMFull\nLGBM forecaster with real Bayesian tuning and SHAP.\n\n\nForecasterRecursiveModelFull\nForecasterRecursiveModel with real Bayesian tuning and SHAP.\n\n\nForecasterRecursiveXGBFull\nXGBoost forecaster with real Bayesian tuning and SHAP.\n\n\n\n\n\nmanager.models.ForecasterRecursiveLGBMFull(iteration, lags=12, **kwargs)\nLGBM forecaster with real Bayesian tuning and SHAP.\nInherits the LGBM forecaster initialisation from ForecasterRecursiveLGBM (spotforecast2-safe) and adds the real tune() and get_global_shap_feature_importance() from ForecasterRecursiveModelFull.\n\n\n&gt;&gt;&gt; from spotforecast2.manager.models import ForecasterRecursiveLGBMFull\n&gt;&gt;&gt; model = ForecasterRecursiveLGBMFull(iteration=0)\n&gt;&gt;&gt; model.name\n'lgbm'\n&gt;&gt;&gt; model.forecaster is not None\nTrue\n\n\n\n\nmanager.models.ForecasterRecursiveModelFull(\n    iteration,\n    n_trials=_DEFAULT_N_TRIALS,\n    **kwargs,\n)\nForecasterRecursiveModel with real Bayesian tuning and SHAP.\nThis class overrides the two stubs in spotforecast2-safe:\n\n:meth:tune — performs a full Bayesian hyperparameter search using bayesian_search_forecaster (Optuna).\n:meth:get_global_shap_feature_importance — computes global SHAP values using shap.TreeExplainer.\n\n\n\n&gt;&gt;&gt; from spotforecast2.manager.models import ForecasterRecursiveModelFull\n&gt;&gt;&gt; model = ForecasterRecursiveModelFull(iteration=0)\n&gt;&gt;&gt; hasattr(model, 'tune')\nTrue\n&gt;&gt;&gt; hasattr(model, 'get_global_shap_feature_importance')\nTrue\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_global_shap_feature_importance\nReturn global SHAP-based feature importances.\n\n\ntune\nTune the forecaster via Bayesian search (Optuna).\n\n\n\n\n\nmanager.models.ForecasterRecursiveModelFull.get_global_shap_feature_importance(\n    frac=0.1,\n)\nReturn global SHAP-based feature importances.\nUses shap.TreeExplainer on the underlying estimator to compute mean absolute SHAP values across a random sample of the training data.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfrac\nfloat\nFraction of training data to sample (0 &lt; frac &lt;= 1).\n0.1\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: Feature importances sorted descending. Empty\n\n\n\npd.Series\nif the model has not been tuned.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the forecaster has not been initialized.\n\n\n\n\n\n\n\nmanager.models.ForecasterRecursiveModelFull.tune()\nTune the forecaster via Bayesian search (Optuna).\nLoads time-series data, builds exogenous features, and runs bayesian_search_forecaster over the search space registered for self.name in SEARCH_SPACES.\nAfter tuning the model is fitted with the best parameters and, if self.save_model_to_file is True, persisted to disk.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nKeyError\nIf self.name is not in SEARCH_SPACES.\n\n\n\n\n\n\n\n\n\nmanager.models.ForecasterRecursiveXGBFull(iteration, lags=12, **kwargs)\nXGBoost forecaster with real Bayesian tuning and SHAP.\nInherits the XGBoost forecaster initialisation from ForecasterRecursiveXGB (spotforecast2-safe) and adds the real tune() and get_global_shap_feature_importance() from ForecasterRecursiveModelFull.\n\n\n&gt;&gt;&gt; from spotforecast2.manager.models import ForecasterRecursiveXGBFull\n&gt;&gt;&gt; model = ForecasterRecursiveXGBFull(iteration=0)\n&gt;&gt;&gt; model.name\n'xgb'",
    "crumbs": [
      "API Reference",
      "Manager",
      "models"
    ]
  },
  {
    "objectID": "docs/reference/manager.trainer_full.html",
    "href": "docs/reference/manager.trainer_full.html",
    "title": "manager.trainer_full",
    "section": "",
    "text": "manager.trainer_full\nModule for managing full model training.",
    "crumbs": [
      "API Reference",
      "Manager",
      "trainer_full"
    ]
  },
  {
    "objectID": "docs/reference/manager.trainer_full.html#functions",
    "href": "docs/reference/manager.trainer_full.html#functions",
    "title": "manager.trainer_full",
    "section": "Functions",
    "text": "Functions\n\n\n\nName\nDescription\n\n\n\n\nhandle_training\nCheck if a new model needs to be trained and trigger training if necessary.\n\n\nsearch_space_lgbm\nOptuna search space for LightGBM hyperparameters.\n\n\nsearch_space_xgb\nOptuna search space for XGBoost hyperparameters.\n\n\ntrain_new_model\nTrain a new forecaster model and optionally save it to disk.\n\n\n\n\nhandle_training\nmanager.trainer_full.handle_training(\n    model_class,\n    model_name=None,\n    model_dir=None,\n    force=False,\n    train_size=None,\n    end_dev=None,\n    data_filename=None,\n    **kwargs,\n)\nCheck if a new model needs to be trained and trigger training if necessary.\nTrains a new model if no model exists, if the existing model is older than 7 days, or if retraining is forced.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmodel_class\ntype\nThe class of the forecaster model to train, for example spotforecast2_safe.forecaster.ForecasterLGBM.\nrequired\n\n\nmodel_name\nOptional[str]\nName of the model (e.g., ‘lgbm’). If None, it is inferred from the model_class name.\nNone\n\n\nmodel_dir\nOptional[Union[str, Path]]\nDirectory where models are stored, see also get_cache_home().\nNone\n\n\nforce\nbool\nIf True, force retraining even if the current model is recent. Default is False.\nFalse\n\n\ntrain_size\nOptional[pd.Timedelta]\nOptional size of the training set. Default is None.\nNone\n\n\nend_dev\nOptional[Union[str, pd.Timestamp]]\nOptional cutoff date for training. Default is None.\nNone\n\n\ndata_filename\nOptional[str]\nOptional filename of the data used for training. Default is None.\nNone\n\n\n**kwargs\nAny\nAdditional keyword arguments passed to the model constructor.\n{}\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from unittest.mock import patch\n&gt;&gt;&gt; from spotforecast2.manager.trainer_full import handle_training\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 1: No existing model - triggers training\n&gt;&gt;&gt; class MockModel:\n...     '''Mock model class'''\n...     def __init__(self, iteration, end_dev, train_size=None):\n...         self.iteration = iteration\n...         self.end_dev = end_dev\n...         self.name = 'test'\n...     def tune(self):\n...         pass\n&gt;&gt;&gt;\n&gt;&gt;&gt; with tempfile.TemporaryDirectory() as tmpdir:\n...     with patch('spotforecast2.manager.trainer_full.get_last_model') as mock_get:\n...         with patch('spotforecast2.manager.trainer_full.train_new_model') as mock_train:\n...             mock_get.return_value = (-1, None)\n...             handle_training(MockModel, model_name='test', model_dir=tmpdir)\n...             print(f\"Training called: {mock_train.called}\")\n...             print(f\"Iteration: {mock_train.call_args[0][1]}\")\nTraining called: True\nIteration: 0\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 2: Recent model exists - no retraining\n&gt;&gt;&gt; class RecentModel:\n...     '''Model with recent training'''\n...     def __init__(self):\n...         self.end_dev = pd.Timestamp.now('UTC') - pd.Timedelta(hours=24)\n&gt;&gt;&gt;\n&gt;&gt;&gt; with tempfile.TemporaryDirectory() as tmpdir:\n...     with patch('spotforecast2.manager.trainer_full.get_last_model') as mock_get:\n...         with patch('spotforecast2.manager.trainer_full.train_new_model') as mock_train:\n...             mock_existing = RecentModel()\n...             mock_get.return_value = (1, mock_existing)\n...             handle_training(MockModel, model_name='recent', model_dir=tmpdir)\n...             print(f\"Training skipped: {not mock_train.called}\")\nTraining skipped: True\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 3: Old model exists - triggers retraining\n&gt;&gt;&gt; class OldModel:\n...     '''Model with old training'''\n...     def __init__(self):\n...         self.end_dev = pd.Timestamp.now('UTC') - pd.Timedelta(days=10)\n&gt;&gt;&gt;\n&gt;&gt;&gt; with tempfile.TemporaryDirectory() as tmpdir:\n...     with patch('spotforecast2.manager.trainer_full.get_last_model') as mock_get:\n...         with patch('spotforecast2.manager.trainer_full.train_new_model') as mock_train:\n...             mock_old = OldModel()\n...             mock_get.return_value = (2, mock_old)\n...             handle_training(MockModel, model_name='old', model_dir=tmpdir)\n...             print(f\"Retraining triggered: {mock_train.called}\")\n...             print(f\"New iteration: {mock_train.call_args[0][1]}\")\nRetraining triggered: True\nNew iteration: 3\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 4: Force retraining even with recent model\n&gt;&gt;&gt; with tempfile.TemporaryDirectory() as tmpdir:\n...     with patch('spotforecast2.manager.trainer_full.get_last_model') as mock_get:\n...         with patch('spotforecast2.manager.trainer_full.train_new_model') as mock_train:\n...             mock_recent = RecentModel()\n...             mock_get.return_value = (0, mock_recent)\n...             handle_training(MockModel, model_name='forced', model_dir=tmpdir, force=True)\n...             print(f\"Force training executed: {mock_train.called}\")\nForce training executed: True\n\n\n\nsearch_space_lgbm\nmanager.trainer_full.search_space_lgbm(trial)\nOptuna search space for LightGBM hyperparameters.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntrial\nAny\nAn :class:optuna.trial.Trial instance.\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\ndict\ndict\nSuggested hyperparameters for the current trial.\n\n\n\n\n\nExamples\n&gt;&gt;&gt; from spotforecast2.manager.trainer_full import search_space_lgbm\n&gt;&gt;&gt; # Without Optuna, verify the function signature exists\n&gt;&gt;&gt; callable(search_space_lgbm)\nTrue\n\n\n\nsearch_space_xgb\nmanager.trainer_full.search_space_xgb(trial)\nOptuna search space for XGBoost hyperparameters.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntrial\nAny\nAn :class:optuna.trial.Trial instance.\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\ndict\ndict\nSuggested hyperparameters for the current trial.\n\n\n\n\n\nExamples\n&gt;&gt;&gt; from spotforecast2.manager.trainer_full import search_space_xgb\n&gt;&gt;&gt; callable(search_space_xgb)\nTrue\n\n\n\ntrain_new_model\nmanager.trainer_full.train_new_model(\n    model_class,\n    n_iteration,\n    model_name=None,\n    train_size=None,\n    save_to_file=True,\n    model_dir=None,\n    end_dev=None,\n    data_filename=None,\n    **kwargs,\n)\nTrain a new forecaster model and optionally save it to disk.\nThis function fetches the latest data, calculates the training cutoff, initializes a model of the given class, triggers the tuning process, and saves the model following the naming convention: {model_name}_forecaster_{n_iteration}.joblib.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmodel_class\ntype\nThe class of the forecaster model to train. The class should accept iteration, end_dev, and train_size in its constructor and provide a tune() method.\nrequired\n\n\nn_iteration\nint\nThe iteration number for this training run. This acts as an incrementing version number for the model. When using handle_training, the first model starts at iteration 0. Upon subsequent forced or scheduled retrainings, it is incremented by 1 (get_last_model_iteration + 1). It is primarily used to determine the filename when saving the model to disk (e.g., lgbm_forecaster_0.joblib, lgbm_forecaster_1.joblib).\nrequired\n\n\nmodel_name\nOptional[str]\nOptional name of the model to train. If None, the name is inferred from the model class. Defaults to None.\nNone\n\n\ntrain_size\nOptional[pd.Timedelta]\nOptional size of the training set as a pandas Timedelta. Determines the lookback window length from end_dev. If provided, the training data will start at end_dev - train_size. If None, all available data up to end_dev is used. Defaults to None.\nNone\n\n\nsave_to_file\nbool\nIf True, saves the model to disk after training. Defaults to True.\nTrue\n\n\nmodel_dir\nOptional[Union[str, Path]]\nDirectory where the model should be saved. If None, defaults to the library’s cache home.\nNone\n\n\nend_dev\nOptional[Union[str, pd.Timestamp]]\nOptional cutoff date for training. This represents the absolute point in time separating training/development data from unseen future data. If None, it is calculated automatically to be one day before the latest available index in the data.\nNone\n\n\ndata_filename\nOptional[str]\nOptional filename for the data to be used for training, e.g., ‘interim/energy_load.csv’. If None, the default data file is used. Defaults to None.\nNone\n\n\n**kwargs\nAny\nAdditional keyword arguments to be passed to the model constructor.\n{}\n\n\n\n\n\nNotes\nRelationship between train_size and end_dev: The actual training data spans from max(dataset_start, end_dev - train_size) to end_dev. - If train_size is larger than the available history before end_dev, the framework gracefully clips the start date to the beginning of the dataset without throwing an error. - If end_dev is set to a time before the start of the dataset, the training subset will be empty and the forecaster will fail to fit.\n\n\nExamples\n\nimport pandas as pd\nfrom spotforecast2.manager.trainer_full import train_new_model\n\n# Define a mock model class for demonstration\nclass MyModel:\n    def __init__(self, iteration, end_dev, train_size, **kwargs):\n        self.iteration = iteration\n        self.end_dev = end_dev\n        self.train_size = train_size\n    def tune(self): print(f\"Tuning model {self.iteration} up to {self.end_dev}!\")\n    def get_params(self): return {}\n    @property\n    def name(self): return \"mymodel\"\n\n# Train using exactly 3 years of data leading up to the end of 2025:\n# Note: In a real scenario, this fetches data and saves a joblib file.\n# We pass save_to_file=False to avoid writing disk artifacts in the doc example.\nfrom spotforecast2_safe.data.fetch_data import get_package_data_home\ndemo_file = get_package_data_home() / \"demo01.csv\"\n\nmodel = train_new_model(\n    model_class=MyModel,\n    n_iteration=0,\n    train_size=pd.Timedelta(days=3*365),\n    end_dev=\"2025-12-31 00:00+00:00\",\n    save_to_file=False,\n    data_filename=str(demo_file)\n)\n\nTuning model 0 up to 2025-12-31 00:00:00+00:00!\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAny\nThe trained model instance.",
    "crumbs": [
      "API Reference",
      "Manager",
      "trainer_full"
    ]
  },
  {
    "objectID": "docs/reference/model_selection.grid_search.html",
    "href": "docs/reference/model_selection.grid_search.html",
    "title": "model_selection.grid_search",
    "section": "",
    "text": "model_selection.grid_search\n\n\n\n\n\nName\nDescription\n\n\n\n\ngrid_search_forecaster\nExhaustive grid search over parameter values for a Forecaster.\n\n\n\n\n\nmodel_selection.grid_search.grid_search_forecaster(\n    forecaster,\n    y,\n    cv,\n    param_grid,\n    metric,\n    exog=None,\n    lags_grid=None,\n    return_best=True,\n    n_jobs='auto',\n    verbose=False,\n    show_progress=True,\n    suppress_warnings=False,\n    output_file=None,\n)\nExhaustive grid search over parameter values for a Forecaster.",
    "crumbs": [
      "API Reference",
      "Model Selection",
      "grid_search"
    ]
  },
  {
    "objectID": "docs/reference/model_selection.grid_search.html#functions",
    "href": "docs/reference/model_selection.grid_search.html#functions",
    "title": "model_selection.grid_search",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ngrid_search_forecaster\nExhaustive grid search over parameter values for a Forecaster.\n\n\n\n\n\nmodel_selection.grid_search.grid_search_forecaster(\n    forecaster,\n    y,\n    cv,\n    param_grid,\n    metric,\n    exog=None,\n    lags_grid=None,\n    return_best=True,\n    n_jobs='auto',\n    verbose=False,\n    show_progress=True,\n    suppress_warnings=False,\n    output_file=None,\n)\nExhaustive grid search over parameter values for a Forecaster.",
    "crumbs": [
      "API Reference",
      "Model Selection",
      "grid_search"
    ]
  },
  {
    "objectID": "docs/reference/model_selection.split_base.html",
    "href": "docs/reference/model_selection.split_base.html",
    "title": "model_selection.split_base",
    "section": "",
    "text": "model_selection.split_base\nmodel_selection.split_base\nBase class for time series cross-validation splitting.\nAll classes are imported from spotforecast2_safe.model_selection.split_base.",
    "crumbs": [
      "API Reference",
      "Model Selection",
      "split_base"
    ]
  },
  {
    "objectID": "docs/reference/model_selection.spotoptim_search.html",
    "href": "docs/reference/model_selection.spotoptim_search.html",
    "title": "model_selection.spotoptim_search",
    "section": "",
    "text": "model_selection.spotoptim_search\nHyperparameter search functions for forecasters using SpotOptim.\nThis module provides an alternative to Bayesian (Optuna-based) search by leveraging the SpotOptim surrogate-model-based optimizer. It follows the same interface as :func:spotforecast2.model_selection.bayesian_search_forecaster, so the two can be used interchangeably.",
    "crumbs": [
      "API Reference",
      "Model Selection",
      "spotoptim_search"
    ]
  },
  {
    "objectID": "docs/reference/model_selection.spotoptim_search.html#functions",
    "href": "docs/reference/model_selection.spotoptim_search.html#functions",
    "title": "model_selection.spotoptim_search",
    "section": "Functions",
    "text": "Functions\n\n\n\nName\nDescription\n\n\n\n\narray_to_params\nConvert a SpotOptim parameter array back to a dict.\n\n\nconvert_search_space\nConvert search space into SpotOptim compatible format.\n\n\nparse_lags_from_strings\nParse a lags representation back to a Python object.\n\n\nspotoptim_objective\nSpotOptim objective function to evaluate hyperparameter sets.\n\n\nspotoptim_search\nCore implementation of the SpotOptim search logic.\n\n\nspotoptim_search_forecaster\nHyperparameter optimisation for a Forecaster using SpotOptim.\n\n\n\n\narray_to_params\nmodel_selection.spotoptim_search.array_to_params(\n    params_array,\n    var_name,\n    var_type,\n    bounds,\n)\nConvert a SpotOptim parameter array back to a dict.\nEach element of params_array is mapped to the corresponding name / type / bounds entry, converting to the correct Python type.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparams_array\nnp.ndarray\n1-D array of raw parameter values from SpotOptim.\nrequired\n\n\nvar_name\nlist\nParameter names (same order as params_array).\nrequired\n\n\nvar_type\nlist\nParameter types (\"int\", \"float\", \"factor\").\nrequired\n\n\nbounds\nlist\nParameter bounds.\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDict[str, Any]\nDictionary mapping parameter names to typed values.\n\n\n\n\n\nExamples\nBasic usage:\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.model_selection.spotoptim_search import (\n...     array_to_params,\n... )\n&gt;&gt;&gt; array_to_params(\n...     np.array([100.0, 0.05]),\n...     var_name=[\"n_estimators\", \"lr\"],\n...     var_type=[\"int\", \"float\"],\n...     bounds=[(50, 200), (0.01, 0.3)],\n... )\n{'n_estimators': 100, 'lr': 0.05}\nGenerating textual output of parameter mapping:\n\nimport numpy as np\nfrom spotforecast2.model_selection.spotoptim_search import array_to_params\n\nparams_array = np.array([0.05, 5.0, 2.0])\nvar_name = [\"alpha\", \"max_depth\", \"model\"]\nvar_type = [\"float\", \"int\", \"factor\"]\nbounds = [(0.01, 10.0), (2, 8), [\"Ridge\", \"Lasso\", \"ElasticNet\"]]\n\nparams_dict = array_to_params(params_array, var_name, var_type, bounds)\n\nfor k, v in params_dict.items():\n    print(f\"{k}: {v} (type: {type(v).__name__})\")\n\nalpha: 0.05 (type: float)\nmax_depth: 5 (type: int)\nmodel: ElasticNet (type: str)\n\n\n\n\n\nconvert_search_space\nmodel_selection.spotoptim_search.convert_search_space(search_space)\nConvert search space into SpotOptim compatible format.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsearch_space\nParameterSet | dict[str, Any]\nSearch space as a SpotOptim ParameterSet or a dictionary.\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[Any]\ntuple containing:\n\n\n\nlist[str]\n- bounds: List of parameter bounds or categories.\n\n\n\nlist[str]\n- var_type: List of variable types (‘float’, ‘int’, or ‘factor’).\n\n\n\nlist[Callable | None]\n- var_name: List of variable names.\n\n\n\ntuple[list[Any], list[str], list[str], list[Callable | None]]\n- var_trans: List of transformation functions (e.g., log10) or None.\n\n\n\n\n\nExamples\nBasic usage:\n&gt;&gt;&gt; from spotoptim.hyperparameters import ParameterSet\n&gt;&gt;&gt; from spotforecast2.model_selection.spotoptim_search import (\n...     convert_search_space,\n... )\n&gt;&gt;&gt; ps = ParameterSet()\n&gt;&gt;&gt; _ = ps.add_float(\"alpha\", 0.01, 10.0)\n&gt;&gt;&gt; b, t, n, tr = convert_search_space(ps)\n&gt;&gt;&gt; b\n[(0.01, 10.0)]\n&gt;&gt;&gt; t\n['float']\nConverting a complex dictionary search space:\n\nfrom spotforecast2.model_selection.spotoptim_search import convert_search_space\n\nsearch_space = {\n    \"learning_rate\": (0.001, 0.1, \"log10\"),\n    \"max_depth\": (2, 10),\n    \"model_type\": [\"RandomForest\", \"XGBoost\"]\n}\n\nbounds, vt, vn, vtr = convert_search_space(search_space)\n\nfor name, typ, bound, trans in zip(vn, vt, bounds, vtr):\n    print(f\"{name} ({typ}): {bound} | transform: {trans}\")\n\nlearning_rate (float): (0.001, 0.1) | transform: log10\nmax_depth (int): (2, 10) | transform: None\nmodel_type (factor): ['RandomForest', 'XGBoost'] | transform: None\n\n\n\n\n\nparse_lags_from_strings\nmodel_selection.spotoptim_search.parse_lags_from_strings(lags_str)\nParse a lags representation back to a Python object.\nHandles three input scenarios: 1. Already an integer or list: returned as is. 2. Single integer as string: \"24\" → 24 3. List representation: \"[1, 2, 3]\" → [1, 2, 3]\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlags_str\nstr | int | list\nLag specification (string, int, or list).\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nint | list\nEither an integer or a list of integers representing lags.\n\n\n\n\n\nExamples\nBasic parsing:\n&gt;&gt;&gt; from spotforecast2.model_selection.spotoptim_search import (\n...     parse_lags_from_strings,\n... )\n&gt;&gt;&gt; parse_lags_from_strings(24)\n24\n&gt;&gt;&gt; parse_lags_from_strings(\"[1, 2, 3]\")\n[1, 2, 3]\nVisualizing the safety threshold (Example of dynamic documentation):\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef check_safety_threshold(val, threshold):\n    return 1 if val &gt;= threshold else 0\n\nthreshold = 0.95\nx = np.linspace(0.8, 1.0, 50)\ny = [check_safety_threshold(val, threshold) for val in x]\n\nplt.step(x, y, where='post')\nplt.axvline(threshold, color='red', linestyle='--')\nplt.title(\"Safety Status Transition\")\n# plt.show()  # Commented for non-interactive environments\n\nText(0.5, 1.0, 'Safety Status Transition')\n\n\n\n\n\n\n\n\n\n\n\n\nspotoptim_objective\nmodel_selection.spotoptim_search.spotoptim_objective(\n    X,\n    forecaster_search,\n    cv_name,\n    cv,\n    metric,\n    y,\n    exog,\n    n_jobs,\n    verbose,\n    show_progress,\n    suppress_warnings,\n    var_name,\n    var_type,\n    bounds,\n    all_metric_values,\n    all_lags,\n    all_params,\n)\nSpotOptim objective function to evaluate hyperparameter sets.\nEvaluates a given array of hyperparameter configurations X and returns an array of the primary metric errors.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\n2D array of hyperparameters from SpotOptim.\nrequired\n\n\nforecaster_search\nobject\nThe forecaster to evaluate.\nrequired\n\n\ncv_name\nstr\nType of cross-validation (“TimeSeriesFold” or “OneStepAheadFold”).\nrequired\n\n\ncv\nTimeSeriesFold | OneStepAheadFold\nCross-validation configuration.\nrequired\n\n\nmetric\nlist[Callable]\nList of metrics to compute.\nrequired\n\n\ny\npd.Series\nTarget time series.\nrequired\n\n\nexog\npd.Series | pd.DataFrame | None\nExogenous variables.\nrequired\n\n\nn_jobs\nint\nNumber of parallel jobs.\nrequired\n\n\nverbose\nbool\nVerbosity level flag.\nrequired\n\n\nshow_progress\nbool\nShow progress bar flag.\nrequired\n\n\nsuppress_warnings\nbool\nSuppress warnings flag.\nrequired\n\n\nvar_name\nlist\nParameter names.\nrequired\n\n\nvar_type\nlist\nParameter types.\nrequired\n\n\nbounds\nlist\nParameter bounds.\nrequired\n\n\nall_metric_values\nlist[list[float]]\nList to record all metric results.\nrequired\n\n\nall_lags\nlist\nList to record all evaluated lag configurations.\nrequired\n\n\nall_params\nlist[dict]\nList to record all evaluated parameters.\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: 1D array of results for the primary metric.\n\n\n\n\n\nExamples\nGenerating textual output of parameter evaluation:\n\nimport numpy as np\nimport pandas as pd\nfrom spotforecast2_safe.model_selection import TimeSeriesFold\nfrom spotforecast2.model_selection.spotoptim_search import spotoptim_objective\n\n# Mock forecaster for documentation\nclass MockForecaster:\n    def set_params(self, **kwargs): pass\n    def set_lags(self, lags): pass\n\n# Provide dummy data and configuration\nX = np.array([[0.05], [0.1]])\ncv = TimeSeriesFold(initial_train_size=10, steps=2)\nmetric = [lambda y_true, y_pred: np.mean(np.abs(y_true - y_pred))]\n\n# Track results\nmetric_vals, lags, params = [], [], []\n\n# When evaluated for real, the mock objects would produce metrics.\n# Here we just show the call structure.\nprint(\"Ready to evaluate hyperparameters.\")\n\nReady to evaluate hyperparameters.\n\n\n\n\n\nspotoptim_search\nmodel_selection.spotoptim_search.spotoptim_search(\n    forecaster,\n    y,\n    cv,\n    search_space,\n    metric,\n    exog=None,\n    n_trials=10,\n    n_initial=5,\n    random_state=123,\n    return_best=True,\n    n_jobs='auto',\n    verbose=False,\n    show_progress=True,\n    suppress_warnings=False,\n    output_file=None,\n    kwargs_spotoptim=None,\n)\nCore implementation of the SpotOptim search logic.\nThis function performs the hyperparameter optimization process using SpotOptim, evaluating configurations via cross-validation or one-step-ahead forecasting.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nforecaster\nobject\nThe initial forecaster object.\nrequired\n\n\ny\npd.Series\nThe target time series.\nrequired\n\n\ncv\nTimeSeriesFold | OneStepAheadFold\nCross-validation or one-step-ahead configuration.\nrequired\n\n\nsearch_space\nParameterSet | Dict[str, Any]\nParameter bounds for SpotOptim.\nrequired\n\n\nmetric\nstr | Callable | list[str | Callable]\nOptimization metric(s).\nrequired\n\n\nexog\npd.Series | pd.DataFrame | None\nExogenous variables.\nNone\n\n\nn_trials\nint\nMaximum number of trials.\n10\n\n\nn_initial\nint\nNumber of initial evaluations.\n5\n\n\nrandom_state\nint\nRandom seed.\n123\n\n\nreturn_best\nbool\nRefit internal forecaster with best params.\nTrue\n\n\nn_jobs\nint | str\nNumber of parallel jobs.\n'auto'\n\n\nverbose\nbool\nVerbosity flag.\nFalse\n\n\nshow_progress\nbool\nShow progress bar flag.\nTrue\n\n\nsuppress_warnings\nbool\nSuppress warnings during evaluation.\nFalse\n\n\noutput_file\nstr | None\nFile to save results to.\nNone\n\n\nkwargs_spotoptim\ndict | None\nAdditional args for SpotOptim.\nNone\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\ntuple\ntuple[pd.DataFrame, object]\n(results_df, optimizer)\n\n\n\n\n\nExamples\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import Ridge\nfrom spotforecast2_safe.forecaster.recursive import ForecasterRecursive\nfrom spotforecast2.model_selection import TimeSeriesFold\nfrom spotforecast2.model_selection.spotoptim_search import spotoptim_search\n\nnp.random.seed(42)\ny = pd.Series(\n    np.random.randn(100).cumsum(),\n    index=pd.date_range(\"2022-01-01\", periods=100, freq=\"h\"),\n)\nforecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\ncv = TimeSeriesFold(steps=5, initial_train_size=80, refit=False)\nsearch_space = {\"alpha\": (0.01, 10.0)}\n\nresults, _ = spotoptim_search(\n    forecaster=forecaster,\n    y=y,\n    cv=cv,\n    search_space=search_space,\n    metric=\"mean_absolute_error\",\n    n_trials=2,\n    n_initial=1,\n    return_best=False,\n    show_progress=False,\n)\n\nprint(f\"Evaluated {len(results)} configurations.\")\n\nEvaluated 2 configurations.\n\n\n\n\n\nspotoptim_search_forecaster\nmodel_selection.spotoptim_search.spotoptim_search_forecaster(\n    forecaster,\n    y,\n    cv,\n    search_space,\n    metric,\n    exog=None,\n    n_trials=10,\n    n_initial=5,\n    random_state=123,\n    return_best=True,\n    n_jobs='auto',\n    verbose=False,\n    show_progress=True,\n    suppress_warnings=False,\n    output_file=None,\n    kwargs_spotoptim=None,\n)\nHyperparameter optimisation for a Forecaster using SpotOptim.\nDrop-in alternative to :func:~spotforecast2.model_selection.bayesian_search_forecaster that uses the SpotOptim surrogate-model-based optimizer instead of Optuna’s TPE sampler.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nforecaster\nobject\nForecaster model (e.g. ForecasterRecursive).\nrequired\n\n\ny\npd.Series\nTraining time series. Must have a datetime or numeric index.\nrequired\n\n\ncv\nTimeSeriesFold | OneStepAheadFold\nCross-validation strategy — TimeSeriesFold or OneStepAheadFold.\nrequired\n\n\nsearch_space\nParameterSet | Dict[str, Any]\nHyperparameter search space. Either a :class:~spotoptim.hyperparameters.ParameterSet or a plain dict (see examples below).\nrequired\n\n\nmetric\nstr | Callable | list[str | Callable]\nMetric name, callable, or list thereof.\nrequired\n\n\nexog\npd.Series | pd.DataFrame | None\nOptional exogenous variable(s).\nNone\n\n\nn_trials\nint\nTotal evaluations (initial + sequential).\n10\n\n\nn_initial\nint\nRandom initial points before surrogate kicks in.\n5\n\n\nrandom_state\nint\nRNG seed.\n123\n\n\nreturn_best\nbool\nRe-fit forecaster with best params after search.\nTrue\n\n\nn_jobs\nint | str\nParallel jobs for backtesting (\"auto\" or int).\n'auto'\n\n\nverbose\nbool\nPrint optimisation progress.\nFalse\n\n\nshow_progress\nbool\nShow progress bar during backtesting/validation.\nTrue\n\n\nsuppress_warnings\nbool\nSuppress spotforecast warnings.\nFalse\n\n\noutput_file\nstr | None\nSave results as TSV to this path.\nNone\n\n\nkwargs_spotoptim\ndict | None\nExtra kwargs passed to SpotOptim().\nNone\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\ntuple\npd.DataFrame\n(results, optimizer) where results is a sorted\n\n\n\nobject\nDataFrame and optimizer is the SpotOptim instance.\n\n\n\n\n\nRaises\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf exog length ≠ y length and return_best is True.\n\n\n\nTypeError\nIf cv is not TimeSeriesFold or OneStepAheadFold.\n\n\n\n\n\nExamples\n1 — Dict-based search space (no ParameterSet needed):\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import Ridge\nfrom spotforecast2_safe.forecaster.recursive import ForecasterRecursive\nfrom spotforecast2.model_selection import (\n    TimeSeriesFold,\n    spotoptim_search_forecaster,\n)\n\nnp.random.seed(42)\ny = pd.Series(\n    np.random.randn(200).cumsum(),\n    index=pd.date_range(\"2022-01-01\", periods=200, freq=\"h\"),\n    name=\"load\",\n)\n\nforecaster = ForecasterRecursive(estimator=Ridge(), lags=5)\ncv = TimeSeriesFold(\n    steps=5,\n    initial_train_size=150,\n    refit=False,\n)\n\nsearch_space = {\"alpha\": (0.01, 10.0)}\n\nresults, optimizer = spotoptim_search_forecaster(\n    forecaster=forecaster,\n    y=y,\n    cv=cv,\n    search_space=search_space,\n    metric=\"mean_absolute_error\",\n    n_trials=5,\n    n_initial=3,\n    random_state=42,\n    return_best=False,\n    verbose=False,\n    show_progress=False,\n)\n\nprint(f\"Is DataFrame: {isinstance(results, pd.DataFrame)}\")\nprint(f\"Contains 'alpha': {'alpha' in results.columns}\")\n\nIs DataFrame: True\nContains 'alpha': True\n\n\n2 — ParameterSet-based search space:\n\nfrom spotoptim.hyperparameters import ParameterSet\n\nps = ParameterSet()\n_ = ps.add_float(\"alpha\", low=0.01, high=10.0)\n\nresults2, _ = spotoptim_search_forecaster(\n    forecaster=ForecasterRecursive(estimator=Ridge(), lags=5),\n    y=y,\n    cv=cv,\n    search_space=ps,\n    metric=\"mean_absolute_error\",\n    n_trials=5,\n    n_initial=3,\n    return_best=False,\n    verbose=False,\n    show_progress=False,\n)\n\nprint(f\"Number of configurations evaluated: {len(results2)}\")\n\nNumber of configurations evaluated: 5",
    "crumbs": [
      "API Reference",
      "Model Selection",
      "spotoptim_search"
    ]
  },
  {
    "objectID": "docs/reference/model_selection.utils_metrics.html",
    "href": "docs/reference/model_selection.utils_metrics.html",
    "title": "model_selection.utils_metrics",
    "section": "",
    "text": "model_selection.utils_metrics\nmodel_selection.utils_metrics\nMetrics calculation utilities for model selection.",
    "crumbs": [
      "API Reference",
      "Model Selection",
      "utils_metrics"
    ]
  },
  {
    "objectID": "docs/reference/preprocessing._common.html",
    "href": "docs/reference/preprocessing._common.html",
    "title": "preprocessing._common",
    "section": "",
    "text": "preprocessing._common\npreprocessing._common\nCommon preprocessing functions and utilities.\nAll functions are imported from spotforecast2_safe.preprocessing._common.",
    "crumbs": [
      "API Reference",
      "Preprocessing",
      "_common"
    ]
  },
  {
    "objectID": "docs/reference/preprocessing._rolling.html",
    "href": "docs/reference/preprocessing._rolling.html",
    "title": "preprocessing._rolling",
    "section": "",
    "text": "preprocessing._rolling\n\n\n\n\n\nName\nDescription\n\n\n\n\nRollingFeatures\nCompute rolling window statistics over time series data.\n\n\n\n\n\npreprocessing._rolling.RollingFeatures(stats, window_sizes, features_names=None)\nCompute rolling window statistics over time series data.\nThis transformer computes rolling statistics (mean, std, min, max, sum, median) over windows of specified sizes from a time series. The class follows the scikit-learn transformer API with fit() and transform() methods, making it compatible with scikit-learn pipelines. It also provides transform_batch() for pandas Series input.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstats\nstr | List[str] | List[Any]\nRolling statistics to compute. Can be a single string (‘mean’, ‘std’, ‘min’, ‘max’, ‘sum’, ‘median’), list of statistic names, or list of callable functions. Multiple statistics can be computed simultaneously.\nrequired\n\n\nwindow_sizes\nint | List[int]\nWindow size(s) for rolling computation. Can be a single integer or list of integers. Multiple windows are applied to all statistics.\nrequired\n\n\nfeatures_names\nList[str] | None\nCustom names for output features. If None, names are auto-generated from statistic names and window sizes (e.g., ‘roll_mean_7’, ‘roll_std_14’). Defaults to None.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nstats\n\nStatistics specification as provided during initialization.\n\n\nwindow_sizes\n\nList of window sizes for rolling computation.\n\n\nfeatures_names\n\nList of output feature names.\n\n\nstats_funcs\n\nList of compiled/numba-optimized statistical functions.\n\n\n\n\n\n\n\nOutput contains NaN values for positions where the rolling window cannot be fully computed (first window_size-1 positions).\nStatistics are computed using numba-optimized JIT functions for performance.\nThe transformer returns numpy arrays from transform() and pandas DataFrames from transform_batch() to maintain index alignment.\nSupports custom user-defined functions in the stats parameter.\n\n\n\n\nCreate a transformer with single statistic and window size:\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import RollingFeatures\n&gt;&gt;&gt; y = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0])\n&gt;&gt;&gt; rf = RollingFeatures(stats='mean', window_sizes=3)\n&gt;&gt;&gt; rf.fit(y)\n&gt;&gt;&gt; features = rf.transform(y)\n&gt;&gt;&gt; features.shape\n(10, 1)\n&gt;&gt;&gt; features[:4]  # First 3 values are NaN\narray([[nan],\n       [nan],\n       [2.],\n       [3.]])\nCreate a transformer with multiple statistics and window sizes:\n&gt;&gt;&gt; rf = RollingFeatures(\n...     stats=['mean', 'std', 'min', 'max'],\n...     window_sizes=[3, 7]\n... )\n&gt;&gt;&gt; rf.fit(y)\n&gt;&gt;&gt; features = rf.transform(y)\n&gt;&gt;&gt; features.shape\n(10, 8)  # 4 stats × 2 window sizes\n&gt;&gt;&gt; rf.features_names\n['roll_mean_3', 'roll_std_3', 'roll_min_3', 'roll_max_3',\n 'roll_mean_7', 'roll_std_7', 'roll_min_7', 'roll_max_7']\nUse with pandas Series to preserve index:\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=10, freq='D')\n&gt;&gt;&gt; y_series = pd.Series(y, index=dates)\n&gt;&gt;&gt; rf = RollingFeatures(stats=['mean', 'max'], window_sizes=5)\n&gt;&gt;&gt; features_df = rf.transform_batch(y_series)\n&gt;&gt;&gt; features_df.shape\n(10, 2)\n&gt;&gt;&gt; features_df.index.equals(y_series.index)\nTrue\nUse with custom feature names:\n&gt;&gt;&gt; rf = RollingFeatures(\n...     stats='mean',\n...     window_sizes=[7, 14, 30],\n...     features_names=['ma_7', 'ma_14', 'ma_30']\n... )\n&gt;&gt;&gt; rf.fit(y)\n&gt;&gt;&gt; rf.features_names\n['ma_7', 'ma_14', 'ma_30']\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfit\nFit the rolling features transformer (no-op).\n\n\ntransform\nCompute rolling window statistics from time series data.\n\n\ntransform_batch\nCompute rolling features from a pandas Series with index preservation.\n\n\n\n\n\npreprocessing._rolling.RollingFeatures.fit(X, y=None)\nFit the rolling features transformer (no-op).\nThis transformer does not learn any parameters from the data. Method exists for scikit-learn compatibility.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nAny\nTime series data (not used for fitting).\nrequired\n\n\ny\nAny\nTarget values (ignored). Defaults to None.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nself\nRollingFeatures\nReturns the fitted transformer.\n\n\n\n\n\n\n\npreprocessing._rolling.RollingFeatures.transform(X)\nCompute rolling window statistics from time series data.\nFor each statistic and window size combination, computes the rolling statistic across the input time series. The output contains NaN values for the initial positions where the window cannot be fully computed.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nTime series data as 1D numpy array or array-like.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Array of shape (len(X), len(features_names)) containing the computed rolling statistics. Each column corresponds to a feature in features_names. Early positions contain NaN values before the window is fully populated.\n\n\n\n\n\n\n\npreprocessing._rolling.RollingFeatures.transform_batch(X)\nCompute rolling features from a pandas Series with index preservation.\nTransforms a pandas Series into a DataFrame of rolling statistics while preserving the original index. Useful for maintaining time alignment with the input data.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\npd.Series\nTime series data as pandas Series. The index is preserved in output.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: DataFrame with shape (len(X), len(features_names)) where columns are feature names and index matches the input Series. Contains NaN values at the beginning where windows are incomplete.\n\n\n\n\n\n\nThis method is preferred over transform() when working with time-indexed data, as it preserves the temporal index and is compatible with forecasting workflows.",
    "crumbs": [
      "API Reference",
      "Preprocessing",
      "_rolling"
    ]
  },
  {
    "objectID": "docs/reference/preprocessing._rolling.html#classes",
    "href": "docs/reference/preprocessing._rolling.html#classes",
    "title": "preprocessing._rolling",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nRollingFeatures\nCompute rolling window statistics over time series data.\n\n\n\n\n\npreprocessing._rolling.RollingFeatures(stats, window_sizes, features_names=None)\nCompute rolling window statistics over time series data.\nThis transformer computes rolling statistics (mean, std, min, max, sum, median) over windows of specified sizes from a time series. The class follows the scikit-learn transformer API with fit() and transform() methods, making it compatible with scikit-learn pipelines. It also provides transform_batch() for pandas Series input.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstats\nstr | List[str] | List[Any]\nRolling statistics to compute. Can be a single string (‘mean’, ‘std’, ‘min’, ‘max’, ‘sum’, ‘median’), list of statistic names, or list of callable functions. Multiple statistics can be computed simultaneously.\nrequired\n\n\nwindow_sizes\nint | List[int]\nWindow size(s) for rolling computation. Can be a single integer or list of integers. Multiple windows are applied to all statistics.\nrequired\n\n\nfeatures_names\nList[str] | None\nCustom names for output features. If None, names are auto-generated from statistic names and window sizes (e.g., ‘roll_mean_7’, ‘roll_std_14’). Defaults to None.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nstats\n\nStatistics specification as provided during initialization.\n\n\nwindow_sizes\n\nList of window sizes for rolling computation.\n\n\nfeatures_names\n\nList of output feature names.\n\n\nstats_funcs\n\nList of compiled/numba-optimized statistical functions.\n\n\n\n\n\n\n\nOutput contains NaN values for positions where the rolling window cannot be fully computed (first window_size-1 positions).\nStatistics are computed using numba-optimized JIT functions for performance.\nThe transformer returns numpy arrays from transform() and pandas DataFrames from transform_batch() to maintain index alignment.\nSupports custom user-defined functions in the stats parameter.\n\n\n\n\nCreate a transformer with single statistic and window size:\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import RollingFeatures\n&gt;&gt;&gt; y = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0])\n&gt;&gt;&gt; rf = RollingFeatures(stats='mean', window_sizes=3)\n&gt;&gt;&gt; rf.fit(y)\n&gt;&gt;&gt; features = rf.transform(y)\n&gt;&gt;&gt; features.shape\n(10, 1)\n&gt;&gt;&gt; features[:4]  # First 3 values are NaN\narray([[nan],\n       [nan],\n       [2.],\n       [3.]])\nCreate a transformer with multiple statistics and window sizes:\n&gt;&gt;&gt; rf = RollingFeatures(\n...     stats=['mean', 'std', 'min', 'max'],\n...     window_sizes=[3, 7]\n... )\n&gt;&gt;&gt; rf.fit(y)\n&gt;&gt;&gt; features = rf.transform(y)\n&gt;&gt;&gt; features.shape\n(10, 8)  # 4 stats × 2 window sizes\n&gt;&gt;&gt; rf.features_names\n['roll_mean_3', 'roll_std_3', 'roll_min_3', 'roll_max_3',\n 'roll_mean_7', 'roll_std_7', 'roll_min_7', 'roll_max_7']\nUse with pandas Series to preserve index:\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=10, freq='D')\n&gt;&gt;&gt; y_series = pd.Series(y, index=dates)\n&gt;&gt;&gt; rf = RollingFeatures(stats=['mean', 'max'], window_sizes=5)\n&gt;&gt;&gt; features_df = rf.transform_batch(y_series)\n&gt;&gt;&gt; features_df.shape\n(10, 2)\n&gt;&gt;&gt; features_df.index.equals(y_series.index)\nTrue\nUse with custom feature names:\n&gt;&gt;&gt; rf = RollingFeatures(\n...     stats='mean',\n...     window_sizes=[7, 14, 30],\n...     features_names=['ma_7', 'ma_14', 'ma_30']\n... )\n&gt;&gt;&gt; rf.fit(y)\n&gt;&gt;&gt; rf.features_names\n['ma_7', 'ma_14', 'ma_30']\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfit\nFit the rolling features transformer (no-op).\n\n\ntransform\nCompute rolling window statistics from time series data.\n\n\ntransform_batch\nCompute rolling features from a pandas Series with index preservation.\n\n\n\n\n\npreprocessing._rolling.RollingFeatures.fit(X, y=None)\nFit the rolling features transformer (no-op).\nThis transformer does not learn any parameters from the data. Method exists for scikit-learn compatibility.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nAny\nTime series data (not used for fitting).\nrequired\n\n\ny\nAny\nTarget values (ignored). Defaults to None.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nself\nRollingFeatures\nReturns the fitted transformer.\n\n\n\n\n\n\n\npreprocessing._rolling.RollingFeatures.transform(X)\nCompute rolling window statistics from time series data.\nFor each statistic and window size combination, computes the rolling statistic across the input time series. The output contains NaN values for the initial positions where the window cannot be fully computed.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nTime series data as 1D numpy array or array-like.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Array of shape (len(X), len(features_names)) containing the computed rolling statistics. Each column corresponds to a feature in features_names. Early positions contain NaN values before the window is fully populated.\n\n\n\n\n\n\n\npreprocessing._rolling.RollingFeatures.transform_batch(X)\nCompute rolling features from a pandas Series with index preservation.\nTransforms a pandas Series into a DataFrame of rolling statistics while preserving the original index. Useful for maintaining time alignment with the input data.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\npd.Series\nTime series data as pandas Series. The index is preserved in output.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: DataFrame with shape (len(X), len(features_names)) where columns are feature names and index matches the input Series. Contains NaN values at the beginning where windows are incomplete.\n\n\n\n\n\n\nThis method is preferred over transform() when working with time-indexed data, as it preserves the temporal index and is compatible with forecasting workflows.",
    "crumbs": [
      "API Reference",
      "Preprocessing",
      "_rolling"
    ]
  },
  {
    "objectID": "docs/reference/preprocessing.imputation.html",
    "href": "docs/reference/preprocessing.imputation.html",
    "title": "preprocessing.imputation",
    "section": "",
    "text": "preprocessing.imputation\n\n\n\n\n\nName\nDescription\n\n\n\n\ncustom_weights\nReturn 0 if index is in or near any gap.\n\n\nget_missing_weights\nReturn imputed DataFrame and a series indicating missing weights.\n\n\n\n\n\npreprocessing.imputation.custom_weights(index, weights_series)\nReturn 0 if index is in or near any gap.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nindex\npd.Index\nThe index to check.\nrequired\n\n\nweights_series\npd.Series\nSeries containing weights.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nfloat\nfloat\nThe weight corresponding to the index.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.imputation import custom_weights\n&gt;&gt;&gt; data = fetch_data(filename=\"data_in.csv\")\n&gt;&gt;&gt; _, missing_weights = get_missing_weights(data, window_size=72, verbose=False)\n&gt;&gt;&gt; for idx in data.index[:5]:\n...     weight = custom_weights(idx, missing_weights)\n...     print(f\"Index: {idx}, Weight: {weight}\")\n\n\n\n\npreprocessing.imputation.get_missing_weights(\n    data,\n    window_size=72,\n    verbose=False,\n)\nReturn imputed DataFrame and a series indicating missing weights.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame\nThe input dataset.\nrequired\n\n\nwindow_size\nint\nThe size of the rolling window to consider for missing values.\n72\n\n\nverbose\nbool\nWhether to print additional information.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple[pd.DataFrame, pd.Series]\nTuple[pd.DataFrame, pd.Series]: A tuple containing the forward and backward filled DataFrame and a boolean series where True indicates missing weights.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.imputation import get_missing_weights\n&gt;&gt;&gt; data = fetch_data(filename=\"data_in.csv\")\n&gt;&gt;&gt; filled_data, missing_weights = get_missing_weights(data, window_size=72, verbose=True)",
    "crumbs": [
      "API Reference",
      "Preprocessing",
      "imputation"
    ]
  },
  {
    "objectID": "docs/reference/preprocessing.imputation.html#functions",
    "href": "docs/reference/preprocessing.imputation.html#functions",
    "title": "preprocessing.imputation",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncustom_weights\nReturn 0 if index is in or near any gap.\n\n\nget_missing_weights\nReturn imputed DataFrame and a series indicating missing weights.\n\n\n\n\n\npreprocessing.imputation.custom_weights(index, weights_series)\nReturn 0 if index is in or near any gap.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nindex\npd.Index\nThe index to check.\nrequired\n\n\nweights_series\npd.Series\nSeries containing weights.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nfloat\nfloat\nThe weight corresponding to the index.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.imputation import custom_weights\n&gt;&gt;&gt; data = fetch_data(filename=\"data_in.csv\")\n&gt;&gt;&gt; _, missing_weights = get_missing_weights(data, window_size=72, verbose=False)\n&gt;&gt;&gt; for idx in data.index[:5]:\n...     weight = custom_weights(idx, missing_weights)\n...     print(f\"Index: {idx}, Weight: {weight}\")\n\n\n\n\npreprocessing.imputation.get_missing_weights(\n    data,\n    window_size=72,\n    verbose=False,\n)\nReturn imputed DataFrame and a series indicating missing weights.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame\nThe input dataset.\nrequired\n\n\nwindow_size\nint\nThe size of the rolling window to consider for missing values.\n72\n\n\nverbose\nbool\nWhether to print additional information.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple[pd.DataFrame, pd.Series]\nTuple[pd.DataFrame, pd.Series]: A tuple containing the forward and backward filled DataFrame and a boolean series where True indicates missing weights.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.imputation import get_missing_weights\n&gt;&gt;&gt; data = fetch_data(filename=\"data_in.csv\")\n&gt;&gt;&gt; filled_data, missing_weights = get_missing_weights(data, window_size=72, verbose=True)",
    "crumbs": [
      "API Reference",
      "Preprocessing",
      "imputation"
    ]
  },
  {
    "objectID": "docs/reference/preprocessing.outlier_plots.html",
    "href": "docs/reference/preprocessing.outlier_plots.html",
    "title": "preprocessing.outlier_plots",
    "section": "",
    "text": "preprocessing.outlier_plots\n\n\n\n\n\nName\nDescription\n\n\n\n\nvisualize_outliers_hist\nVisualize outliers in DataFrame using stacked histograms.\n\n\nvisualize_outliers_plotly_scatter\nVisualize outliers in time series using Plotly scatter plots.\n\n\n\n\n\npreprocessing.outlier_plots.visualize_outliers_hist(\n    data,\n    data_original,\n    columns=None,\n    contamination=0.01,\n    random_state=1234,\n    figsize=(10, 5),\n    bins=50,\n    **kwargs,\n)\nVisualize outliers in DataFrame using stacked histograms.\nCreates a histogram for each specified column, displaying both regular data and detected outliers in different colors. Uses IsolationForest for outlier detection.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame\nThe DataFrame with cleaned data (outliers may be NaN).\nrequired\n\n\ndata_original\npd.DataFrame\nThe original DataFrame before outlier detection.\nrequired\n\n\ncolumns\nOptional[list[str]]\nList of column names to visualize. If None, all columns are used. Default: None.\nNone\n\n\ncontamination\nfloat\nThe estimated proportion of outliers in the dataset. Default: 0.01.\n0.01\n\n\nrandom_state\nint\nRandom seed for reproducibility. Default: 1234.\n1234\n\n\nfigsize\ntuple[int, int]\nFigure size as (width, height). Default: (10, 5).\n(10, 5)\n\n\nbins\nint\nNumber of histogram bins. Default: 50.\n50\n\n\n**kwargs\nAny\nAdditional keyword arguments passed to plt.hist() (e.g., color, alpha, edgecolor, etc.).\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone. Displays matplotlib figures.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf data or data_original is empty, or if specified columns don’t exist.\n\n\n\nImportError\nIf matplotlib is not installed.\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier_plots import visualize_outliers_hist\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; data_original = pd.DataFrame({\n...     'temperature': np.concatenate([\n...         np.random.normal(20, 5, 100),\n...         [50, 60, 70]  # outliers\n...     ]),\n...     'humidity': np.concatenate([\n...         np.random.normal(60, 10, 100),\n...         [95, 98, 99]  # outliers\n...     ])\n... })\n&gt;&gt;&gt; data_cleaned = data_original.copy()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Visualize outliers\n&gt;&gt;&gt; visualize_outliers_hist(\n...     data_cleaned,\n...     data_original,\n...     contamination=0.03,\n...     figsize=(12, 5),\n...     alpha=0.7\n... )\n\n\n\n\npreprocessing.outlier_plots.visualize_outliers_plotly_scatter(\n    data,\n    data_original,\n    columns=None,\n    contamination=0.01,\n    random_state=1234,\n    **kwargs,\n)\nVisualize outliers in time series using Plotly scatter plots.\nCreates an interactive time series plot for each specified column, showing regular data as a line and detected outliers as scatter points. Uses IsolationForest for outlier detection.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame\nThe DataFrame with cleaned data (outliers may be NaN).\nrequired\n\n\ndata_original\npd.DataFrame\nThe original DataFrame before outlier detection.\nrequired\n\n\ncolumns\nOptional[list[str]]\nList of column names to visualize. If None, all columns are used. Default: None.\nNone\n\n\ncontamination\nfloat\nThe estimated proportion of outliers in the dataset. Default: 0.01.\n0.01\n\n\nrandom_state\nint\nRandom seed for reproducibility. Default: 1234.\n1234\n\n\n**kwargs\nAny\nAdditional keyword arguments passed to go.Figure.update_layout() (e.g., template, height, etc.).\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone. Displays Plotly figures.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf data or data_original is empty, or if specified columns don’t exist.\n\n\n\nImportError\nIf plotly is not installed.\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier_plots import visualize_outliers_plotly_scatter\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample time series data\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=103, freq='h')\n&gt;&gt;&gt; data_original = pd.DataFrame({\n...     'temperature': np.concatenate([\n...         np.random.normal(20, 5, 100),\n...         [50, 60, 70]  # outliers\n...     ]),\n...     'humidity': np.concatenate([\n...         np.random.normal(60, 10, 100),\n...         [95, 98, 99]  # outliers\n...     ])\n... }, index=dates)\n&gt;&gt;&gt; data_cleaned = data_original.copy()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Visualize outliers\n&gt;&gt;&gt; visualize_outliers_plotly_scatter(\n...     data_cleaned,\n...     data_original,\n...     contamination=0.03,\n...     template='plotly_white'\n... )",
    "crumbs": [
      "API Reference",
      "Preprocessing",
      "outlier_plots"
    ]
  },
  {
    "objectID": "docs/reference/preprocessing.outlier_plots.html#functions",
    "href": "docs/reference/preprocessing.outlier_plots.html#functions",
    "title": "preprocessing.outlier_plots",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nvisualize_outliers_hist\nVisualize outliers in DataFrame using stacked histograms.\n\n\nvisualize_outliers_plotly_scatter\nVisualize outliers in time series using Plotly scatter plots.\n\n\n\n\n\npreprocessing.outlier_plots.visualize_outliers_hist(\n    data,\n    data_original,\n    columns=None,\n    contamination=0.01,\n    random_state=1234,\n    figsize=(10, 5),\n    bins=50,\n    **kwargs,\n)\nVisualize outliers in DataFrame using stacked histograms.\nCreates a histogram for each specified column, displaying both regular data and detected outliers in different colors. Uses IsolationForest for outlier detection.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame\nThe DataFrame with cleaned data (outliers may be NaN).\nrequired\n\n\ndata_original\npd.DataFrame\nThe original DataFrame before outlier detection.\nrequired\n\n\ncolumns\nOptional[list[str]]\nList of column names to visualize. If None, all columns are used. Default: None.\nNone\n\n\ncontamination\nfloat\nThe estimated proportion of outliers in the dataset. Default: 0.01.\n0.01\n\n\nrandom_state\nint\nRandom seed for reproducibility. Default: 1234.\n1234\n\n\nfigsize\ntuple[int, int]\nFigure size as (width, height). Default: (10, 5).\n(10, 5)\n\n\nbins\nint\nNumber of histogram bins. Default: 50.\n50\n\n\n**kwargs\nAny\nAdditional keyword arguments passed to plt.hist() (e.g., color, alpha, edgecolor, etc.).\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone. Displays matplotlib figures.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf data or data_original is empty, or if specified columns don’t exist.\n\n\n\nImportError\nIf matplotlib is not installed.\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier_plots import visualize_outliers_hist\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; data_original = pd.DataFrame({\n...     'temperature': np.concatenate([\n...         np.random.normal(20, 5, 100),\n...         [50, 60, 70]  # outliers\n...     ]),\n...     'humidity': np.concatenate([\n...         np.random.normal(60, 10, 100),\n...         [95, 98, 99]  # outliers\n...     ])\n... })\n&gt;&gt;&gt; data_cleaned = data_original.copy()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Visualize outliers\n&gt;&gt;&gt; visualize_outliers_hist(\n...     data_cleaned,\n...     data_original,\n...     contamination=0.03,\n...     figsize=(12, 5),\n...     alpha=0.7\n... )\n\n\n\n\npreprocessing.outlier_plots.visualize_outliers_plotly_scatter(\n    data,\n    data_original,\n    columns=None,\n    contamination=0.01,\n    random_state=1234,\n    **kwargs,\n)\nVisualize outliers in time series using Plotly scatter plots.\nCreates an interactive time series plot for each specified column, showing regular data as a line and detected outliers as scatter points. Uses IsolationForest for outlier detection.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame\nThe DataFrame with cleaned data (outliers may be NaN).\nrequired\n\n\ndata_original\npd.DataFrame\nThe original DataFrame before outlier detection.\nrequired\n\n\ncolumns\nOptional[list[str]]\nList of column names to visualize. If None, all columns are used. Default: None.\nNone\n\n\ncontamination\nfloat\nThe estimated proportion of outliers in the dataset. Default: 0.01.\n0.01\n\n\nrandom_state\nint\nRandom seed for reproducibility. Default: 1234.\n1234\n\n\n**kwargs\nAny\nAdditional keyword arguments passed to go.Figure.update_layout() (e.g., template, height, etc.).\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone. Displays Plotly figures.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf data or data_original is empty, or if specified columns don’t exist.\n\n\n\nImportError\nIf plotly is not installed.\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier_plots import visualize_outliers_plotly_scatter\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample time series data\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=103, freq='h')\n&gt;&gt;&gt; data_original = pd.DataFrame({\n...     'temperature': np.concatenate([\n...         np.random.normal(20, 5, 100),\n...         [50, 60, 70]  # outliers\n...     ]),\n...     'humidity': np.concatenate([\n...         np.random.normal(60, 10, 100),\n...         [95, 98, 99]  # outliers\n...     ])\n... }, index=dates)\n&gt;&gt;&gt; data_cleaned = data_original.copy()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Visualize outliers\n&gt;&gt;&gt; visualize_outliers_plotly_scatter(\n...     data_cleaned,\n...     data_original,\n...     contamination=0.03,\n...     template='plotly_white'\n... )",
    "crumbs": [
      "API Reference",
      "Preprocessing",
      "outlier_plots"
    ]
  },
  {
    "objectID": "docs/reference/preprocessing.time_series_visualization.html",
    "href": "docs/reference/preprocessing.time_series_visualization.html",
    "title": "preprocessing.time_series_visualization",
    "section": "",
    "text": "preprocessing.time_series_visualization\nTime series visualization.\n\n\n\n\n\nName\nDescription\n\n\n\n\nplot_forecast\nPlot model forecast against actuals and display CV metrics.\n\n\nplot_predictions\nPlot actual values against one or more prediction series.\n\n\nplot_seasonality\nPlot seasonal patterns (annual, weekly, daily) for a given target.\n\n\nplot_zoomed_timeseries\nPlot a time series with a zoomed-in focus area.\n\n\nvisualize_ts_comparison\nVisualize time series with optional statistical overlays.\n\n\nvisualize_ts_plotly\nVisualize multiple time series datasets interactively with Plotly.\n\n\n\n\n\npreprocessing.time_series_visualization.plot_forecast(\n    model,\n    X,\n    y,\n    cv_results=None,\n    title='Forecast',\n    figsize=None,\n    show=True,\n    nrows=None,\n    ncols=1,\n    sharex=True,\n)\nPlot model forecast against actuals and display CV metrics.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmodel\nAny\nFitted scikit-learn model.\nrequired\n\n\nX\npd.DataFrame\nFeature matrix (e.g., test set).\nrequired\n\n\ny\nUnion[pd.Series, pd.DataFrame]\nTarget series or DataFrame (e.g., test set).\nrequired\n\n\ncv_results\nOptional[Dict[str, Any]]\nOptional dictionary of cross-validation results from evaluate() or sklearn.model_selection.cross_validate().\nNone\n\n\ntitle\nstr\nTitle of the plot. Defaults to “Forecast”.\n'Forecast'\n\n\nfigsize\nOptional[tuple]\nFigure dimensions.\nNone\n\n\nshow\nbool\nWhether to display the plot. Defaults to True.\nTrue\n\n\nnrows\nOptional[int]\nNumber of rows for subplots (multivariate).\nNone\n\n\nncols\nint\nNumber of columns for subplots (multivariate).\n1\n\n\nsharex\nbool\nWhether to share x-axis for subplots. Defaults to True.\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nplt.Figure\nplt.Figure: The matplotlib Figure object.\n\n\n\n\n\n\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import plot_forecast\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=10, freq=\"D\")\n&gt;&gt;&gt; X = pd.DataFrame({\"feat\": np.arange(10)}, index=dates)\n&gt;&gt;&gt; y = pd.Series(np.arange(10), index=dates)\n&gt;&gt;&gt; model = LinearRegression().fit(X, y)\n&gt;&gt;&gt; # Plot forecast\n&gt;&gt;&gt; fig = plot_forecast(model, X, y, show=False)\n&gt;&gt;&gt; plt.close(fig)\n\n\n\n\npreprocessing.time_series_visualization.plot_predictions(\n    y_true,\n    predictions,\n    slice_seq=None,\n    title='Predictions vs Actuals',\n    figsize=None,\n    show=True,\n    nrows=None,\n    ncols=1,\n    sharex=True,\n)\nPlot actual values against one or more prediction series.\nAllows visualizing model performance by overlaying predictions on top of actual data. Supports slicing to focus on a specific time range (e.g., the recent test set). Handles both univariate and multivariate targets by creating subplots for multiple targets.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ny_true\nUnion[pd.Series, pd.DataFrame]\nSeries or DataFrame containing the actual target values.\nrequired\n\n\npredictions\nDict[str, Union[pd.Series, pd.DataFrame, np.ndarray]]\nDictionary where keys are labels (e.g., model names) and values are the corresponding predictions. If arrays are provided, they must have the same length as the sliced y_true.\nrequired\n\n\nslice_seq\nOptional[slice]\nOptional slice object to select a subset of the data. If None, the entire series is plotted. Example: slice(-96, None) to select the last 96 points.\nNone\n\n\ntitle\nstr\nTitle of the plot. Defaults to “Predictions vs Actuals”.\n'Predictions vs Actuals'\n\n\nfigsize\nOptional[tuple]\nTuple defining figure width and height. If None, automatically calculated based on number of subplots.\nNone\n\n\nshow\nbool\nWhether to display the plot. Defaults to True.\nTrue\n\n\nnrows\nOptional[int]\nNumber of rows for subplots (multivariate). Defaults to n_targets.\nNone\n\n\nncols\nint\nNumber of columns for subplots (multivariate). Defaults to 1.\n1\n\n\nsharex\nbool\nWhether to share x-axis for subplots. Defaults to True.\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nplt.Figure\nplt.Figure: The matplotlib Figure object containing the plot.\n\n\n\n\n\n\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import plot_predictions\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=10, freq=\"D\")\n&gt;&gt;&gt; y_true = pd.Series(np.arange(10), index=dates, name=\"Target\")\n&gt;&gt;&gt; predictions = {\"Model A\": y_true + 0.5}\n&gt;&gt;&gt; # Plot predictions\n&gt;&gt;&gt; fig = plot_predictions(y_true, predictions, show=False)\n&gt;&gt;&gt; plt.close(fig)\n\n\n\n\npreprocessing.time_series_visualization.plot_seasonality(\n    data,\n    target,\n    figsize=(8, 5),\n    show=True,\n    logscale=False,\n)\nPlot seasonal patterns (annual, weekly, daily) for a given target.\nCreates a 2x2 grid of plots: 1. Distribution by month (boxplot + median). 2. Distribution by week day (boxplot + median). 3. Distribution by hour of day (boxplot + median). 4. Mean target value by day of week and hour.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame\nDataFrame containing the time series data. Must have a DatetimeIndex or an index convertible to datetime.\nrequired\n\n\ntarget\nstr\nName of the column to plot.\nrequired\n\n\nfigsize\ntuple[int, int]\nFigure dimensions (width, height). Defaults to (8, 5).\n(8, 5)\n\n\nshow\nbool\nWhether to display the plot immediately. Defaults to True.\nTrue\n\n\nlogscale\nUnion[bool, list[bool]]\nWhether to use a log scale for the y-axis. Can be a single boolean (applies to all 4 plots) or a list of 4 booleans (applies to each plot individually). Defaults to False.\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nplt.Figure\nplt.Figure: The matplotlib Figure object.\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import plot_seasonality\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=1000, freq=\"h\")\n&gt;&gt;&gt; df = pd.DataFrame({\"value\": range(1, 1001)}, index=dates)\n&gt;&gt;&gt; # Plot seasonality with log scale for all plots\n&gt;&gt;&gt; fig = plot_seasonality(data=df, target=\"value\", logscale=True, show=False)\n&gt;&gt;&gt; plt.close(fig)\n&gt;&gt;&gt; # Plot seasonality with log scale for the first plot only\n&gt;&gt;&gt; fig = plot_seasonality(\n...     data=df,\n...     target=\"value\",\n...     logscale=[True, False, False, False],\n...     show=False\n... )\n&gt;&gt;&gt; plt.close(fig)\n\n\n\n\npreprocessing.time_series_visualization.plot_zoomed_timeseries(\n    data,\n    target,\n    zoom,\n    title=None,\n    figsize=(8, 4),\n    show=True,\n)\nPlot a time series with a zoomed-in focus area.\nCreates a two-panel plot: 1. Top panel: Full time series with the zoom area highlighted. 2. Bottom panel: Zoomed-in view of the specified time range.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame\nDataFrame containing the time series data. Must have a DatetimeIndex or an index convertible to datetime.\nrequired\n\n\ntarget\nstr\nName of the column to plot.\nrequired\n\n\nzoom\ntuple[str, str]\nTuple of (start_date, end_date) strings defining the zoom range.\nrequired\n\n\ntitle\nOptional[str]\nOptional title for the plot. If None, defaults to target name.\nNone\n\n\nfigsize\ntuple[int, int]\nFigure dimensions (width, height). Defaults to (8, 4).\n(8, 4)\n\n\nshow\nbool\nWhether to display the plot immediately. Defaults to True.\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nplt.Figure\nplt.Figure: The matplotlib Figure object.\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import plot_zoomed_timeseries\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=100, freq=\"h\")\n&gt;&gt;&gt; df = pd.DataFrame({\"value\": range(100)}, index=dates)\n&gt;&gt;&gt; # Plot with zoom\n&gt;&gt;&gt; fig = plot_zoomed_timeseries(\n...     data=df,\n...     target=\"value\",\n...     zoom=(\"2023-01-02 00:00\", \"2023-01-03 00:00\"),\n...     show=False\n... )\n&gt;&gt;&gt; plt.close(fig)\n\n\n\n\npreprocessing.time_series_visualization.visualize_ts_comparison(\n    dataframes,\n    columns=None,\n    title_suffix='',\n    figsize=(1000, 500),\n    template='plotly_white',\n    colors=None,\n    show_mean=False,\n    **kwargs,\n)\nVisualize time series with optional statistical overlays.\nSimilar to visualize_ts_plotly but adds options for statistical overlays like mean values across all datasets.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataframes\nDict[str, pd.DataFrame]\nDictionary mapping dataset names to pandas DataFrames.\nrequired\n\n\ncolumns\nOptional[List[str]]\nList of column names to visualize. If None, all columns are used. Default: None.\nNone\n\n\ntitle_suffix\nstr\nSuffix to append to column names. Default: ““.\n''\n\n\nfigsize\ntuple[int, int]\nFigure size as (width, height) in pixels. Default: (1000, 500).\n(1000, 500)\n\n\ntemplate\nstr\nPlotly template. Default: ‘plotly_white’.\n'plotly_white'\n\n\ncolors\nOptional[Dict[str, str]]\nDictionary mapping dataset names to colors. Default: None.\nNone\n\n\nshow_mean\nbool\nIf True, overlay the mean of all datasets. Default: False.\nFalse\n\n\n**kwargs\nAny\nAdditional keyword arguments for go.Scatter().\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone. Displays Plotly figures.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf dataframes is empty.\n\n\n\nImportError\nIf plotly is not installed.\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import visualize_ts_comparison\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; dates1 = pd.date_range('2024-01-01', periods=100, freq='h')\n&gt;&gt;&gt; dates2 = pd.date_range('2024-05-11', periods=100, freq='h')\n&gt;&gt;&gt;\n&gt;&gt;&gt; df1 = pd.DataFrame({\n...     'temperature': np.random.normal(20, 5, 100)\n... }, index=dates1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; df2 = pd.DataFrame({\n...     'temperature': np.random.normal(22, 5, 100)\n... }, index=dates2)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Compare with mean overlay\n&gt;&gt;&gt; visualize_ts_comparison(\n...     {'Dataset1': df1, 'Dataset2': df2},\n...     show_mean=True\n... )\n\n\n\n\npreprocessing.time_series_visualization.visualize_ts_plotly(\n    dataframes,\n    columns=None,\n    title_suffix='',\n    figsize=(1000, 500),\n    template='plotly_white',\n    colors=None,\n    **kwargs,\n)\nVisualize multiple time series datasets interactively with Plotly.\nCreates interactive Plotly scatter plots for specified columns across multiple datasets (e.g., train, validation, test splits). Each dataset is displayed as a separate line with a unique color and name in the legend.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataframes\nDict[str, pd.DataFrame]\nDictionary mapping dataset names to pandas DataFrames with datetime index. Example: {‘Train’: df_train, ‘Validation’: df_val, ‘Test’: df_test}\nrequired\n\n\ncolumns\nOptional[List[str]]\nList of column names to visualize. If None, all columns are used. Default: None.\nNone\n\n\ntitle_suffix\nstr\nSuffix to append to the column name in the title. Useful for adding units or descriptions. Default: ““.\n''\n\n\nfigsize\ntuple[int, int]\nFigure size as (width, height) in pixels. Default: (1000, 500).\n(1000, 500)\n\n\ntemplate\nstr\nPlotly template name for styling. Options include ‘plotly_white’, ‘plotly_dark’, ‘plotly’, ‘ggplot2’, etc. Default: ‘plotly_white’.\n'plotly_white'\n\n\ncolors\nOptional[Dict[str, str]]\nDictionary mapping dataset names to colors. If None, uses Plotly default colors. Example: {‘Train’: ‘blue’, ‘Validation’: ‘orange’}. Default: None.\nNone\n\n\n**kwargs\nAny\nAdditional keyword arguments passed to go.Scatter() (e.g., mode=‘lines+markers’, line=dict(dash=‘dash’)).\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone. Displays Plotly figures.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf dataframes dict is empty, contains no columns, or if specified columns don’t exist in all dataframes.\n\n\n\nImportError\nIf plotly is not installed.\n\n\n\nTypeError\nIf dataframes parameter is not a dictionary.\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import visualize_ts_plotly\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample time series data\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; dates_train = pd.date_range('2024-01-01', periods=100, freq='h')\n&gt;&gt;&gt; dates_val = pd.date_range('2024-05-11', periods=50, freq='h')\n&gt;&gt;&gt; dates_test = pd.date_range('2024-07-01', periods=30, freq='h')\n&gt;&gt;&gt;\n&gt;&gt;&gt; data_train = pd.DataFrame({\n...     'temperature': np.random.normal(20, 5, 100),\n...     'humidity': np.random.normal(60, 10, 100)\n... }, index=dates_train)\n&gt;&gt;&gt;\n&gt;&gt;&gt; data_val = pd.DataFrame({\n...     'temperature': np.random.normal(22, 5, 50),\n...     'humidity': np.random.normal(55, 10, 50)\n... }, index=dates_val)\n&gt;&gt;&gt;\n&gt;&gt;&gt; data_test = pd.DataFrame({\n...     'temperature': np.random.normal(25, 5, 30),\n...     'humidity': np.random.normal(50, 10, 30)\n... }, index=dates_test)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Visualize all datasets\n&gt;&gt;&gt; dataframes = {\n...     'Train': data_train,\n...     'Validation': data_val,\n...     'Test': data_test\n... }\n&gt;&gt;&gt; visualize_ts_plotly(dataframes)\nSingle dataset example:\n&gt;&gt;&gt; # Visualize single dataset\n&gt;&gt;&gt; dataframes = {'Data': data_train}\n&gt;&gt;&gt; visualize_ts_plotly(dataframes, columns=['temperature'])\nCustom styling:\n&gt;&gt;&gt; visualize_ts_plotly(\n...     dataframes,\n...     columns=['temperature'],\n...     template='plotly_dark',\n...     colors={'Train': 'blue', 'Validation': 'green', 'Test': 'red'},\n...     mode='lines+markers'\n... )",
    "crumbs": [
      "API Reference",
      "Preprocessing",
      "time_series_visualization"
    ]
  },
  {
    "objectID": "docs/reference/preprocessing.time_series_visualization.html#functions",
    "href": "docs/reference/preprocessing.time_series_visualization.html#functions",
    "title": "preprocessing.time_series_visualization",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nplot_forecast\nPlot model forecast against actuals and display CV metrics.\n\n\nplot_predictions\nPlot actual values against one or more prediction series.\n\n\nplot_seasonality\nPlot seasonal patterns (annual, weekly, daily) for a given target.\n\n\nplot_zoomed_timeseries\nPlot a time series with a zoomed-in focus area.\n\n\nvisualize_ts_comparison\nVisualize time series with optional statistical overlays.\n\n\nvisualize_ts_plotly\nVisualize multiple time series datasets interactively with Plotly.\n\n\n\n\n\npreprocessing.time_series_visualization.plot_forecast(\n    model,\n    X,\n    y,\n    cv_results=None,\n    title='Forecast',\n    figsize=None,\n    show=True,\n    nrows=None,\n    ncols=1,\n    sharex=True,\n)\nPlot model forecast against actuals and display CV metrics.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmodel\nAny\nFitted scikit-learn model.\nrequired\n\n\nX\npd.DataFrame\nFeature matrix (e.g., test set).\nrequired\n\n\ny\nUnion[pd.Series, pd.DataFrame]\nTarget series or DataFrame (e.g., test set).\nrequired\n\n\ncv_results\nOptional[Dict[str, Any]]\nOptional dictionary of cross-validation results from evaluate() or sklearn.model_selection.cross_validate().\nNone\n\n\ntitle\nstr\nTitle of the plot. Defaults to “Forecast”.\n'Forecast'\n\n\nfigsize\nOptional[tuple]\nFigure dimensions.\nNone\n\n\nshow\nbool\nWhether to display the plot. Defaults to True.\nTrue\n\n\nnrows\nOptional[int]\nNumber of rows for subplots (multivariate).\nNone\n\n\nncols\nint\nNumber of columns for subplots (multivariate).\n1\n\n\nsharex\nbool\nWhether to share x-axis for subplots. Defaults to True.\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nplt.Figure\nplt.Figure: The matplotlib Figure object.\n\n\n\n\n\n\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import plot_forecast\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=10, freq=\"D\")\n&gt;&gt;&gt; X = pd.DataFrame({\"feat\": np.arange(10)}, index=dates)\n&gt;&gt;&gt; y = pd.Series(np.arange(10), index=dates)\n&gt;&gt;&gt; model = LinearRegression().fit(X, y)\n&gt;&gt;&gt; # Plot forecast\n&gt;&gt;&gt; fig = plot_forecast(model, X, y, show=False)\n&gt;&gt;&gt; plt.close(fig)\n\n\n\n\npreprocessing.time_series_visualization.plot_predictions(\n    y_true,\n    predictions,\n    slice_seq=None,\n    title='Predictions vs Actuals',\n    figsize=None,\n    show=True,\n    nrows=None,\n    ncols=1,\n    sharex=True,\n)\nPlot actual values against one or more prediction series.\nAllows visualizing model performance by overlaying predictions on top of actual data. Supports slicing to focus on a specific time range (e.g., the recent test set). Handles both univariate and multivariate targets by creating subplots for multiple targets.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ny_true\nUnion[pd.Series, pd.DataFrame]\nSeries or DataFrame containing the actual target values.\nrequired\n\n\npredictions\nDict[str, Union[pd.Series, pd.DataFrame, np.ndarray]]\nDictionary where keys are labels (e.g., model names) and values are the corresponding predictions. If arrays are provided, they must have the same length as the sliced y_true.\nrequired\n\n\nslice_seq\nOptional[slice]\nOptional slice object to select a subset of the data. If None, the entire series is plotted. Example: slice(-96, None) to select the last 96 points.\nNone\n\n\ntitle\nstr\nTitle of the plot. Defaults to “Predictions vs Actuals”.\n'Predictions vs Actuals'\n\n\nfigsize\nOptional[tuple]\nTuple defining figure width and height. If None, automatically calculated based on number of subplots.\nNone\n\n\nshow\nbool\nWhether to display the plot. Defaults to True.\nTrue\n\n\nnrows\nOptional[int]\nNumber of rows for subplots (multivariate). Defaults to n_targets.\nNone\n\n\nncols\nint\nNumber of columns for subplots (multivariate). Defaults to 1.\n1\n\n\nsharex\nbool\nWhether to share x-axis for subplots. Defaults to True.\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nplt.Figure\nplt.Figure: The matplotlib Figure object containing the plot.\n\n\n\n\n\n\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import plot_predictions\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=10, freq=\"D\")\n&gt;&gt;&gt; y_true = pd.Series(np.arange(10), index=dates, name=\"Target\")\n&gt;&gt;&gt; predictions = {\"Model A\": y_true + 0.5}\n&gt;&gt;&gt; # Plot predictions\n&gt;&gt;&gt; fig = plot_predictions(y_true, predictions, show=False)\n&gt;&gt;&gt; plt.close(fig)\n\n\n\n\npreprocessing.time_series_visualization.plot_seasonality(\n    data,\n    target,\n    figsize=(8, 5),\n    show=True,\n    logscale=False,\n)\nPlot seasonal patterns (annual, weekly, daily) for a given target.\nCreates a 2x2 grid of plots: 1. Distribution by month (boxplot + median). 2. Distribution by week day (boxplot + median). 3. Distribution by hour of day (boxplot + median). 4. Mean target value by day of week and hour.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame\nDataFrame containing the time series data. Must have a DatetimeIndex or an index convertible to datetime.\nrequired\n\n\ntarget\nstr\nName of the column to plot.\nrequired\n\n\nfigsize\ntuple[int, int]\nFigure dimensions (width, height). Defaults to (8, 5).\n(8, 5)\n\n\nshow\nbool\nWhether to display the plot immediately. Defaults to True.\nTrue\n\n\nlogscale\nUnion[bool, list[bool]]\nWhether to use a log scale for the y-axis. Can be a single boolean (applies to all 4 plots) or a list of 4 booleans (applies to each plot individually). Defaults to False.\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nplt.Figure\nplt.Figure: The matplotlib Figure object.\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import plot_seasonality\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=1000, freq=\"h\")\n&gt;&gt;&gt; df = pd.DataFrame({\"value\": range(1, 1001)}, index=dates)\n&gt;&gt;&gt; # Plot seasonality with log scale for all plots\n&gt;&gt;&gt; fig = plot_seasonality(data=df, target=\"value\", logscale=True, show=False)\n&gt;&gt;&gt; plt.close(fig)\n&gt;&gt;&gt; # Plot seasonality with log scale for the first plot only\n&gt;&gt;&gt; fig = plot_seasonality(\n...     data=df,\n...     target=\"value\",\n...     logscale=[True, False, False, False],\n...     show=False\n... )\n&gt;&gt;&gt; plt.close(fig)\n\n\n\n\npreprocessing.time_series_visualization.plot_zoomed_timeseries(\n    data,\n    target,\n    zoom,\n    title=None,\n    figsize=(8, 4),\n    show=True,\n)\nPlot a time series with a zoomed-in focus area.\nCreates a two-panel plot: 1. Top panel: Full time series with the zoom area highlighted. 2. Bottom panel: Zoomed-in view of the specified time range.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame\nDataFrame containing the time series data. Must have a DatetimeIndex or an index convertible to datetime.\nrequired\n\n\ntarget\nstr\nName of the column to plot.\nrequired\n\n\nzoom\ntuple[str, str]\nTuple of (start_date, end_date) strings defining the zoom range.\nrequired\n\n\ntitle\nOptional[str]\nOptional title for the plot. If None, defaults to target name.\nNone\n\n\nfigsize\ntuple[int, int]\nFigure dimensions (width, height). Defaults to (8, 4).\n(8, 4)\n\n\nshow\nbool\nWhether to display the plot immediately. Defaults to True.\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nplt.Figure\nplt.Figure: The matplotlib Figure object.\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import plot_zoomed_timeseries\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=100, freq=\"h\")\n&gt;&gt;&gt; df = pd.DataFrame({\"value\": range(100)}, index=dates)\n&gt;&gt;&gt; # Plot with zoom\n&gt;&gt;&gt; fig = plot_zoomed_timeseries(\n...     data=df,\n...     target=\"value\",\n...     zoom=(\"2023-01-02 00:00\", \"2023-01-03 00:00\"),\n...     show=False\n... )\n&gt;&gt;&gt; plt.close(fig)\n\n\n\n\npreprocessing.time_series_visualization.visualize_ts_comparison(\n    dataframes,\n    columns=None,\n    title_suffix='',\n    figsize=(1000, 500),\n    template='plotly_white',\n    colors=None,\n    show_mean=False,\n    **kwargs,\n)\nVisualize time series with optional statistical overlays.\nSimilar to visualize_ts_plotly but adds options for statistical overlays like mean values across all datasets.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataframes\nDict[str, pd.DataFrame]\nDictionary mapping dataset names to pandas DataFrames.\nrequired\n\n\ncolumns\nOptional[List[str]]\nList of column names to visualize. If None, all columns are used. Default: None.\nNone\n\n\ntitle_suffix\nstr\nSuffix to append to column names. Default: ““.\n''\n\n\nfigsize\ntuple[int, int]\nFigure size as (width, height) in pixels. Default: (1000, 500).\n(1000, 500)\n\n\ntemplate\nstr\nPlotly template. Default: ‘plotly_white’.\n'plotly_white'\n\n\ncolors\nOptional[Dict[str, str]]\nDictionary mapping dataset names to colors. Default: None.\nNone\n\n\nshow_mean\nbool\nIf True, overlay the mean of all datasets. Default: False.\nFalse\n\n\n**kwargs\nAny\nAdditional keyword arguments for go.Scatter().\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone. Displays Plotly figures.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf dataframes is empty.\n\n\n\nImportError\nIf plotly is not installed.\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import visualize_ts_comparison\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; dates1 = pd.date_range('2024-01-01', periods=100, freq='h')\n&gt;&gt;&gt; dates2 = pd.date_range('2024-05-11', periods=100, freq='h')\n&gt;&gt;&gt;\n&gt;&gt;&gt; df1 = pd.DataFrame({\n...     'temperature': np.random.normal(20, 5, 100)\n... }, index=dates1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; df2 = pd.DataFrame({\n...     'temperature': np.random.normal(22, 5, 100)\n... }, index=dates2)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Compare with mean overlay\n&gt;&gt;&gt; visualize_ts_comparison(\n...     {'Dataset1': df1, 'Dataset2': df2},\n...     show_mean=True\n... )\n\n\n\n\npreprocessing.time_series_visualization.visualize_ts_plotly(\n    dataframes,\n    columns=None,\n    title_suffix='',\n    figsize=(1000, 500),\n    template='plotly_white',\n    colors=None,\n    **kwargs,\n)\nVisualize multiple time series datasets interactively with Plotly.\nCreates interactive Plotly scatter plots for specified columns across multiple datasets (e.g., train, validation, test splits). Each dataset is displayed as a separate line with a unique color and name in the legend.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataframes\nDict[str, pd.DataFrame]\nDictionary mapping dataset names to pandas DataFrames with datetime index. Example: {‘Train’: df_train, ‘Validation’: df_val, ‘Test’: df_test}\nrequired\n\n\ncolumns\nOptional[List[str]]\nList of column names to visualize. If None, all columns are used. Default: None.\nNone\n\n\ntitle_suffix\nstr\nSuffix to append to the column name in the title. Useful for adding units or descriptions. Default: ““.\n''\n\n\nfigsize\ntuple[int, int]\nFigure size as (width, height) in pixels. Default: (1000, 500).\n(1000, 500)\n\n\ntemplate\nstr\nPlotly template name for styling. Options include ‘plotly_white’, ‘plotly_dark’, ‘plotly’, ‘ggplot2’, etc. Default: ‘plotly_white’.\n'plotly_white'\n\n\ncolors\nOptional[Dict[str, str]]\nDictionary mapping dataset names to colors. If None, uses Plotly default colors. Example: {‘Train’: ‘blue’, ‘Validation’: ‘orange’}. Default: None.\nNone\n\n\n**kwargs\nAny\nAdditional keyword arguments passed to go.Scatter() (e.g., mode=‘lines+markers’, line=dict(dash=‘dash’)).\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone. Displays Plotly figures.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf dataframes dict is empty, contains no columns, or if specified columns don’t exist in all dataframes.\n\n\n\nImportError\nIf plotly is not installed.\n\n\n\nTypeError\nIf dataframes parameter is not a dictionary.\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import visualize_ts_plotly\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample time series data\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; dates_train = pd.date_range('2024-01-01', periods=100, freq='h')\n&gt;&gt;&gt; dates_val = pd.date_range('2024-05-11', periods=50, freq='h')\n&gt;&gt;&gt; dates_test = pd.date_range('2024-07-01', periods=30, freq='h')\n&gt;&gt;&gt;\n&gt;&gt;&gt; data_train = pd.DataFrame({\n...     'temperature': np.random.normal(20, 5, 100),\n...     'humidity': np.random.normal(60, 10, 100)\n... }, index=dates_train)\n&gt;&gt;&gt;\n&gt;&gt;&gt; data_val = pd.DataFrame({\n...     'temperature': np.random.normal(22, 5, 50),\n...     'humidity': np.random.normal(55, 10, 50)\n... }, index=dates_val)\n&gt;&gt;&gt;\n&gt;&gt;&gt; data_test = pd.DataFrame({\n...     'temperature': np.random.normal(25, 5, 30),\n...     'humidity': np.random.normal(50, 10, 30)\n... }, index=dates_test)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Visualize all datasets\n&gt;&gt;&gt; dataframes = {\n...     'Train': data_train,\n...     'Validation': data_val,\n...     'Test': data_test\n... }\n&gt;&gt;&gt; visualize_ts_plotly(dataframes)\nSingle dataset example:\n&gt;&gt;&gt; # Visualize single dataset\n&gt;&gt;&gt; dataframes = {'Data': data_train}\n&gt;&gt;&gt; visualize_ts_plotly(dataframes, columns=['temperature'])\nCustom styling:\n&gt;&gt;&gt; visualize_ts_plotly(\n...     dataframes,\n...     columns=['temperature'],\n...     template='plotly_dark',\n...     colors={'Train': 'blue', 'Validation': 'green', 'Test': 'red'},\n...     mode='lines+markers'\n... )",
    "crumbs": [
      "API Reference",
      "Preprocessing",
      "time_series_visualization"
    ]
  },
  {
    "objectID": "docs/reference/tasks.task_demo.html",
    "href": "docs/reference/tasks.task_demo.html",
    "title": "tasks.task_demo",
    "section": "",
    "text": "tasks.task_demo\nTask demo: compare baseline, covariate, and custom LightGBM forecasts against ground truth.\nThis script executes the baseline N-to-1 task, the covariate-enhanced N-to-1 pipeline, and a custom LightGBM model with optimized hyperparameters, then loads the ground truth from ~/spotforecast2_data/data_test.csv using the safety-critical load_actual_combined function from spotforecast2_safe, and plots Actual vs Predicted using Plotly.\n\n\n\nActual combined values (ground truth)\nBaseline combined prediction (n2n_predict)\nCovariate combined prediction (n2n_predict_with_covariates, default LGBM)\nCustom LightGBM combined prediction (optimized hyperparameters, Europe/Berlin tz)\n\n\n\n\n\nUses load_actual_combined from spotforecast2_safe for validated data loading\nDemoConfig provides immutable configuration with sensible defaults\nPath objects ensure cross-platform compatibility\nComprehensive error handling with file existence checks\n\n\n\n\nRun the demo:\n&gt;&gt;&gt; python tasks/task_demo.py\nForce training (case-insensitive boolean):\n&gt;&gt;&gt; python tasks/task_demo.py --force_train false\nSave the plot as a single HTML file (default: task_demo_plot.html):\n&gt;&gt;&gt; python tasks/task_demo.py --html\nSave to a specific path:\n&gt;&gt;&gt; python tasks/task_demo.py --html results/plot.html\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nmain\nRun the demo, compute predictions for three models, and plot actual vs predicted.\n\n\n\n\n\ntasks.task_demo.main(force_train=True, html_path=None)\nRun the demo, compute predictions for three models, and plot actual vs predicted.",
    "crumbs": [
      "API Reference",
      "Tasks",
      "task_demo"
    ]
  },
  {
    "objectID": "docs/reference/tasks.task_demo.html#the-plot-includes",
    "href": "docs/reference/tasks.task_demo.html#the-plot-includes",
    "title": "tasks.task_demo",
    "section": "",
    "text": "Actual combined values (ground truth)\nBaseline combined prediction (n2n_predict)\nCovariate combined prediction (n2n_predict_with_covariates, default LGBM)\nCustom LightGBM combined prediction (optimized hyperparameters, Europe/Berlin tz)",
    "crumbs": [
      "API Reference",
      "Tasks",
      "task_demo"
    ]
  },
  {
    "objectID": "docs/reference/tasks.task_demo.html#safety-critical-features",
    "href": "docs/reference/tasks.task_demo.html#safety-critical-features",
    "title": "tasks.task_demo",
    "section": "",
    "text": "Uses load_actual_combined from spotforecast2_safe for validated data loading\nDemoConfig provides immutable configuration with sensible defaults\nPath objects ensure cross-platform compatibility\nComprehensive error handling with file existence checks",
    "crumbs": [
      "API Reference",
      "Tasks",
      "task_demo"
    ]
  },
  {
    "objectID": "docs/reference/tasks.task_demo.html#examples",
    "href": "docs/reference/tasks.task_demo.html#examples",
    "title": "tasks.task_demo",
    "section": "",
    "text": "Run the demo:\n&gt;&gt;&gt; python tasks/task_demo.py\nForce training (case-insensitive boolean):\n&gt;&gt;&gt; python tasks/task_demo.py --force_train false\nSave the plot as a single HTML file (default: task_demo_plot.html):\n&gt;&gt;&gt; python tasks/task_demo.py --html\nSave to a specific path:\n&gt;&gt;&gt; python tasks/task_demo.py --html results/plot.html",
    "crumbs": [
      "API Reference",
      "Tasks",
      "task_demo"
    ]
  },
  {
    "objectID": "docs/reference/tasks.task_demo.html#functions",
    "href": "docs/reference/tasks.task_demo.html#functions",
    "title": "tasks.task_demo",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nmain\nRun the demo, compute predictions for three models, and plot actual vs predicted.\n\n\n\n\n\ntasks.task_demo.main(force_train=True, html_path=None)\nRun the demo, compute predictions for three models, and plot actual vs predicted.",
    "crumbs": [
      "API Reference",
      "Tasks",
      "task_demo"
    ]
  },
  {
    "objectID": "docs/reference/tasks.task_n_to_1.html",
    "href": "docs/reference/tasks.task_n_to_1.html",
    "title": "tasks.task_n_to_1",
    "section": "",
    "text": "tasks.task_n_to_1\ntasks.task_n_to_1",
    "crumbs": [
      "API Reference",
      "Tasks",
      "task_n_to_1"
    ]
  },
  {
    "objectID": "docs/reference/tasks.task_n_to_1_with_covariates.html",
    "href": "docs/reference/tasks.task_n_to_1_with_covariates.html",
    "title": "tasks.task_n_to_1_with_covariates",
    "section": "",
    "text": "tasks.task_n_to_1_with_covariates\nN-to-1 Forecasting with Exogenous Covariates and Prediction Aggregation.\nThis module implements a complete end-to-end pipeline for multi-step time series forecasting with exogenous variables (weather, holidays, calendar features), followed by prediction aggregation using configurable weights.\n\n\n\nPerforms multi-output recursive forecasting with exogenous covariates\nAggregates predictions using weighted combinations\nSupports flexible model selection (string or object-based)\nAllows customization via kwargs for all underlying functions\n\n\n\n\n\nAutomatic weather, holiday, and calendar feature generation\nCyclical and polynomial feature engineering\nConfigurable recursive forecaster with LGBMRegressor default\nWeighted prediction aggregation\nComprehensive parameter flexibility via **kwargs\nDetailed logging and progress tracking\n\n\n\n\nBasic usage with default parameters:\n&gt;&gt;&gt; from spotforecast2.scripts.n_to_1_with_covariates import main\n&gt;&gt;&gt; main()\nWith custom forecast horizon and weights:\n&gt;&gt;&gt; predictions = main(\n...     forecast_horizon=48,\n...     weights=[1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0, 1.0]\n... )\nWith custom location (latitude, longitude):\n&gt;&gt;&gt; predictions = main(\n...     forecast_horizon=24,\n...     latitude=48.1351,\n...     longitude=11.5820,\n...     verbose=True\n... )\nWith feature engineering options:\n&gt;&gt;&gt; predictions = main(\n...     forecast_horizon=24,\n...     include_weather_windows=True,\n...     include_holiday_features=True,\n...     include_poly_features=True,\n... )\nPassing custom estimator object:\n&gt;&gt;&gt; from lightgbm import LGBMRegressor\n&gt;&gt;&gt; estimator = LGBMRegressor(n_estimators=200, learning_rate=0.01)\n&gt;&gt;&gt; predictions = main(forecast_horizon=24, estimator=estimator)\nAvailable Parameters:\n\n\n\nforecast_horizon (int): Number of steps ahead to forecast. Default: 24. contamination (float): Outlier detection threshold [0, 1]. Default: 0.01. window_size (int): Rolling window size for feature engineering. Default: 72. lags (int): Number of lag features to create. Default: 24. train_ratio (float): Train-test split ratio [0, 1]. Default: 0.8. verbose (bool): Enable detailed progress logging. Default: True.\nLocation & Time Parameters: latitude (float): Location latitude for sun features. Default: 51.5136 (Dortmund). longitude (float): Location longitude for sun features. Default: 7.4653 (Dortmund). timezone (str): Timezone for data processing. Default: “UTC”. country_code (str): Country code for holidays (ISO 3166-1 alpha-2). Default: “DE”. state (str): State/region code for holidays (depends on country). Default: “NW”.\n\n\n\ninclude_weather_windows (bool): Include rolling weather statistics. Default: False. include_holiday_features (bool): Include holiday indicator features. Default: False. include_poly_features (bool): Include polynomial interaction features. Default: False.\n\n\n\nestimator (Optional[Union[str, object]]): Forecaster estimator. Can be: - None: Uses default LGBMRegressor(n_estimators=100) - “ForecasterRecursive”: String reference (uses default) - LGBMRegressor(…): Custom estimator object Default: None.\n\n\n\nweights (Optional[Union[Dict[str, float], List[float], np.ndarray]]): Weights for prediction aggregation. Can be: - None: Defaults to uniform weights (1.0 for each column) - Dict: Column name -&gt; weight mapping - List/Array: Weights in column order Default: [1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0, 1.0].\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nmain\nExecute the complete N-to-1 forecasting pipeline with default parameters.\n\n\nn_to_1_with_covariates\nExecute N-to-1 forecasting pipeline with exogenous covariates.\n\n\n\n\n\ntasks.task_n_to_1_with_covariates.main()\nExecute the complete N-to-1 forecasting pipeline with default parameters.\nThis is the entry point when running the script directly. It executes the full forecasting pipeline with default settings and prints comprehensive results.\nThe default configuration: - Forecasts 24 steps ahead - Uses Dortmund, Germany coordinates - Applies default contamination and window parameters - Aggregates with predefined weights - Provides verbose output\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone. Results are printed to stdout.\n\n\n\n\n\n\nRun the script directly:\n&gt;&gt;&gt; python n_to_1_with_covariates.py\nOr call main() programmatically:\n&gt;&gt;&gt; from spotforecast2.scripts.n_to_1_with_covariates import main\n&gt;&gt;&gt; main()\n\n\n\n\ntasks.task_n_to_1_with_covariates.n_to_1_with_covariates(\n    forecast_horizon=24,\n    contamination=0.01,\n    window_size=72,\n    lags=24,\n    train_ratio=0.8,\n    latitude=51.5136,\n    longitude=7.4653,\n    timezone='UTC',\n    country_code='DE',\n    state='NW',\n    estimator=None,\n    include_weather_windows=False,\n    include_holiday_features=False,\n    include_poly_features=False,\n    weights=None,\n    verbose=True,\n    show_progress=True,\n    **kwargs,\n)\nExecute N-to-1 forecasting pipeline with exogenous covariates.\nThis function performs a complete time series forecasting workflow: 1. Fetches and preprocesses data 2. Engineers features (calendar, weather, holidays, cyclical, polynomial) 3. Trains recursive forecaster on multiple targets 4. Aggregates predictions using weighted combination\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nforecast_horizon\nint\nNumber of forecast steps ahead. Determines how many time steps to predict into the future. Typical values: 24 (1 day), 48 (2 days), 168 (1 week). Default: 24.\n24\n\n\ncontamination\nfloat\nOutlier contamination level for anomaly detection. Expected proportion of outliers in the training data [0, 1]. Higher values detect fewer outliers. Default: 0.01 (1%).\n0.01\n\n\nwindow_size\nint\nRolling window size for feature engineering (hours). Size of the rolling window for computing statistics. Must be &gt; lags. Typical range: 24-168. Default: 72.\n72\n\n\nlags\nint\nNumber of lagged features to create. Creates AR(p) features with p=lags. Typical values: 12, 24, 48. Default: 24.\n24\n\n\ntrain_ratio\nfloat\nProportion of data for training [0, 1]. Remaining data (1 - train_ratio) used for validation/testing. Typical values: 0.7-0.9. Default: 0.8.\n0.8\n\n\nlatitude\nfloat\nGeographic latitude for solar features. Used to compute sunrise/sunset times for day/night features. Default: 51.5136 (Dortmund, Germany).\n51.5136\n\n\nlongitude\nfloat\nGeographic longitude for solar features. Used to compute sunrise/sunset times for day/night features. Default: 7.4653 (Dortmund, Germany).\n7.4653\n\n\ntimezone\nstr\nTimezone for time-based features. Any timezone recognized by pytz. Default: “UTC”.\n'UTC'\n\n\ncountry_code\nstr\nISO 3166-1 alpha-2 country code for holidays. Examples: “DE” (Germany), “US” (USA), “GB” (UK). Default: “DE”.\n'DE'\n\n\nstate\nstr\nState/region code for holidays. Country-dependent. For Germany: “BW”, “BY”, “NW”, etc. Default: “NW” (Nordrhein-Westfalen).\n'NW'\n\n\nestimator\nOptional[Union[str, object]]\nForecaster model. Can be: - None: Uses LGBMRegressor(n_estimators=100, verbose=-1). - “ForecasterRecursive”: References default estimator (same as None). - LGBMRegressor(…): Custom pre-configured estimator. - Any sklearn-compatible regressor. Default: None.\nNone\n\n\ninclude_weather_windows\nbool\nAdd rolling weather statistics. Creates moving averages, min, max of weather features over multiple windows (1D, 7D). Increases feature count significantly. Default: False.\nFalse\n\n\ninclude_holiday_features\nbool\nAdd holiday binary indicators. Creates features indicating holidays and special dates. Useful for capturing demand patterns around holidays. Default: False.\nFalse\n\n\ninclude_poly_features\nbool\nAdd polynomial interactions. Creates 2nd-order interaction terms between selected features. Useful for capturing non-linear relationships. Default: False.\nFalse\n\n\nweights\nOptional[Union[Dict[str, float], List[float], np.ndarray]]\nWeights for combining multi-output predictions. Can be: - None: Default weights [1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0, 1.0] - Dict: {“col_name”: weight, …} for specific columns - List: [w1, w2, …] in column order - np.ndarray: Same as list Default: None (uses default weights).\nNone\n\n\nverbose\nbool\nEnable progress logging. Prints intermediate results and timestamps. Default: True.\nTrue\n\n\nshow_progress\nbool\nShow a progress bar for major pipeline steps. Default: True.\nTrue\n\n\n**kwargs\nAny\nAdditional parameters for underlying functions. These are passed to n2n_predict_with_covariates(). Examples: - freq: Frequency for data resampling. Default: “h” (hourly). - columns: Specific columns to forecast. Default: None (all). Any parameter accepted by n2n_predict_with_covariates().\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTuple[pd.DataFrame, pd.Series, Dict, Dict]\nTuple[pd.DataFrame, pd.Series, Dict, Dict]: A tuple containing: - predictions (pd.DataFrame): Multi-output forecasts from recursive model. Each column represents a target variable. Index is datetime matching the forecast period. - combined_prediction (pd.Series): Aggregated forecast from weighted combination. Single column combining all output predictions. Index is datetime matching the forecast period. - model_metrics (Dict): Performance metrics from recursive forecaster. Keys may include: ‘mae’, ‘rmse’, ‘mape’, etc. - feature_info (Dict): Information about engineered features. Contains feature counts, types, and engineering details.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf forecast_horizon &lt;= 0 or invalid parameter combinations.\n\n\n\nFileNotFoundError\nIf data source files cannot be accessed.\n\n\n\nRuntimeError\nIf model training fails or data processing errors occur.\n\n\n\n\n\n\nBasic usage (uses all defaults):\n&gt;&gt;&gt; predictions, combined, metrics, features = n_to_1_with_covariates()\n&gt;&gt;&gt; print(f\"Predictions shape: {predictions.shape}\")\n&gt;&gt;&gt; print(f\"Combined forecast head:\\n{combined.head()}\")\nCustom location and forecast horizon:\n&gt;&gt;&gt; predictions, combined, metrics, features = n_to_1_with_covariates(\n...     forecast_horizon=48,\n...     latitude=48.1351,\n...     longitude=11.5820,\n...     country_code=\"DE\",\n...     state=\"BY\",\n...     verbose=True\n... )\nWith feature engineering enabled:\n&gt;&gt;&gt; predictions, combined, metrics, features = n_to_1_with_covariates(\n...     forecast_horizon=24,\n...     include_weather_windows=True,\n...     include_holiday_features=True,\n...     include_poly_features=True,\n...     verbose=True\n... )\nCustom estimator and weights:\n&gt;&gt;&gt; from lightgbm import LGBMRegressor\n&gt;&gt;&gt; custom_estimator = LGBMRegressor(\n...     n_estimators=200,\n...     learning_rate=0.01,\n...     max_depth=7\n... )\n&gt;&gt;&gt; custom_weights = [1.0, 1.0, -0.5, -0.5]\n&gt;&gt;&gt; predictions, combined, metrics, features = n_to_1_with_covariates(\n...     forecast_horizon=24,\n...     estimator=custom_estimator,\n...     weights=custom_weights,\n...     verbose=True\n... )\nWith all advanced options:\n&gt;&gt;&gt; predictions, combined, metrics, features = n_to_1_with_covariates(\n...     forecast_horizon=72,\n...     contamination=0.02,\n...     window_size=168,\n...     lags=48,\n...     train_ratio=0.75,\n...     latitude=50.1109,\n...     longitude=8.6821,\n...     timezone=\"Europe/Berlin\",\n...     country_code=\"DE\",\n...     state=\"HE\",\n...     include_weather_windows=True,\n...     include_holiday_features=True,\n...     include_poly_features=True,\n...     weights={\"power\": 1.0, \"demand\": 0.8},\n...     verbose=True,\n...     freq=\"h\",\n... )\n&gt;&gt;&gt; print(f\"Model Metrics: {metrics}\")\n&gt;&gt;&gt; print(f\"Feature Info: {features}\")",
    "crumbs": [
      "API Reference",
      "Tasks",
      "task_n_to_1_with_covariates"
    ]
  },
  {
    "objectID": "docs/reference/tasks.task_n_to_1_with_covariates.html#the-pipeline",
    "href": "docs/reference/tasks.task_n_to_1_with_covariates.html#the-pipeline",
    "title": "tasks.task_n_to_1_with_covariates",
    "section": "",
    "text": "Performs multi-output recursive forecasting with exogenous covariates\nAggregates predictions using weighted combinations\nSupports flexible model selection (string or object-based)\nAllows customization via kwargs for all underlying functions",
    "crumbs": [
      "API Reference",
      "Tasks",
      "task_n_to_1_with_covariates"
    ]
  },
  {
    "objectID": "docs/reference/tasks.task_n_to_1_with_covariates.html#key-features",
    "href": "docs/reference/tasks.task_n_to_1_with_covariates.html#key-features",
    "title": "tasks.task_n_to_1_with_covariates",
    "section": "",
    "text": "Automatic weather, holiday, and calendar feature generation\nCyclical and polynomial feature engineering\nConfigurable recursive forecaster with LGBMRegressor default\nWeighted prediction aggregation\nComprehensive parameter flexibility via **kwargs\nDetailed logging and progress tracking",
    "crumbs": [
      "API Reference",
      "Tasks",
      "task_n_to_1_with_covariates"
    ]
  },
  {
    "objectID": "docs/reference/tasks.task_n_to_1_with_covariates.html#examples",
    "href": "docs/reference/tasks.task_n_to_1_with_covariates.html#examples",
    "title": "tasks.task_n_to_1_with_covariates",
    "section": "",
    "text": "Basic usage with default parameters:\n&gt;&gt;&gt; from spotforecast2.scripts.n_to_1_with_covariates import main\n&gt;&gt;&gt; main()\nWith custom forecast horizon and weights:\n&gt;&gt;&gt; predictions = main(\n...     forecast_horizon=48,\n...     weights=[1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0, 1.0]\n... )\nWith custom location (latitude, longitude):\n&gt;&gt;&gt; predictions = main(\n...     forecast_horizon=24,\n...     latitude=48.1351,\n...     longitude=11.5820,\n...     verbose=True\n... )\nWith feature engineering options:\n&gt;&gt;&gt; predictions = main(\n...     forecast_horizon=24,\n...     include_weather_windows=True,\n...     include_holiday_features=True,\n...     include_poly_features=True,\n... )\nPassing custom estimator object:\n&gt;&gt;&gt; from lightgbm import LGBMRegressor\n&gt;&gt;&gt; estimator = LGBMRegressor(n_estimators=200, learning_rate=0.01)\n&gt;&gt;&gt; predictions = main(forecast_horizon=24, estimator=estimator)\nAvailable Parameters:",
    "crumbs": [
      "API Reference",
      "Tasks",
      "task_n_to_1_with_covariates"
    ]
  },
  {
    "objectID": "docs/reference/tasks.task_n_to_1_with_covariates.html#forecasting-parameters",
    "href": "docs/reference/tasks.task_n_to_1_with_covariates.html#forecasting-parameters",
    "title": "tasks.task_n_to_1_with_covariates",
    "section": "",
    "text": "forecast_horizon (int): Number of steps ahead to forecast. Default: 24. contamination (float): Outlier detection threshold [0, 1]. Default: 0.01. window_size (int): Rolling window size for feature engineering. Default: 72. lags (int): Number of lag features to create. Default: 24. train_ratio (float): Train-test split ratio [0, 1]. Default: 0.8. verbose (bool): Enable detailed progress logging. Default: True.\nLocation & Time Parameters: latitude (float): Location latitude for sun features. Default: 51.5136 (Dortmund). longitude (float): Location longitude for sun features. Default: 7.4653 (Dortmund). timezone (str): Timezone for data processing. Default: “UTC”. country_code (str): Country code for holidays (ISO 3166-1 alpha-2). Default: “DE”. state (str): State/region code for holidays (depends on country). Default: “NW”.",
    "crumbs": [
      "API Reference",
      "Tasks",
      "task_n_to_1_with_covariates"
    ]
  },
  {
    "objectID": "docs/reference/tasks.task_n_to_1_with_covariates.html#feature-engineering-parameters",
    "href": "docs/reference/tasks.task_n_to_1_with_covariates.html#feature-engineering-parameters",
    "title": "tasks.task_n_to_1_with_covariates",
    "section": "",
    "text": "include_weather_windows (bool): Include rolling weather statistics. Default: False. include_holiday_features (bool): Include holiday indicator features. Default: False. include_poly_features (bool): Include polynomial interaction features. Default: False.",
    "crumbs": [
      "API Reference",
      "Tasks",
      "task_n_to_1_with_covariates"
    ]
  },
  {
    "objectID": "docs/reference/tasks.task_n_to_1_with_covariates.html#model-parameters",
    "href": "docs/reference/tasks.task_n_to_1_with_covariates.html#model-parameters",
    "title": "tasks.task_n_to_1_with_covariates",
    "section": "",
    "text": "estimator (Optional[Union[str, object]]): Forecaster estimator. Can be: - None: Uses default LGBMRegressor(n_estimators=100) - “ForecasterRecursive”: String reference (uses default) - LGBMRegressor(…): Custom estimator object Default: None.",
    "crumbs": [
      "API Reference",
      "Tasks",
      "task_n_to_1_with_covariates"
    ]
  },
  {
    "objectID": "docs/reference/tasks.task_n_to_1_with_covariates.html#aggregation-parameters",
    "href": "docs/reference/tasks.task_n_to_1_with_covariates.html#aggregation-parameters",
    "title": "tasks.task_n_to_1_with_covariates",
    "section": "",
    "text": "weights (Optional[Union[Dict[str, float], List[float], np.ndarray]]): Weights for prediction aggregation. Can be: - None: Defaults to uniform weights (1.0 for each column) - Dict: Column name -&gt; weight mapping - List/Array: Weights in column order Default: [1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0, 1.0].",
    "crumbs": [
      "API Reference",
      "Tasks",
      "task_n_to_1_with_covariates"
    ]
  },
  {
    "objectID": "docs/reference/tasks.task_n_to_1_with_covariates.html#functions",
    "href": "docs/reference/tasks.task_n_to_1_with_covariates.html#functions",
    "title": "tasks.task_n_to_1_with_covariates",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nmain\nExecute the complete N-to-1 forecasting pipeline with default parameters.\n\n\nn_to_1_with_covariates\nExecute N-to-1 forecasting pipeline with exogenous covariates.\n\n\n\n\n\ntasks.task_n_to_1_with_covariates.main()\nExecute the complete N-to-1 forecasting pipeline with default parameters.\nThis is the entry point when running the script directly. It executes the full forecasting pipeline with default settings and prints comprehensive results.\nThe default configuration: - Forecasts 24 steps ahead - Uses Dortmund, Germany coordinates - Applies default contamination and window parameters - Aggregates with predefined weights - Provides verbose output\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone. Results are printed to stdout.\n\n\n\n\n\n\nRun the script directly:\n&gt;&gt;&gt; python n_to_1_with_covariates.py\nOr call main() programmatically:\n&gt;&gt;&gt; from spotforecast2.scripts.n_to_1_with_covariates import main\n&gt;&gt;&gt; main()\n\n\n\n\ntasks.task_n_to_1_with_covariates.n_to_1_with_covariates(\n    forecast_horizon=24,\n    contamination=0.01,\n    window_size=72,\n    lags=24,\n    train_ratio=0.8,\n    latitude=51.5136,\n    longitude=7.4653,\n    timezone='UTC',\n    country_code='DE',\n    state='NW',\n    estimator=None,\n    include_weather_windows=False,\n    include_holiday_features=False,\n    include_poly_features=False,\n    weights=None,\n    verbose=True,\n    show_progress=True,\n    **kwargs,\n)\nExecute N-to-1 forecasting pipeline with exogenous covariates.\nThis function performs a complete time series forecasting workflow: 1. Fetches and preprocesses data 2. Engineers features (calendar, weather, holidays, cyclical, polynomial) 3. Trains recursive forecaster on multiple targets 4. Aggregates predictions using weighted combination\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nforecast_horizon\nint\nNumber of forecast steps ahead. Determines how many time steps to predict into the future. Typical values: 24 (1 day), 48 (2 days), 168 (1 week). Default: 24.\n24\n\n\ncontamination\nfloat\nOutlier contamination level for anomaly detection. Expected proportion of outliers in the training data [0, 1]. Higher values detect fewer outliers. Default: 0.01 (1%).\n0.01\n\n\nwindow_size\nint\nRolling window size for feature engineering (hours). Size of the rolling window for computing statistics. Must be &gt; lags. Typical range: 24-168. Default: 72.\n72\n\n\nlags\nint\nNumber of lagged features to create. Creates AR(p) features with p=lags. Typical values: 12, 24, 48. Default: 24.\n24\n\n\ntrain_ratio\nfloat\nProportion of data for training [0, 1]. Remaining data (1 - train_ratio) used for validation/testing. Typical values: 0.7-0.9. Default: 0.8.\n0.8\n\n\nlatitude\nfloat\nGeographic latitude for solar features. Used to compute sunrise/sunset times for day/night features. Default: 51.5136 (Dortmund, Germany).\n51.5136\n\n\nlongitude\nfloat\nGeographic longitude for solar features. Used to compute sunrise/sunset times for day/night features. Default: 7.4653 (Dortmund, Germany).\n7.4653\n\n\ntimezone\nstr\nTimezone for time-based features. Any timezone recognized by pytz. Default: “UTC”.\n'UTC'\n\n\ncountry_code\nstr\nISO 3166-1 alpha-2 country code for holidays. Examples: “DE” (Germany), “US” (USA), “GB” (UK). Default: “DE”.\n'DE'\n\n\nstate\nstr\nState/region code for holidays. Country-dependent. For Germany: “BW”, “BY”, “NW”, etc. Default: “NW” (Nordrhein-Westfalen).\n'NW'\n\n\nestimator\nOptional[Union[str, object]]\nForecaster model. Can be: - None: Uses LGBMRegressor(n_estimators=100, verbose=-1). - “ForecasterRecursive”: References default estimator (same as None). - LGBMRegressor(…): Custom pre-configured estimator. - Any sklearn-compatible regressor. Default: None.\nNone\n\n\ninclude_weather_windows\nbool\nAdd rolling weather statistics. Creates moving averages, min, max of weather features over multiple windows (1D, 7D). Increases feature count significantly. Default: False.\nFalse\n\n\ninclude_holiday_features\nbool\nAdd holiday binary indicators. Creates features indicating holidays and special dates. Useful for capturing demand patterns around holidays. Default: False.\nFalse\n\n\ninclude_poly_features\nbool\nAdd polynomial interactions. Creates 2nd-order interaction terms between selected features. Useful for capturing non-linear relationships. Default: False.\nFalse\n\n\nweights\nOptional[Union[Dict[str, float], List[float], np.ndarray]]\nWeights for combining multi-output predictions. Can be: - None: Default weights [1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0, 1.0] - Dict: {“col_name”: weight, …} for specific columns - List: [w1, w2, …] in column order - np.ndarray: Same as list Default: None (uses default weights).\nNone\n\n\nverbose\nbool\nEnable progress logging. Prints intermediate results and timestamps. Default: True.\nTrue\n\n\nshow_progress\nbool\nShow a progress bar for major pipeline steps. Default: True.\nTrue\n\n\n**kwargs\nAny\nAdditional parameters for underlying functions. These are passed to n2n_predict_with_covariates(). Examples: - freq: Frequency for data resampling. Default: “h” (hourly). - columns: Specific columns to forecast. Default: None (all). Any parameter accepted by n2n_predict_with_covariates().\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTuple[pd.DataFrame, pd.Series, Dict, Dict]\nTuple[pd.DataFrame, pd.Series, Dict, Dict]: A tuple containing: - predictions (pd.DataFrame): Multi-output forecasts from recursive model. Each column represents a target variable. Index is datetime matching the forecast period. - combined_prediction (pd.Series): Aggregated forecast from weighted combination. Single column combining all output predictions. Index is datetime matching the forecast period. - model_metrics (Dict): Performance metrics from recursive forecaster. Keys may include: ‘mae’, ‘rmse’, ‘mape’, etc. - feature_info (Dict): Information about engineered features. Contains feature counts, types, and engineering details.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf forecast_horizon &lt;= 0 or invalid parameter combinations.\n\n\n\nFileNotFoundError\nIf data source files cannot be accessed.\n\n\n\nRuntimeError\nIf model training fails or data processing errors occur.\n\n\n\n\n\n\nBasic usage (uses all defaults):\n&gt;&gt;&gt; predictions, combined, metrics, features = n_to_1_with_covariates()\n&gt;&gt;&gt; print(f\"Predictions shape: {predictions.shape}\")\n&gt;&gt;&gt; print(f\"Combined forecast head:\\n{combined.head()}\")\nCustom location and forecast horizon:\n&gt;&gt;&gt; predictions, combined, metrics, features = n_to_1_with_covariates(\n...     forecast_horizon=48,\n...     latitude=48.1351,\n...     longitude=11.5820,\n...     country_code=\"DE\",\n...     state=\"BY\",\n...     verbose=True\n... )\nWith feature engineering enabled:\n&gt;&gt;&gt; predictions, combined, metrics, features = n_to_1_with_covariates(\n...     forecast_horizon=24,\n...     include_weather_windows=True,\n...     include_holiday_features=True,\n...     include_poly_features=True,\n...     verbose=True\n... )\nCustom estimator and weights:\n&gt;&gt;&gt; from lightgbm import LGBMRegressor\n&gt;&gt;&gt; custom_estimator = LGBMRegressor(\n...     n_estimators=200,\n...     learning_rate=0.01,\n...     max_depth=7\n... )\n&gt;&gt;&gt; custom_weights = [1.0, 1.0, -0.5, -0.5]\n&gt;&gt;&gt; predictions, combined, metrics, features = n_to_1_with_covariates(\n...     forecast_horizon=24,\n...     estimator=custom_estimator,\n...     weights=custom_weights,\n...     verbose=True\n... )\nWith all advanced options:\n&gt;&gt;&gt; predictions, combined, metrics, features = n_to_1_with_covariates(\n...     forecast_horizon=72,\n...     contamination=0.02,\n...     window_size=168,\n...     lags=48,\n...     train_ratio=0.75,\n...     latitude=50.1109,\n...     longitude=8.6821,\n...     timezone=\"Europe/Berlin\",\n...     country_code=\"DE\",\n...     state=\"HE\",\n...     include_weather_windows=True,\n...     include_holiday_features=True,\n...     include_poly_features=True,\n...     weights={\"power\": 1.0, \"demand\": 0.8},\n...     verbose=True,\n...     freq=\"h\",\n... )\n&gt;&gt;&gt; print(f\"Model Metrics: {metrics}\")\n&gt;&gt;&gt; print(f\"Feature Info: {features}\")",
    "crumbs": [
      "API Reference",
      "Tasks",
      "task_n_to_1_with_covariates"
    ]
  },
  {
    "objectID": "docs/reference/utils.data_transform.html",
    "href": "docs/reference/utils.data_transform.html",
    "title": "utils.data_transform",
    "section": "",
    "text": "utils.data_transform\nData transformation utilities for time series forecasting.\nThis module provides functions for normalizing and transforming data formats.\n\n\n\n\n\nName\nDescription\n\n\n\n\nexpand_index\nCreate a new index extending from the end of the original index.\n\n\ninput_to_frame\nConvert input data to a pandas DataFrame.\n\n\n\n\n\nutils.data_transform.expand_index(index, steps)\nCreate a new index extending from the end of the original index.\nThis function generates future indices for forecasting by extending the time series index by a specified number of steps. Handles both DatetimeIndex and RangeIndex appropriately.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nindex\nUnion[pd.Index, None]\nOriginal pandas Index (DatetimeIndex or RangeIndex). If None, creates a RangeIndex starting from 0.\nrequired\n\n\nsteps\nint\nNumber of future steps to generate.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Index\nNew pandas Index with steps future periods.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf steps is not an integer, or if index is neither DatetimeIndex nor RangeIndex.\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2.utils.data_transform import expand_index\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DatetimeIndex\n&gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=5, freq=\"D\")\n&gt;&gt;&gt; new_index = expand_index(dates, 3)\n&gt;&gt;&gt; new_index\nDatetimeIndex(['2023-01-06', '2023-01-07', '2023-01-08'], dtype='datetime64[ns]', freq='D')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # RangeIndex\n&gt;&gt;&gt; range_idx = pd.RangeIndex(start=0, stop=10)\n&gt;&gt;&gt; new_index = expand_index(range_idx, 5)\n&gt;&gt;&gt; new_index\nRangeIndex(start=10, stop=15, step=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # None index (creates new RangeIndex)\n&gt;&gt;&gt; new_index = expand_index(None, 3)\n&gt;&gt;&gt; new_index\nRangeIndex(start=0, stop=3, step=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: steps not an integer\n&gt;&gt;&gt; try:\n...     expand_index(dates, 3.5)\n... except TypeError as e:\n...     print(\"Error: steps must be an integer\")\nError: steps must be an integer\n\n\n\n\nutils.data_transform.input_to_frame(data, input_name)\nConvert input data to a pandas DataFrame.\nThis function ensures consistent DataFrame format for internal processing. If data is already a DataFrame, it’s returned as-is. If it’s a Series, it’s converted to a single-column DataFrame.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\nUnion[pd.Series, pd.DataFrame]\nInput data as pandas Series or DataFrame.\nrequired\n\n\ninput_name\nstr\nName of the input data type. Accepted values are: - ‘y’: Target time series - ‘last_window’: Last window for prediction - ‘exog’: Exogenous variables\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\nDataFrame version of the input data. For Series input, uses the series\n\n\n\npd.DataFrame\nname if available, otherwise uses a default name based on input_name.\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2.utils.data_transform import input_to_frame\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series with name\n&gt;&gt;&gt; y = pd.Series([1, 2, 3], name=\"sales\")\n&gt;&gt;&gt; df = input_to_frame(y, input_name=\"y\")\n&gt;&gt;&gt; df.columns.tolist()\n['sales']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series without name (uses default)\n&gt;&gt;&gt; y_no_name = pd.Series([1, 2, 3])\n&gt;&gt;&gt; df = input_to_frame(y_no_name, input_name=\"y\")\n&gt;&gt;&gt; df.columns.tolist()\n['y']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DataFrame (returned as-is)\n&gt;&gt;&gt; df_input = pd.DataFrame({\"temp\": [20, 21], \"humidity\": [50, 55]})\n&gt;&gt;&gt; df_output = input_to_frame(df_input, input_name=\"exog\")\n&gt;&gt;&gt; df_output.columns.tolist()\n['temp', 'humidity']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Exog series without name\n&gt;&gt;&gt; exog = pd.Series([10, 20, 30])\n&gt;&gt;&gt; df_exog = input_to_frame(exog, input_name=\"exog\")\n&gt;&gt;&gt; df_exog.columns.tolist()\n['exog']",
    "crumbs": [
      "API Reference",
      "Utils",
      "data_transform"
    ]
  },
  {
    "objectID": "docs/reference/utils.data_transform.html#functions",
    "href": "docs/reference/utils.data_transform.html#functions",
    "title": "utils.data_transform",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nexpand_index\nCreate a new index extending from the end of the original index.\n\n\ninput_to_frame\nConvert input data to a pandas DataFrame.\n\n\n\n\n\nutils.data_transform.expand_index(index, steps)\nCreate a new index extending from the end of the original index.\nThis function generates future indices for forecasting by extending the time series index by a specified number of steps. Handles both DatetimeIndex and RangeIndex appropriately.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nindex\nUnion[pd.Index, None]\nOriginal pandas Index (DatetimeIndex or RangeIndex). If None, creates a RangeIndex starting from 0.\nrequired\n\n\nsteps\nint\nNumber of future steps to generate.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Index\nNew pandas Index with steps future periods.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf steps is not an integer, or if index is neither DatetimeIndex nor RangeIndex.\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2.utils.data_transform import expand_index\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DatetimeIndex\n&gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=5, freq=\"D\")\n&gt;&gt;&gt; new_index = expand_index(dates, 3)\n&gt;&gt;&gt; new_index\nDatetimeIndex(['2023-01-06', '2023-01-07', '2023-01-08'], dtype='datetime64[ns]', freq='D')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # RangeIndex\n&gt;&gt;&gt; range_idx = pd.RangeIndex(start=0, stop=10)\n&gt;&gt;&gt; new_index = expand_index(range_idx, 5)\n&gt;&gt;&gt; new_index\nRangeIndex(start=10, stop=15, step=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # None index (creates new RangeIndex)\n&gt;&gt;&gt; new_index = expand_index(None, 3)\n&gt;&gt;&gt; new_index\nRangeIndex(start=0, stop=3, step=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: steps not an integer\n&gt;&gt;&gt; try:\n...     expand_index(dates, 3.5)\n... except TypeError as e:\n...     print(\"Error: steps must be an integer\")\nError: steps must be an integer\n\n\n\n\nutils.data_transform.input_to_frame(data, input_name)\nConvert input data to a pandas DataFrame.\nThis function ensures consistent DataFrame format for internal processing. If data is already a DataFrame, it’s returned as-is. If it’s a Series, it’s converted to a single-column DataFrame.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\nUnion[pd.Series, pd.DataFrame]\nInput data as pandas Series or DataFrame.\nrequired\n\n\ninput_name\nstr\nName of the input data type. Accepted values are: - ‘y’: Target time series - ‘last_window’: Last window for prediction - ‘exog’: Exogenous variables\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\nDataFrame version of the input data. For Series input, uses the series\n\n\n\npd.DataFrame\nname if available, otherwise uses a default name based on input_name.\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2.utils.data_transform import input_to_frame\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series with name\n&gt;&gt;&gt; y = pd.Series([1, 2, 3], name=\"sales\")\n&gt;&gt;&gt; df = input_to_frame(y, input_name=\"y\")\n&gt;&gt;&gt; df.columns.tolist()\n['sales']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series without name (uses default)\n&gt;&gt;&gt; y_no_name = pd.Series([1, 2, 3])\n&gt;&gt;&gt; df = input_to_frame(y_no_name, input_name=\"y\")\n&gt;&gt;&gt; df.columns.tolist()\n['y']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DataFrame (returned as-is)\n&gt;&gt;&gt; df_input = pd.DataFrame({\"temp\": [20, 21], \"humidity\": [50, 55]})\n&gt;&gt;&gt; df_output = input_to_frame(df_input, input_name=\"exog\")\n&gt;&gt;&gt; df_output.columns.tolist()\n['temp', 'humidity']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Exog series without name\n&gt;&gt;&gt; exog = pd.Series([10, 20, 30])\n&gt;&gt;&gt; df_exog = input_to_frame(exog, input_name=\"exog\")\n&gt;&gt;&gt; df_exog.columns.tolist()\n['exog']",
    "crumbs": [
      "API Reference",
      "Utils",
      "data_transform"
    ]
  },
  {
    "objectID": "docs/reference/utils.generate_holiday.html",
    "href": "docs/reference/utils.generate_holiday.html",
    "title": "utils.generate_holiday",
    "section": "",
    "text": "utils.generate_holiday\nutils.generate_holiday\nUtilities for generating holiday dataframe as covariate.",
    "crumbs": [
      "API Reference",
      "Utils",
      "generate_holiday"
    ]
  },
  {
    "objectID": "docs/reference/weather.weather_client.html",
    "href": "docs/reference/weather.weather_client.html",
    "title": "weather.weather_client",
    "section": "",
    "text": "weather.weather_client\nweather.weather_client\nWeather data fetching and processing using Open-Meteo API.",
    "crumbs": [
      "API Reference",
      "Weather",
      "weather_client"
    ]
  },
  {
    "objectID": "docs/preprocessing/outliers.html",
    "href": "docs/preprocessing/outliers.html",
    "title": "Outlier Detection and Visualization",
    "section": "",
    "text": "This module provides comprehensive tools for detecting and visualizing outliers in time series data using the Isolation Forest algorithm.\n\n\nThe outlier detection module includes three main functions:\n\nget_outliers() - Detect outliers using Isolation Forest\nvisualize_outliers_hist() - Visualize outliers with static histograms\nvisualize_outliers_plotly_scatter() - Visualize outliers with interactive Plotly scatter plots\n\nThese functions work together to provide a complete workflow for outlier analysis in time series data.\n\n\n\nThe outlier visualization functions require matplotlib for histograms and plotly for interactive scatter plots.\nUsing pip:\npip install matplotlib plotly\nUsing uv:\nuv pip install matplotlib plotly\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom spotforecast2_safe.preprocessing.outlier import get_outliers\n\n# Create sample data with outliers\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'temperature': np.concatenate([\n        np.random.normal(20, 5, 100),\n        [50, 60, 70]  # outliers\n    ]),\n    'humidity': np.concatenate([\n        np.random.normal(60, 10, 100),\n        [95, 98, 99]  # outliers\n    ])\n})\n\n# Detect outliers\noutliers = get_outliers(data, contamination=0.03)\n\nfor col, outlier_vals in outliers.items():\n    print(f\"{col}: {len(outlier_vals)} outliers detected\")\n\n\n\nfrom spotforecast2.preprocessing.outlier_plots import visualize_outliers_hist\n\n# Create sample data\nnp.random.seed(42)\ndata_original = pd.DataFrame({\n    'temperature': np.concatenate([\n        np.random.normal(20, 5, 100),\n        [50, 60, 70]  # outliers\n    ])\n})\n\ndata_cleaned = data_original.copy()\n\n# Visualize outliers\nvisualize_outliers_hist(\n    data_cleaned,\n    data_original,\n    contamination=0.03,\n    figsize=(12, 5),\n    alpha=0.7\n)\n\n\n\nfrom spotforecast2.preprocessing.outlier_plots import visualize_outliers_plotly_scatter\n\n# Create time series data\ndates = pd.date_range('2024-01-01', periods=103, freq='h')\ndata_original = pd.DataFrame({\n    'temperature': np.concatenate([\n        np.random.normal(20, 5, 100),\n        [50, 60, 70]  # outliers\n    ])\n}, index=dates)\n\ndata_cleaned = data_original.copy()\n\n# Visualize outliers with interactive plot\nvisualize_outliers_plotly_scatter(\n    data_cleaned,\n    data_original,\n    contamination=0.03,\n    template='plotly_white'\n)\n\n\n\n\n\n\nDetect outliers in each column using Isolation Forest.\nSignature:\ndef get_outliers(\n    data: pd.DataFrame,\n    data_original: Optional[pd.DataFrame] = None,\n    contamination: float = 0.01,\n    random_state: int = 1234,\n) -&gt; Dict[str, pd.Series]\nParameters:\n\n\n\n\n\n\n\n\n\nParameter\nType\nDefault\nDescription\n\n\n\n\ndata\nDataFrame\nRequired\nThe input DataFrame to check for outliers\n\n\ndata_original\nDataFrame\nNone\nOptional original DataFrame before outlier detection. If provided, helps identify which values became NaN due to outlier detection\n\n\ncontamination\nfloat\n0.01\nThe estimated proportion of outliers in the dataset (between 0 and 1)\n\n\nrandom_state\nint\n1234\nRandom seed for reproducibility\n\n\n\nReturns:\nA dictionary mapping column names to pandas Series of outlier values.\nRaises:\n\nValueError - If data is empty or contains no columns\n\nExample:\nimport pandas as pd\nimport numpy as np\nfrom spotforecast2_safe.preprocessing.outlier import get_outliers\n\n# Create sample data\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'A': np.concatenate([np.random.normal(0, 1, 100), [10, 11, 12]]),\n    'B': np.concatenate([np.random.normal(5, 2, 100), [100, 110, 120]])\n})\n\n# Detect outliers\noutliers = get_outliers(data, contamination=0.03)\nfor col, outlier_vals in outliers.items():\n    print(f\"{col}: {len(outlier_vals)} outliers detected\")\n\n\n\nVisualize outliers using stacked histograms.\nSignature:\ndef visualize_outliers_hist(\n    data: pd.DataFrame,\n    data_original: pd.DataFrame,\n    columns: Optional[list[str]] = None,\n    contamination: float = 0.01,\n    random_state: int = 1234,\n    figsize: tuple[int, int] = (10, 5),\n    bins: int = 50,\n    **kwargs: Any,\n) -&gt; None\nParameters:\n\n\n\n\n\n\n\n\n\nParameter\nType\nDefault\nDescription\n\n\n\n\ndata\nDataFrame\nRequired\nThe DataFrame with cleaned data (outliers may be NaN)\n\n\ndata_original\nDataFrame\nRequired\nThe original DataFrame before outlier detection\n\n\ncolumns\nlist[str]\nNone\nList of column names to visualize. If None, all columns are used\n\n\ncontamination\nfloat\n0.01\nThe estimated proportion of outliers in the dataset\n\n\nrandom_state\nint\n1234\nRandom seed for reproducibility\n\n\nfigsize\ntuple[int, int]\n(10, 5)\nFigure size as (width, height)\n\n\nbins\nint\n50\nNumber of histogram bins\n\n\n**kwargs\nAny\n-\nAdditional keyword arguments passed to plt.hist() (e.g., color, alpha, edgecolor)\n\n\n\nReturns:\nNone. Displays matplotlib figures.\nRaises:\n\nValueError - If data or data_original is empty, or if specified columns don’t exist\n\nExample:\nimport pandas as pd\nimport numpy as np\nfrom spotforecast2.preprocessing.outlier_plots import visualize_outliers_hist\n\n# Create sample data\nnp.random.seed(42)\ndata_original = pd.DataFrame({\n    'temperature': np.concatenate([\n        np.random.normal(20, 5, 100),\n        [50, 60, 70]  # outliers\n    ]),\n    'humidity': np.concatenate([\n        np.random.normal(60, 10, 100),\n        [95, 98, 99]  # outliers\n    ])\n})\n\ndata_cleaned = data_original.copy()\n\n# Visualize outliers\nvisualize_outliers_hist(\n    data_cleaned,\n    data_original,\n    contamination=0.03,\n    figsize=(12, 5),\n    alpha=0.7,\n    edgecolor='black'\n)\n\n\n\nVisualize outliers using interactive Plotly scatter plots.\nSignature:\ndef visualize_outliers_plotly_scatter(\n    data: pd.DataFrame,\n    data_original: pd.DataFrame,\n    columns: Optional[list[str]] = None,\n    contamination: float = 0.01,\n    random_state: int = 1234,\n    **kwargs: Any,\n) -&gt; None\nParameters:\n\n\n\n\n\n\n\n\n\nParameter\nType\nDefault\nDescription\n\n\n\n\ndata\nDataFrame\nRequired\nThe DataFrame with cleaned data (outliers may be NaN)\n\n\ndata_original\nDataFrame\nRequired\nThe original DataFrame before outlier detection\n\n\ncolumns\nlist[str]\nNone\nList of column names to visualize. If None, all columns are used\n\n\ncontamination\nfloat\n0.01\nThe estimated proportion of outliers in the dataset\n\n\nrandom_state\nint\n1234\nRandom seed for reproducibility\n\n\n**kwargs\nAny\n-\nAdditional keyword arguments passed to go.Figure.update_layout() (e.g., template, height)\n\n\n\nReturns:\nNone. Displays Plotly figures.\nRaises:\n\nValueError - If data or data_original is empty, or if specified columns don’t exist\nImportError - If plotly is not installed\n\nExample:\nimport pandas as pd\nimport numpy as np\nfrom spotforecast2.preprocessing.outlier_plots import visualize_outliers_plotly_scatter\n\n# Create time series data\nnp.random.seed(42)\ndates = pd.date_range('2024-01-01', periods=103, freq='h')\ndata_original = pd.DataFrame({\n    'temperature': np.concatenate([\n        np.random.normal(20, 5, 100),\n        [50, 60, 70]  # outliers\n    ]),\n    'humidity': np.concatenate([\n        np.random.normal(60, 10, 100),\n        [95, 98, 99]  # outliers\n    ])\n}, index=dates)\n\ndata_cleaned = data_original.copy()\n\n# Visualize outliers\nvisualize_outliers_plotly_scatter(\n    data_cleaned,\n    data_original,\n    contamination=0.03,\n    template='plotly_white',\n    height=600\n)\n\n\n\n\nHere’s a complete example showing the typical workflow for outlier detection and visualization:\nimport pandas as pd\nimport numpy as np\nfrom spotforecast2_safe.preprocessing.outlier import get_outliers\nfrom spotforecast2.preprocessing.outlier_plots import (\n    visualize_outliers_hist,\n    visualize_outliers_plotly_scatter)\n\n# Create realistic time series data with outliers\nnp.random.seed(42)\ndates = pd.date_range('2024-01-01', periods=200, freq='h')\ndata_original = pd.DataFrame({\n    'temperature': np.concatenate([\n        np.random.normal(20, 5, 197),\n        [50, 60, 70]  # outliers\n    ]),\n    'humidity': np.concatenate([\n        np.random.normal(60, 10, 197),\n        [95, 98, 99]  # outliers\n    ]),\n    'pressure': np.concatenate([\n        np.random.normal(1013, 10, 197),\n        [800, 1200, 950]  # outliers\n    ])\n}, index=dates)\n\n# Make a copy for cleaning\ndata_cleaned = data_original.copy()\n\n# Step 1: Detect outliers\nprint(\"=== Outlier Detection ===\")\noutliers = get_outliers(\n    data_original,\n    contamination=0.015\n)\n\nfor col, outlier_vals in outliers.items():\n    pct = (len(outlier_vals) / len(data_original)) * 100\n    print(f\"{col}: {len(outlier_vals)} outliers ({pct:.2f}%)\")\n\n# Step 2: Visualize with histograms\nprint(\"\\n=== Histogram Visualization ===\")\nvisualize_outliers_hist(\n    data_cleaned,\n    data_original,\n    contamination=0.015,\n    figsize=(14, 4),\n    alpha=0.7\n)\n\n# Step 3: Visualize with Plotly (interactive)\nprint(\"\\n=== Interactive Plotly Visualization ===\")\nvisualize_outliers_plotly_scatter(\n    data_cleaned,\n    data_original,\n    contamination=0.015,\n    template='plotly_white'\n)\n\n# Step 4: Selective column visualization\nprint(\"\\n=== Selective Column Analysis ===\")\nvisualize_outliers_hist(\n    data_cleaned,\n    data_original,\n    columns=['temperature', 'humidity'],\n    contamination=0.015\n)\n\n\n\n\n\nThe contamination parameter controls the expected proportion of outliers in the dataset:\n\n0.01 (1%) - Conservative, detects severe outliers only\n0.02 (2%) - Moderate, typical for most applications\n0.05 (5%) - Liberal, detects more potential anomalies\n\nChoose based on your domain knowledge and data characteristics.\n\n\n\nThe random_state parameter ensures reproducibility:\n# Same random_state produces consistent results\noutliers1 = get_outliers(data, random_state=42)\noutliers2 = get_outliers(data, random_state=42)\n# outliers1 == outliers2\n\n\n\nWhen using visualize_outliers_hist(), you can pass additional matplotlib histogram options:\nvisualize_outliers_hist(\n    data_cleaned,\n    data_original,\n    bins=100,           # More granular bins\n    alpha=0.5,          # Transparency\n    edgecolor='black',  # Border around bars\n    linewidth=0.5       # Border thickness\n)\n\n\n\nWhen using visualize_outliers_plotly_scatter(), you can customize the Plotly figure:\nvisualize_outliers_plotly_scatter(\n    data_cleaned,\n    data_original,\n    template='plotly_dark',    # Dark theme\n    height=700,                 # Figure height\n    width=1200                  # Figure width\n)\n\n\n\n\n\n\nThe underlying algorithm uses scikit-learn’s IsolationForest, which:\n\nRandomly selects features and split values\nIsolates anomalies by exploiting their rarity\nAssigns anomaly scores based on path lengths\nMarks points with scores exceeding the contamination threshold as outliers\n\nKey characteristics:\n\nNo distance computation needed (efficient for high dimensions)\nScales well with number of features\nRobust to varying scales\nNo hyperparameter tuning required beyond contamination\n\n\n\n\n\n\n\nClean your data before outlier detection:\n# Remove missing values\ndata_clean = data.dropna()\n\n# Standardize if needed\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndata_scaled = pd.DataFrame(\n    scaler.fit_transform(data),\n    index=data.index,\n    columns=data.columns\n)\n\n\n\nEstimate contamination based on domain knowledge:\n# For known outlier percentage\ncontamination = n_outliers / len(data)\n\n# For exploratory analysis, try multiple values\nfor cont in [0.01, 0.02, 0.05]:\n    outliers = get_outliers(data, contamination=cont)\n    print(f\"Contamination {cont}: {len(outliers)} outliers\")\n\n\n\nAlways visualize results:\n# Histogram for distribution analysis\nvisualize_outliers_hist(data_cleaned, data_original)\n\n# Time series plot for temporal patterns\nvisualize_outliers_plotly_scatter(data_cleaned, data_original)\n\n\n\nVerify outliers make sense in context:\noutliers = get_outliers(data, contamination=0.02)\n\nfor col, vals in outliers.items():\n    print(f\"\\n{col}:\")\n    print(f\"  Regular range: {data[col].min():.2f} - {data[col].max():.2f}\")\n    print(f\"  Outlier values: {sorted(vals.unique())}\")\n    print(f\"  Outlier indices: {list(vals.index)}\")\n\n\n\n\nAll examples in this guide are validated by tests/test_docs_outliers_examples.py with 43 comprehensive pytest cases covering:\n\nBasic outlier detection functionality\nContamination parameter variations (0.01, 0.02, 0.05)\nRandom state reproducibility\nData integrity and value validation\nComplete workflow integration\nEdge cases (small/large datasets, extreme values, NaN handling)\nTimeseries data with DatetimeIndex\nAPI examples and return types\nSafety-critical behavior validation\n\nRun the tests:\n# Run outliers documentation tests\nuv run pytest tests/test_docs_outliers_examples.py -v\n\n# Quick check\nuv run pytest tests/test_docs_outliers_examples.py --tb=no -q\n\n\n\n\n\nSolution: Increase the contamination parameter:\n# Try higher contamination\noutliers = get_outliers(data, contamination=0.05)\n\n\n\nSolution: Decrease the contamination parameter:\n# Be more conservative\noutliers = get_outliers(data, contamination=0.01)\n\n\n\nSolution: Install plotly:\npip install plotly\nOr use histogram visualization instead:\nvisualize_outliers_hist(data_cleaned, data_original)\n\n\n\n\n\nIsolation Forest Algorithm",
    "crumbs": [
      "Preprocessing Guides",
      "Outlier Detection"
    ]
  },
  {
    "objectID": "docs/preprocessing/outliers.html#overview",
    "href": "docs/preprocessing/outliers.html#overview",
    "title": "Outlier Detection and Visualization",
    "section": "",
    "text": "The outlier detection module includes three main functions:\n\nget_outliers() - Detect outliers using Isolation Forest\nvisualize_outliers_hist() - Visualize outliers with static histograms\nvisualize_outliers_plotly_scatter() - Visualize outliers with interactive Plotly scatter plots\n\nThese functions work together to provide a complete workflow for outlier analysis in time series data.",
    "crumbs": [
      "Preprocessing Guides",
      "Outlier Detection"
    ]
  },
  {
    "objectID": "docs/preprocessing/outliers.html#installation",
    "href": "docs/preprocessing/outliers.html#installation",
    "title": "Outlier Detection and Visualization",
    "section": "",
    "text": "The outlier visualization functions require matplotlib for histograms and plotly for interactive scatter plots.\nUsing pip:\npip install matplotlib plotly\nUsing uv:\nuv pip install matplotlib plotly",
    "crumbs": [
      "Preprocessing Guides",
      "Outlier Detection"
    ]
  },
  {
    "objectID": "docs/preprocessing/outliers.html#quick-start",
    "href": "docs/preprocessing/outliers.html#quick-start",
    "title": "Outlier Detection and Visualization",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom spotforecast2_safe.preprocessing.outlier import get_outliers\n\n# Create sample data with outliers\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'temperature': np.concatenate([\n        np.random.normal(20, 5, 100),\n        [50, 60, 70]  # outliers\n    ]),\n    'humidity': np.concatenate([\n        np.random.normal(60, 10, 100),\n        [95, 98, 99]  # outliers\n    ])\n})\n\n# Detect outliers\noutliers = get_outliers(data, contamination=0.03)\n\nfor col, outlier_vals in outliers.items():\n    print(f\"{col}: {len(outlier_vals)} outliers detected\")\n\n\n\nfrom spotforecast2.preprocessing.outlier_plots import visualize_outliers_hist\n\n# Create sample data\nnp.random.seed(42)\ndata_original = pd.DataFrame({\n    'temperature': np.concatenate([\n        np.random.normal(20, 5, 100),\n        [50, 60, 70]  # outliers\n    ])\n})\n\ndata_cleaned = data_original.copy()\n\n# Visualize outliers\nvisualize_outliers_hist(\n    data_cleaned,\n    data_original,\n    contamination=0.03,\n    figsize=(12, 5),\n    alpha=0.7\n)\n\n\n\nfrom spotforecast2.preprocessing.outlier_plots import visualize_outliers_plotly_scatter\n\n# Create time series data\ndates = pd.date_range('2024-01-01', periods=103, freq='h')\ndata_original = pd.DataFrame({\n    'temperature': np.concatenate([\n        np.random.normal(20, 5, 100),\n        [50, 60, 70]  # outliers\n    ])\n}, index=dates)\n\ndata_cleaned = data_original.copy()\n\n# Visualize outliers with interactive plot\nvisualize_outliers_plotly_scatter(\n    data_cleaned,\n    data_original,\n    contamination=0.03,\n    template='plotly_white'\n)",
    "crumbs": [
      "Preprocessing Guides",
      "Outlier Detection"
    ]
  },
  {
    "objectID": "docs/preprocessing/outliers.html#api-reference",
    "href": "docs/preprocessing/outliers.html#api-reference",
    "title": "Outlier Detection and Visualization",
    "section": "",
    "text": "Detect outliers in each column using Isolation Forest.\nSignature:\ndef get_outliers(\n    data: pd.DataFrame,\n    data_original: Optional[pd.DataFrame] = None,\n    contamination: float = 0.01,\n    random_state: int = 1234,\n) -&gt; Dict[str, pd.Series]\nParameters:\n\n\n\n\n\n\n\n\n\nParameter\nType\nDefault\nDescription\n\n\n\n\ndata\nDataFrame\nRequired\nThe input DataFrame to check for outliers\n\n\ndata_original\nDataFrame\nNone\nOptional original DataFrame before outlier detection. If provided, helps identify which values became NaN due to outlier detection\n\n\ncontamination\nfloat\n0.01\nThe estimated proportion of outliers in the dataset (between 0 and 1)\n\n\nrandom_state\nint\n1234\nRandom seed for reproducibility\n\n\n\nReturns:\nA dictionary mapping column names to pandas Series of outlier values.\nRaises:\n\nValueError - If data is empty or contains no columns\n\nExample:\nimport pandas as pd\nimport numpy as np\nfrom spotforecast2_safe.preprocessing.outlier import get_outliers\n\n# Create sample data\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'A': np.concatenate([np.random.normal(0, 1, 100), [10, 11, 12]]),\n    'B': np.concatenate([np.random.normal(5, 2, 100), [100, 110, 120]])\n})\n\n# Detect outliers\noutliers = get_outliers(data, contamination=0.03)\nfor col, outlier_vals in outliers.items():\n    print(f\"{col}: {len(outlier_vals)} outliers detected\")\n\n\n\nVisualize outliers using stacked histograms.\nSignature:\ndef visualize_outliers_hist(\n    data: pd.DataFrame,\n    data_original: pd.DataFrame,\n    columns: Optional[list[str]] = None,\n    contamination: float = 0.01,\n    random_state: int = 1234,\n    figsize: tuple[int, int] = (10, 5),\n    bins: int = 50,\n    **kwargs: Any,\n) -&gt; None\nParameters:\n\n\n\n\n\n\n\n\n\nParameter\nType\nDefault\nDescription\n\n\n\n\ndata\nDataFrame\nRequired\nThe DataFrame with cleaned data (outliers may be NaN)\n\n\ndata_original\nDataFrame\nRequired\nThe original DataFrame before outlier detection\n\n\ncolumns\nlist[str]\nNone\nList of column names to visualize. If None, all columns are used\n\n\ncontamination\nfloat\n0.01\nThe estimated proportion of outliers in the dataset\n\n\nrandom_state\nint\n1234\nRandom seed for reproducibility\n\n\nfigsize\ntuple[int, int]\n(10, 5)\nFigure size as (width, height)\n\n\nbins\nint\n50\nNumber of histogram bins\n\n\n**kwargs\nAny\n-\nAdditional keyword arguments passed to plt.hist() (e.g., color, alpha, edgecolor)\n\n\n\nReturns:\nNone. Displays matplotlib figures.\nRaises:\n\nValueError - If data or data_original is empty, or if specified columns don’t exist\n\nExample:\nimport pandas as pd\nimport numpy as np\nfrom spotforecast2.preprocessing.outlier_plots import visualize_outliers_hist\n\n# Create sample data\nnp.random.seed(42)\ndata_original = pd.DataFrame({\n    'temperature': np.concatenate([\n        np.random.normal(20, 5, 100),\n        [50, 60, 70]  # outliers\n    ]),\n    'humidity': np.concatenate([\n        np.random.normal(60, 10, 100),\n        [95, 98, 99]  # outliers\n    ])\n})\n\ndata_cleaned = data_original.copy()\n\n# Visualize outliers\nvisualize_outliers_hist(\n    data_cleaned,\n    data_original,\n    contamination=0.03,\n    figsize=(12, 5),\n    alpha=0.7,\n    edgecolor='black'\n)\n\n\n\nVisualize outliers using interactive Plotly scatter plots.\nSignature:\ndef visualize_outliers_plotly_scatter(\n    data: pd.DataFrame,\n    data_original: pd.DataFrame,\n    columns: Optional[list[str]] = None,\n    contamination: float = 0.01,\n    random_state: int = 1234,\n    **kwargs: Any,\n) -&gt; None\nParameters:\n\n\n\n\n\n\n\n\n\nParameter\nType\nDefault\nDescription\n\n\n\n\ndata\nDataFrame\nRequired\nThe DataFrame with cleaned data (outliers may be NaN)\n\n\ndata_original\nDataFrame\nRequired\nThe original DataFrame before outlier detection\n\n\ncolumns\nlist[str]\nNone\nList of column names to visualize. If None, all columns are used\n\n\ncontamination\nfloat\n0.01\nThe estimated proportion of outliers in the dataset\n\n\nrandom_state\nint\n1234\nRandom seed for reproducibility\n\n\n**kwargs\nAny\n-\nAdditional keyword arguments passed to go.Figure.update_layout() (e.g., template, height)\n\n\n\nReturns:\nNone. Displays Plotly figures.\nRaises:\n\nValueError - If data or data_original is empty, or if specified columns don’t exist\nImportError - If plotly is not installed\n\nExample:\nimport pandas as pd\nimport numpy as np\nfrom spotforecast2.preprocessing.outlier_plots import visualize_outliers_plotly_scatter\n\n# Create time series data\nnp.random.seed(42)\ndates = pd.date_range('2024-01-01', periods=103, freq='h')\ndata_original = pd.DataFrame({\n    'temperature': np.concatenate([\n        np.random.normal(20, 5, 100),\n        [50, 60, 70]  # outliers\n    ]),\n    'humidity': np.concatenate([\n        np.random.normal(60, 10, 100),\n        [95, 98, 99]  # outliers\n    ])\n}, index=dates)\n\ndata_cleaned = data_original.copy()\n\n# Visualize outliers\nvisualize_outliers_plotly_scatter(\n    data_cleaned,\n    data_original,\n    contamination=0.03,\n    template='plotly_white',\n    height=600\n)",
    "crumbs": [
      "Preprocessing Guides",
      "Outlier Detection"
    ]
  },
  {
    "objectID": "docs/preprocessing/outliers.html#complete-workflow-example",
    "href": "docs/preprocessing/outliers.html#complete-workflow-example",
    "title": "Outlier Detection and Visualization",
    "section": "",
    "text": "Here’s a complete example showing the typical workflow for outlier detection and visualization:\nimport pandas as pd\nimport numpy as np\nfrom spotforecast2_safe.preprocessing.outlier import get_outliers\nfrom spotforecast2.preprocessing.outlier_plots import (\n    visualize_outliers_hist,\n    visualize_outliers_plotly_scatter)\n\n# Create realistic time series data with outliers\nnp.random.seed(42)\ndates = pd.date_range('2024-01-01', periods=200, freq='h')\ndata_original = pd.DataFrame({\n    'temperature': np.concatenate([\n        np.random.normal(20, 5, 197),\n        [50, 60, 70]  # outliers\n    ]),\n    'humidity': np.concatenate([\n        np.random.normal(60, 10, 197),\n        [95, 98, 99]  # outliers\n    ]),\n    'pressure': np.concatenate([\n        np.random.normal(1013, 10, 197),\n        [800, 1200, 950]  # outliers\n    ])\n}, index=dates)\n\n# Make a copy for cleaning\ndata_cleaned = data_original.copy()\n\n# Step 1: Detect outliers\nprint(\"=== Outlier Detection ===\")\noutliers = get_outliers(\n    data_original,\n    contamination=0.015\n)\n\nfor col, outlier_vals in outliers.items():\n    pct = (len(outlier_vals) / len(data_original)) * 100\n    print(f\"{col}: {len(outlier_vals)} outliers ({pct:.2f}%)\")\n\n# Step 2: Visualize with histograms\nprint(\"\\n=== Histogram Visualization ===\")\nvisualize_outliers_hist(\n    data_cleaned,\n    data_original,\n    contamination=0.015,\n    figsize=(14, 4),\n    alpha=0.7\n)\n\n# Step 3: Visualize with Plotly (interactive)\nprint(\"\\n=== Interactive Plotly Visualization ===\")\nvisualize_outliers_plotly_scatter(\n    data_cleaned,\n    data_original,\n    contamination=0.015,\n    template='plotly_white'\n)\n\n# Step 4: Selective column visualization\nprint(\"\\n=== Selective Column Analysis ===\")\nvisualize_outliers_hist(\n    data_cleaned,\n    data_original,\n    columns=['temperature', 'humidity'],\n    contamination=0.015\n)",
    "crumbs": [
      "Preprocessing Guides",
      "Outlier Detection"
    ]
  },
  {
    "objectID": "docs/preprocessing/outliers.html#parameters-and-configuration",
    "href": "docs/preprocessing/outliers.html#parameters-and-configuration",
    "title": "Outlier Detection and Visualization",
    "section": "",
    "text": "The contamination parameter controls the expected proportion of outliers in the dataset:\n\n0.01 (1%) - Conservative, detects severe outliers only\n0.02 (2%) - Moderate, typical for most applications\n0.05 (5%) - Liberal, detects more potential anomalies\n\nChoose based on your domain knowledge and data characteristics.\n\n\n\nThe random_state parameter ensures reproducibility:\n# Same random_state produces consistent results\noutliers1 = get_outliers(data, random_state=42)\noutliers2 = get_outliers(data, random_state=42)\n# outliers1 == outliers2\n\n\n\nWhen using visualize_outliers_hist(), you can pass additional matplotlib histogram options:\nvisualize_outliers_hist(\n    data_cleaned,\n    data_original,\n    bins=100,           # More granular bins\n    alpha=0.5,          # Transparency\n    edgecolor='black',  # Border around bars\n    linewidth=0.5       # Border thickness\n)\n\n\n\nWhen using visualize_outliers_plotly_scatter(), you can customize the Plotly figure:\nvisualize_outliers_plotly_scatter(\n    data_cleaned,\n    data_original,\n    template='plotly_dark',    # Dark theme\n    height=700,                 # Figure height\n    width=1200                  # Figure width\n)",
    "crumbs": [
      "Preprocessing Guides",
      "Outlier Detection"
    ]
  },
  {
    "objectID": "docs/preprocessing/outliers.html#algorithm-details",
    "href": "docs/preprocessing/outliers.html#algorithm-details",
    "title": "Outlier Detection and Visualization",
    "section": "",
    "text": "The underlying algorithm uses scikit-learn’s IsolationForest, which:\n\nRandomly selects features and split values\nIsolates anomalies by exploiting their rarity\nAssigns anomaly scores based on path lengths\nMarks points with scores exceeding the contamination threshold as outliers\n\nKey characteristics:\n\nNo distance computation needed (efficient for high dimensions)\nScales well with number of features\nRobust to varying scales\nNo hyperparameter tuning required beyond contamination",
    "crumbs": [
      "Preprocessing Guides",
      "Outlier Detection"
    ]
  },
  {
    "objectID": "docs/preprocessing/outliers.html#best-practices",
    "href": "docs/preprocessing/outliers.html#best-practices",
    "title": "Outlier Detection and Visualization",
    "section": "",
    "text": "Clean your data before outlier detection:\n# Remove missing values\ndata_clean = data.dropna()\n\n# Standardize if needed\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndata_scaled = pd.DataFrame(\n    scaler.fit_transform(data),\n    index=data.index,\n    columns=data.columns\n)\n\n\n\nEstimate contamination based on domain knowledge:\n# For known outlier percentage\ncontamination = n_outliers / len(data)\n\n# For exploratory analysis, try multiple values\nfor cont in [0.01, 0.02, 0.05]:\n    outliers = get_outliers(data, contamination=cont)\n    print(f\"Contamination {cont}: {len(outliers)} outliers\")\n\n\n\nAlways visualize results:\n# Histogram for distribution analysis\nvisualize_outliers_hist(data_cleaned, data_original)\n\n# Time series plot for temporal patterns\nvisualize_outliers_plotly_scatter(data_cleaned, data_original)\n\n\n\nVerify outliers make sense in context:\noutliers = get_outliers(data, contamination=0.02)\n\nfor col, vals in outliers.items():\n    print(f\"\\n{col}:\")\n    print(f\"  Regular range: {data[col].min():.2f} - {data[col].max():.2f}\")\n    print(f\"  Outlier values: {sorted(vals.unique())}\")\n    print(f\"  Outlier indices: {list(vals.index)}\")",
    "crumbs": [
      "Preprocessing Guides",
      "Outlier Detection"
    ]
  },
  {
    "objectID": "docs/preprocessing/outliers.html#testing",
    "href": "docs/preprocessing/outliers.html#testing",
    "title": "Outlier Detection and Visualization",
    "section": "",
    "text": "All examples in this guide are validated by tests/test_docs_outliers_examples.py with 43 comprehensive pytest cases covering:\n\nBasic outlier detection functionality\nContamination parameter variations (0.01, 0.02, 0.05)\nRandom state reproducibility\nData integrity and value validation\nComplete workflow integration\nEdge cases (small/large datasets, extreme values, NaN handling)\nTimeseries data with DatetimeIndex\nAPI examples and return types\nSafety-critical behavior validation\n\nRun the tests:\n# Run outliers documentation tests\nuv run pytest tests/test_docs_outliers_examples.py -v\n\n# Quick check\nuv run pytest tests/test_docs_outliers_examples.py --tb=no -q",
    "crumbs": [
      "Preprocessing Guides",
      "Outlier Detection"
    ]
  },
  {
    "objectID": "docs/preprocessing/outliers.html#troubleshooting",
    "href": "docs/preprocessing/outliers.html#troubleshooting",
    "title": "Outlier Detection and Visualization",
    "section": "",
    "text": "Solution: Increase the contamination parameter:\n# Try higher contamination\noutliers = get_outliers(data, contamination=0.05)\n\n\n\nSolution: Decrease the contamination parameter:\n# Be more conservative\noutliers = get_outliers(data, contamination=0.01)\n\n\n\nSolution: Install plotly:\npip install plotly\nOr use histogram visualization instead:\nvisualize_outliers_hist(data_cleaned, data_original)",
    "crumbs": [
      "Preprocessing Guides",
      "Outlier Detection"
    ]
  },
  {
    "objectID": "docs/preprocessing/outliers.html#see-also",
    "href": "docs/preprocessing/outliers.html#see-also",
    "title": "Outlier Detection and Visualization",
    "section": "",
    "text": "Isolation Forest Algorithm",
    "crumbs": [
      "Preprocessing Guides",
      "Outlier Detection"
    ]
  },
  {
    "objectID": "docs/processing/model_persistence.html",
    "href": "docs/processing/model_persistence.html",
    "title": "Model Persistence Guide",
    "section": "",
    "text": "This guide explains how to use the model persistence feature in spotforecast2, which provides scikit-learn-style caching of trained forecasters to disk.\nKey Feature: Model persistence is fully enabled with support for sample weight functions, providing significant speedup for repeated predictions!\n\n\n\nNo additional installation needed! The implementation uses joblib (already in requirements) and the built-in WeightFunction class.\n\n\n\n\n\nfrom spotforecast2_safe.processing.n2n_predict_with_covariates import n2n_predict_with_covariates\n\n# Models are trained and cached automatically\npredictions, metadata, forecasters = n2n_predict_with_covariates(\n    forecast_horizon=24,\n    verbose=True  # Shows: \"Training X forecasters...\" and \"Saving X trained forecasters...\"\n)\n\n\n\n# Models are loaded from cache (much faster!)\npredictions, metadata, forecasters = n2n_predict_with_covariates(\n    forecast_horizon=24,\n    verbose=True  # Shows: \"All X forecasters loaded from cache\"\n)\n\n\n\n# Force retraining - ignore cache, retrain all models\npredictions, metadata, forecasters = n2n_predict_with_covariates(\n    forecast_horizon=24,\n    force_train=True,  # Ignore cache, retrain all\n    verbose=True\n)\n\n\n\n# Use custom directory for models\npredictions, metadata, forecasters = n2n_predict_with_covariates(\n    forecast_horizon=24,\n    model_dir=\"/path/to/models\",  # Default: None (uses ~/spotforecast2_cache/forecasters)\n    verbose=True\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nType\nDefault\nDescription\n\n\n\n\nforce_train\nbool\nTrue\nForce retraining, ignore cache\n\n\nmodel_dir\nstr/Path\nNone\nCache directory location. If None, uses get_cache_home()/forecasters\n\n\n\n\n\n\nDefault Cache Directory: - Location: ~/spotforecast2_cache/forecasters/ - Environment Variable: SPOTFORECAST2_CACHE (overrides default directory) - Models are stored in the format: model_dir/forecaster_{target_name}.joblib\n\n\n\n\n\n[8/9] Loading or training recursive forecasters with exogenous variables...\n  Attempting to load cached models...\n  ✓ Loaded forecaster for power from ./forecaster_models/forecaster_power.joblib\n  ✓ Loaded forecaster for energy from ./forecaster_models/forecaster_energy.joblib\n  ...\n  ✓ All 10 forecasters loaded from cache\n\n\n\n[8/9] Loading or training recursive forecasters with exogenous variables...\n  Attempting to load cached models...\n  ✓ Loaded forecaster for power from ./forecaster_models/forecaster_power.joblib\n  ✓ Loaded 1 forecasters, will train 1 new ones\n  Training forecaster for energy...\n    ✓ Forecaster trained for energy\n  Saving 1 trained forecasters to disk...\n  ✓ Saved forecaster for energy to ./forecaster_models/forecaster_energy.joblib\n  ✓ Total forecasters available: 2\n\n\n\n[8/9] Loading or training recursive forecasters with exogenous variables...\n  Force retraining all 2 forecasters...\n  Training forecaster for power...\n    ✓ Forecaster trained for power\n  Training forecaster for energy...\n    ✓ Forecaster trained for energy\n  Saving 2 trained forecasters to disk...\n  ✓ Saved forecaster for power to ./forecaster_models/forecaster_power.joblib\n  ✓ Saved forecaster for energy to ./forecaster_models/forecaster_energy.joblib\n  ✓ Total forecasters available: 2\n\n\n\n\n\n\nThe WeightFunction class enables model persistence with sample weights:\nfrom spotforecast2.preprocessing import WeightFunction\n\n# Create picklable weight function\nweights_series = pd.Series([1.0, 0.9, 0.8], index=[0, 1, 2])\nweight_func = WeightFunction(weights_series)\n\n# Use with forecaster - automatically persisted to disk!\nforecaster = ForecasterRecursive(\n    estimator=estimator,\n    weight_func=weight_func\n)\nCalling WeightFunction:\nimport pandas as pd\nfrom spotforecast2.preprocessing import WeightFunction\n\nweights = pd.Series([1.0, 0.9, 0.8], index=[0, 1, 2])\nweight_func = WeightFunction(weights)\n\n# For single index value\nweight = weight_func(0)  # Returns: 1.0\n\n# For multiple index values\nweights = weight_func(pd.Index([0, 1, 2]))  # Returns: array([1.0, 0.9, 0.8])\nBenefits: - ✅ Fully picklable (works with joblib) - ✅ No external dependencies - ✅ No closure limitations - ✅ Follows sklearn conventions\nThis approach ensures all trained models with sample weights can be persisted to disk without any external dependencies.\n\n\n\nThe system implements intelligent selective retraining:\n\nCache Lookup (if force_train=False)\n\nCheck if model cache directory exists\nAttempt to load all target models from disk\nIdentify which targets are missing\n\nSelective Training\n\nTrain only missing models (not cached)\nKeep loaded models in memory\nSaves significant computation time\n\nAuto-Save\n\nNewly trained models automatically saved to disk\nMaintains cache consistency\nNo manual save required\n\nForce Retraining (if force_train=True)\n\nClears cache directory\nTrains all models from scratch\nUseful for model updates or validation\n\n\n\n\n\n\n\n\nFor advanced use cases, you can directly use the persistence helper functions:\nfrom spotforecast2_safe.processing.n2n_predict_with_covariates import (\n    _ensure_model_dir,\n    _get_model_filepath,\n    _save_forecasters,\n    _load_forecasters,\n    _model_directory_exists\n)\n\n# Create/ensure model directory exists\nmodel_dir = _ensure_model_dir(\"./my_models\")\n\n# Get path for a specific model\npath = _get_model_filepath(model_dir, \"power\")\n# Returns: my_models/forecaster_power.joblib\n\n# Load cached models\nforecasters, missing = _load_forecasters(\n    [\"power\", \"energy\", \"temperature\"],\n    model_dir,\n    verbose=True\n)\n# Returns: (loaded_forecasters_dict, missing_targets_list)\n\n# Save models to disk\nsaved_paths = _save_forecasters(\n    {\"power\": forecaster_obj, \"energy\": forecaster_obj},\n    model_dir,\n    verbose=True\n)\n\n# Check if cache directory exists\nif _model_directory_exists(model_dir):\n    print(\"Cache directory found\")\n\n\n\nimport os\nfrom spotforecast2_safe.data import get_cache_home\n\n# Get default cache location\ncache_dir = get_cache_home()\n\n# Or set environment variable\nos.environ['SPOTFORECAST2_CACHE'] = '/custom/cache/path'\ncache_dir = get_cache_home()  # Now uses custom path\n\n# Use in forecasting\npredictions, metadata, forecasters = n2n_predict_with_covariates(\n    forecast_horizon=24,\n    model_dir=str(cache_dir / \"forecasters\"),\n    verbose=True\n)\n\n\n\n\nCore Implementation: - src/spotforecast2/processing/n2n_predict_with_covariates.py - Main forecasting function with persistence - src/spotforecast2/preprocessing/imputation.py - WeightFunction class - src/spotforecast2/utils/forecaster_config.py - Weight function initialization\nTest Files: - tests/test_model_persistence.py (35 unit tests) - tests/test_n2n_persistence_integration.py (12 integration tests) - tests/test_weight_function_pickle.py (6 pickling tests) - tests/test_cache_home.py (14 cache home tests)\n\n\n\n# Run persistence tests\nuv run pytest tests/test_model_persistence.py -v\n\n# Run documentation example tests\nuv run pytest tests/test_docs_model_persistence_examples.py -v\n\n# Run integration tests\nuv run pytest tests/test_n2n_persistence_integration.py -v\n\n# Run weight function pickling tests\nuv run pytest tests/test_weight_function_pickle.py -v\n\n# Run all persistence-related tests\nuv run pytest tests/test_model_persistence.py tests/test_docs_model_persistence_examples.py tests/test_n2n_persistence_integration.py tests/test_weight_function_pickle.py -v\n\n# Quick check (all tests should pass)\nuv run pytest tests/test_model_persistence.py tests/test_docs_model_persistence_examples.py tests/test_n2n_persistence_integration.py tests/test_weight_function_pickle.py --tb=no -q\nDocumentation validation: All examples in this guide are validated by tests/test_docs_model_persistence_examples.py with 43 comprehensive pytest cases.\n\n\n\n\n\nA: Check that the model_dir path is correct and accessible:\n# Verify models exist in the directory\nls ./forecaster_models/\n\n# Check file permissions\nls -la ./forecaster_models/\nUse force_train=True to rebuild the cache if needed:\npredictions, metadata, forecasters = n2n_predict_with_covariates(\n    forecast_horizon=24,\n    force_train=True,  # Rebuild cache\n    model_dir=\"./forecaster_models\",\n    verbose=True\n)\n\n\n\nA: Training takes 5-10 minutes depending on data size and number of models. This is normal - models are then cached for fast reuse. Subsequent runs will be 1-2 seconds.\n\n\n\nA: Delete the model directory:\nrm -rf ./forecaster_models/\nOr set force_train=True to rebuild:\npredictions, metadata, forecasters = n2n_predict_with_covariates(\n    forecast_horizon=24,\n    force_train=True,\n    model_dir=\"./forecaster_models\",\n    verbose=True\n)\n\n\n\nA: Each model is ~1-5 MB compressed with joblib. You can: - Delete model_dir to free space: rm -rf ./forecaster_models/ - Use a different location: Set model_dir to a location with more space - Set force_train=True to rebuild only if needed\n\n\n\nA: Custom estimators work with persistence as long as they’re pickle-compatible (most scikit-learn compatible estimators are):\nfrom lightgbm import LGBMRegressor\n\ncustom_estimator = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=7,\n    random_state=42\n)\n\npredictions, metadata, forecasters = n2n_predict_with_covariates(\n    forecast_horizon=24,\n    estimator=custom_estimator,\n    force_train=False,  # Use cache if available\n    model_dir=\"./models\",\n    verbose=True\n)\n\n\n\n\n\n\nThe model persistence feature uses joblib for serialization, following scikit-learn conventions: - Format: Binary compressed files with .joblib extension - Compression: joblib compress=3 (good balance of speed and size) - Location: Configurable directory (default: ./forecaster_models/) - Naming: forecaster_{target_name}.joblib\n\n\nThe implementation uses a WeightFunction class to ensure sample weights can be pickled. This solves a common problem where local functions with closures cannot be serialized:\nfrom spotforecast2.preprocessing import WeightFunction\nimport pandas as pd\n\n# Weights created from missing data analysis\nweights = pd.Series([1.0, 0.9, 0.8], index=[0, 1, 2])\n\n# Wrap in WeightFunction (picklable, unlike local functions!)\nweight_func = WeightFunction(weights)\n\n# Can be pickled and saved to disk\nimport pickle\npickled = pickle.dumps(weight_func)\n\n# Use with ForecasterRecursive\nforecaster = ForecasterRecursive(\n    estimator=estimator,\n    lags=24,\n    weight_func=weight_func  # Fully picklable!\n)\nThis approach ensures all trained models with sample weights can be persisted to disk without any external dependencies.\n\n\n\n\nThe system implements intelligent selective retraining:\n\nCache Lookup (if force_train=False)\n\nCheck if model cache directory exists\nAttempt to load all target models from disk\nIdentify which targets are missing\n\nSelective Training\n\nTrain only missing models (not cached)\nKeep loaded models in memory\nSaves significant computation time\n\nAuto-Save\n\nNewly trained models automatically saved to disk\nMaintains cache consistency\nNo manual save required\n\nForce Retraining (if force_train=True)\n\nClears cache directory\nTrains all models from scratch\nUseful for model updates or validation\n\n\n\n\n\n✅ Backward Compatible - All new parameters have defaults ✅ Drop-in Replacement - Works with existing code ✅ No Breaking Changes - Safe to upgrade\n\n\n\n\n\nAPI Reference - Forecasting\nAPI Reference - Data\nPreprocessing Guide",
    "crumbs": [
      "Processing Guides",
      "Model Persistence"
    ]
  },
  {
    "objectID": "docs/processing/model_persistence.html#overview",
    "href": "docs/processing/model_persistence.html#overview",
    "title": "Model Persistence Guide",
    "section": "",
    "text": "This guide explains how to use the model persistence feature in spotforecast2, which provides scikit-learn-style caching of trained forecasters to disk.\nKey Feature: Model persistence is fully enabled with support for sample weight functions, providing significant speedup for repeated predictions!",
    "crumbs": [
      "Processing Guides",
      "Model Persistence"
    ]
  },
  {
    "objectID": "docs/processing/model_persistence.html#installation-setup",
    "href": "docs/processing/model_persistence.html#installation-setup",
    "title": "Model Persistence Guide",
    "section": "",
    "text": "No additional installation needed! The implementation uses joblib (already in requirements) and the built-in WeightFunction class.",
    "crumbs": [
      "Processing Guides",
      "Model Persistence"
    ]
  },
  {
    "objectID": "docs/processing/model_persistence.html#quick-start",
    "href": "docs/processing/model_persistence.html#quick-start",
    "title": "Model Persistence Guide",
    "section": "",
    "text": "from spotforecast2_safe.processing.n2n_predict_with_covariates import n2n_predict_with_covariates\n\n# Models are trained and cached automatically\npredictions, metadata, forecasters = n2n_predict_with_covariates(\n    forecast_horizon=24,\n    verbose=True  # Shows: \"Training X forecasters...\" and \"Saving X trained forecasters...\"\n)\n\n\n\n# Models are loaded from cache (much faster!)\npredictions, metadata, forecasters = n2n_predict_with_covariates(\n    forecast_horizon=24,\n    verbose=True  # Shows: \"All X forecasters loaded from cache\"\n)\n\n\n\n# Force retraining - ignore cache, retrain all models\npredictions, metadata, forecasters = n2n_predict_with_covariates(\n    forecast_horizon=24,\n    force_train=True,  # Ignore cache, retrain all\n    verbose=True\n)\n\n\n\n# Use custom directory for models\npredictions, metadata, forecasters = n2n_predict_with_covariates(\n    forecast_horizon=24,\n    model_dir=\"/path/to/models\",  # Default: None (uses ~/spotforecast2_cache/forecasters)\n    verbose=True\n)",
    "crumbs": [
      "Processing Guides",
      "Model Persistence"
    ]
  },
  {
    "objectID": "docs/processing/model_persistence.html#parameters",
    "href": "docs/processing/model_persistence.html#parameters",
    "title": "Model Persistence Guide",
    "section": "",
    "text": "Parameter\nType\nDefault\nDescription\n\n\n\n\nforce_train\nbool\nTrue\nForce retraining, ignore cache\n\n\nmodel_dir\nstr/Path\nNone\nCache directory location. If None, uses get_cache_home()/forecasters",
    "crumbs": [
      "Processing Guides",
      "Model Persistence"
    ]
  },
  {
    "objectID": "docs/processing/model_persistence.html#performance",
    "href": "docs/processing/model_persistence.html#performance",
    "title": "Model Persistence Guide",
    "section": "",
    "text": "Default Cache Directory: - Location: ~/spotforecast2_cache/forecasters/ - Environment Variable: SPOTFORECAST2_CACHE (overrides default directory) - Models are stored in the format: model_dir/forecaster_{target_name}.joblib",
    "crumbs": [
      "Processing Guides",
      "Model Persistence"
    ]
  },
  {
    "objectID": "docs/processing/model_persistence.html#verbose-output-examples",
    "href": "docs/processing/model_persistence.html#verbose-output-examples",
    "title": "Model Persistence Guide",
    "section": "",
    "text": "[8/9] Loading or training recursive forecasters with exogenous variables...\n  Attempting to load cached models...\n  ✓ Loaded forecaster for power from ./forecaster_models/forecaster_power.joblib\n  ✓ Loaded forecaster for energy from ./forecaster_models/forecaster_energy.joblib\n  ...\n  ✓ All 10 forecasters loaded from cache\n\n\n\n[8/9] Loading or training recursive forecasters with exogenous variables...\n  Attempting to load cached models...\n  ✓ Loaded forecaster for power from ./forecaster_models/forecaster_power.joblib\n  ✓ Loaded 1 forecasters, will train 1 new ones\n  Training forecaster for energy...\n    ✓ Forecaster trained for energy\n  Saving 1 trained forecasters to disk...\n  ✓ Saved forecaster for energy to ./forecaster_models/forecaster_energy.joblib\n  ✓ Total forecasters available: 2\n\n\n\n[8/9] Loading or training recursive forecasters with exogenous variables...\n  Force retraining all 2 forecasters...\n  Training forecaster for power...\n    ✓ Forecaster trained for power\n  Training forecaster for energy...\n    ✓ Forecaster trained for energy\n  Saving 2 trained forecasters to disk...\n  ✓ Saved forecaster for power to ./forecaster_models/forecaster_power.joblib\n  ✓ Saved forecaster for energy to ./forecaster_models/forecaster_energy.joblib\n  ✓ Total forecasters available: 2",
    "crumbs": [
      "Processing Guides",
      "Model Persistence"
    ]
  },
  {
    "objectID": "docs/processing/model_persistence.html#key-implementation-details",
    "href": "docs/processing/model_persistence.html#key-implementation-details",
    "title": "Model Persistence Guide",
    "section": "",
    "text": "The WeightFunction class enables model persistence with sample weights:\nfrom spotforecast2.preprocessing import WeightFunction\n\n# Create picklable weight function\nweights_series = pd.Series([1.0, 0.9, 0.8], index=[0, 1, 2])\nweight_func = WeightFunction(weights_series)\n\n# Use with forecaster - automatically persisted to disk!\nforecaster = ForecasterRecursive(\n    estimator=estimator,\n    weight_func=weight_func\n)\nCalling WeightFunction:\nimport pandas as pd\nfrom spotforecast2.preprocessing import WeightFunction\n\nweights = pd.Series([1.0, 0.9, 0.8], index=[0, 1, 2])\nweight_func = WeightFunction(weights)\n\n# For single index value\nweight = weight_func(0)  # Returns: 1.0\n\n# For multiple index values\nweights = weight_func(pd.Index([0, 1, 2]))  # Returns: array([1.0, 0.9, 0.8])\nBenefits: - ✅ Fully picklable (works with joblib) - ✅ No external dependencies - ✅ No closure limitations - ✅ Follows sklearn conventions\nThis approach ensures all trained models with sample weights can be persisted to disk without any external dependencies.\n\n\n\nThe system implements intelligent selective retraining:\n\nCache Lookup (if force_train=False)\n\nCheck if model cache directory exists\nAttempt to load all target models from disk\nIdentify which targets are missing\n\nSelective Training\n\nTrain only missing models (not cached)\nKeep loaded models in memory\nSaves significant computation time\n\nAuto-Save\n\nNewly trained models automatically saved to disk\nMaintains cache consistency\nNo manual save required\n\nForce Retraining (if force_train=True)\n\nClears cache directory\nTrains all models from scratch\nUseful for model updates or validation",
    "crumbs": [
      "Processing Guides",
      "Model Persistence"
    ]
  },
  {
    "objectID": "docs/processing/model_persistence.html#working-with-models",
    "href": "docs/processing/model_persistence.html#working-with-models",
    "title": "Model Persistence Guide",
    "section": "",
    "text": "For advanced use cases, you can directly use the persistence helper functions:\nfrom spotforecast2_safe.processing.n2n_predict_with_covariates import (\n    _ensure_model_dir,\n    _get_model_filepath,\n    _save_forecasters,\n    _load_forecasters,\n    _model_directory_exists\n)\n\n# Create/ensure model directory exists\nmodel_dir = _ensure_model_dir(\"./my_models\")\n\n# Get path for a specific model\npath = _get_model_filepath(model_dir, \"power\")\n# Returns: my_models/forecaster_power.joblib\n\n# Load cached models\nforecasters, missing = _load_forecasters(\n    [\"power\", \"energy\", \"temperature\"],\n    model_dir,\n    verbose=True\n)\n# Returns: (loaded_forecasters_dict, missing_targets_list)\n\n# Save models to disk\nsaved_paths = _save_forecasters(\n    {\"power\": forecaster_obj, \"energy\": forecaster_obj},\n    model_dir,\n    verbose=True\n)\n\n# Check if cache directory exists\nif _model_directory_exists(model_dir):\n    print(\"Cache directory found\")\n\n\n\nimport os\nfrom spotforecast2_safe.data import get_cache_home\n\n# Get default cache location\ncache_dir = get_cache_home()\n\n# Or set environment variable\nos.environ['SPOTFORECAST2_CACHE'] = '/custom/cache/path'\ncache_dir = get_cache_home()  # Now uses custom path\n\n# Use in forecasting\npredictions, metadata, forecasters = n2n_predict_with_covariates(\n    forecast_horizon=24,\n    model_dir=str(cache_dir / \"forecasters\"),\n    verbose=True\n)",
    "crumbs": [
      "Processing Guides",
      "Model Persistence"
    ]
  },
  {
    "objectID": "docs/processing/model_persistence.html#implementation-files",
    "href": "docs/processing/model_persistence.html#implementation-files",
    "title": "Model Persistence Guide",
    "section": "",
    "text": "Core Implementation: - src/spotforecast2/processing/n2n_predict_with_covariates.py - Main forecasting function with persistence - src/spotforecast2/preprocessing/imputation.py - WeightFunction class - src/spotforecast2/utils/forecaster_config.py - Weight function initialization\nTest Files: - tests/test_model_persistence.py (35 unit tests) - tests/test_n2n_persistence_integration.py (12 integration tests) - tests/test_weight_function_pickle.py (6 pickling tests) - tests/test_cache_home.py (14 cache home tests)",
    "crumbs": [
      "Processing Guides",
      "Model Persistence"
    ]
  },
  {
    "objectID": "docs/processing/model_persistence.html#testing",
    "href": "docs/processing/model_persistence.html#testing",
    "title": "Model Persistence Guide",
    "section": "",
    "text": "# Run persistence tests\nuv run pytest tests/test_model_persistence.py -v\n\n# Run documentation example tests\nuv run pytest tests/test_docs_model_persistence_examples.py -v\n\n# Run integration tests\nuv run pytest tests/test_n2n_persistence_integration.py -v\n\n# Run weight function pickling tests\nuv run pytest tests/test_weight_function_pickle.py -v\n\n# Run all persistence-related tests\nuv run pytest tests/test_model_persistence.py tests/test_docs_model_persistence_examples.py tests/test_n2n_persistence_integration.py tests/test_weight_function_pickle.py -v\n\n# Quick check (all tests should pass)\nuv run pytest tests/test_model_persistence.py tests/test_docs_model_persistence_examples.py tests/test_n2n_persistence_integration.py tests/test_weight_function_pickle.py --tb=no -q\nDocumentation validation: All examples in this guide are validated by tests/test_docs_model_persistence_examples.py with 43 comprehensive pytest cases.",
    "crumbs": [
      "Processing Guides",
      "Model Persistence"
    ]
  },
  {
    "objectID": "docs/processing/model_persistence.html#troubleshooting",
    "href": "docs/processing/model_persistence.html#troubleshooting",
    "title": "Model Persistence Guide",
    "section": "",
    "text": "A: Check that the model_dir path is correct and accessible:\n# Verify models exist in the directory\nls ./forecaster_models/\n\n# Check file permissions\nls -la ./forecaster_models/\nUse force_train=True to rebuild the cache if needed:\npredictions, metadata, forecasters = n2n_predict_with_covariates(\n    forecast_horizon=24,\n    force_train=True,  # Rebuild cache\n    model_dir=\"./forecaster_models\",\n    verbose=True\n)\n\n\n\nA: Training takes 5-10 minutes depending on data size and number of models. This is normal - models are then cached for fast reuse. Subsequent runs will be 1-2 seconds.\n\n\n\nA: Delete the model directory:\nrm -rf ./forecaster_models/\nOr set force_train=True to rebuild:\npredictions, metadata, forecasters = n2n_predict_with_covariates(\n    forecast_horizon=24,\n    force_train=True,\n    model_dir=\"./forecaster_models\",\n    verbose=True\n)\n\n\n\nA: Each model is ~1-5 MB compressed with joblib. You can: - Delete model_dir to free space: rm -rf ./forecaster_models/ - Use a different location: Set model_dir to a location with more space - Set force_train=True to rebuild only if needed\n\n\n\nA: Custom estimators work with persistence as long as they’re pickle-compatible (most scikit-learn compatible estimators are):\nfrom lightgbm import LGBMRegressor\n\ncustom_estimator = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=7,\n    random_state=42\n)\n\npredictions, metadata, forecasters = n2n_predict_with_covariates(\n    forecast_horizon=24,\n    estimator=custom_estimator,\n    force_train=False,  # Use cache if available\n    model_dir=\"./models\",\n    verbose=True\n)",
    "crumbs": [
      "Processing Guides",
      "Model Persistence"
    ]
  },
  {
    "objectID": "docs/processing/model_persistence.html#technical-details",
    "href": "docs/processing/model_persistence.html#technical-details",
    "title": "Model Persistence Guide",
    "section": "",
    "text": "The model persistence feature uses joblib for serialization, following scikit-learn conventions: - Format: Binary compressed files with .joblib extension - Compression: joblib compress=3 (good balance of speed and size) - Location: Configurable directory (default: ./forecaster_models/) - Naming: forecaster_{target_name}.joblib\n\n\nThe implementation uses a WeightFunction class to ensure sample weights can be pickled. This solves a common problem where local functions with closures cannot be serialized:\nfrom spotforecast2.preprocessing import WeightFunction\nimport pandas as pd\n\n# Weights created from missing data analysis\nweights = pd.Series([1.0, 0.9, 0.8], index=[0, 1, 2])\n\n# Wrap in WeightFunction (picklable, unlike local functions!)\nweight_func = WeightFunction(weights)\n\n# Can be pickled and saved to disk\nimport pickle\npickled = pickle.dumps(weight_func)\n\n# Use with ForecasterRecursive\nforecaster = ForecasterRecursive(\n    estimator=estimator,\n    lags=24,\n    weight_func=weight_func  # Fully picklable!\n)\nThis approach ensures all trained models with sample weights can be persisted to disk without any external dependencies.\n\n\n\n\nThe system implements intelligent selective retraining:\n\nCache Lookup (if force_train=False)\n\nCheck if model cache directory exists\nAttempt to load all target models from disk\nIdentify which targets are missing\n\nSelective Training\n\nTrain only missing models (not cached)\nKeep loaded models in memory\nSaves significant computation time\n\nAuto-Save\n\nNewly trained models automatically saved to disk\nMaintains cache consistency\nNo manual save required\n\nForce Retraining (if force_train=True)\n\nClears cache directory\nTrains all models from scratch\nUseful for model updates or validation\n\n\n\n\n\n✅ Backward Compatible - All new parameters have defaults ✅ Drop-in Replacement - Works with existing code ✅ No Breaking Changes - Safe to upgrade",
    "crumbs": [
      "Processing Guides",
      "Model Persistence"
    ]
  },
  {
    "objectID": "docs/processing/model_persistence.html#see-also",
    "href": "docs/processing/model_persistence.html#see-also",
    "title": "Model Persistence Guide",
    "section": "",
    "text": "API Reference - Forecasting\nAPI Reference - Data\nPreprocessing Guide",
    "crumbs": [
      "Processing Guides",
      "Model Persistence"
    ]
  }
]