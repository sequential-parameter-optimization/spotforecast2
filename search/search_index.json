{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to spotforecast2","text":"<p>spotforecast2 is a Python package for forecasting, combining the power of <code>sklearn</code>, <code>spotoptim</code>and <code>skforecast</code> with specialized utilities for \"spot\" forecasting.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>\ud83d\udce6 GitHub Repository</li> <li>\ud83d\udcda API Reference</li> <li>\ud83d\ude80 Current Version: 0.2.35</li> </ul>"},{"location":"#installation","title":"Installation","text":"<ul> <li>Download from GitHub</li> </ul> <pre><code>git clone https://github.com/sequential-parameter-optimization/spotforecast2.git\ncd spotforecast2\n</code></pre> <ul> <li>Sync using uv <pre><code>uv sync\n</code></pre></li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>Data Fetching: Easy access to time series data.</li> <li>Preprocessing: Robust tools for curating, cleaning, and splitting data.</li> <li>Forecasting: A rich set of forecasting strategies (constantly extended).</li> <li>Model Selection: <code>spotoptim</code> and <code>optuna</code> search for hyperparameter tuning.</li> <li>Weather Integration: Utilities for fetching and using weather data in forecasts.</li> </ul>"},{"location":"#attributions","title":"Attributions","text":"<p>Parts of the code are ported from skforecast to reduce external dependencies. Many thanks to the skforecast team for their great work!</p>"},{"location":"entsoe/","title":"ENTSO-E Energy Forecasting Guide","text":"<p>This guide provides comprehensive examples for using <code>spotforecast2</code> with ENTSO-E energy data. Examples are organized from beginner to advanced, with each code snippet backed by automated tests.</p>"},{"location":"entsoe/#prerequisites","title":"Prerequisites","text":"<p>Before running these examples, ensure you have:</p> <ol> <li><code>spotforecast2</code> installed: <code>pip install spotforecast2</code></li> <li>An ENTSO-E API key (optional for training examples)</li> </ol>"},{"location":"entsoe/#configuration","title":"Configuration","text":""},{"location":"entsoe/#default-configuration","title":"Default Configuration","text":"<p>The simplest way to get started is using the default configuration:</p> <pre><code>from spotforecast2 import Config\n\nconfig = Config()\nprint(config.API_COUNTRY_CODE)  # 'DE'\nprint(config.predict_size)      # 24\nprint(config.random_state)      # 314159\n</code></pre>"},{"location":"entsoe/#custom-configuration","title":"Custom Configuration","text":"<p>Customize parameters for your specific use case:</p> <pre><code>from spotforecast2 import Config\nimport pandas as pd\n\nconfig = Config(\n    api_country_code='FR',\n    predict_size=48,\n    refit_size=14,\n    random_state=42\n)\nprint(config.API_COUNTRY_CODE)  # 'FR'\nprint(config.predict_size)      # 48\n</code></pre>"},{"location":"entsoe/#configuration-parameters","title":"Configuration Parameters","text":"Parameter Type Default Description <code>api_country_code</code> str \"DE\" ISO country code for ENTSO-E API <code>predict_size</code> int 24 Number of hours to predict ahead <code>refit_size</code> int 7 Number of days between model refits <code>train_size</code> Timedelta 3 years Training data window <code>random_state</code> int 314159 Random seed for reproducibility <code>periods</code> List[Period] 5 periods Cyclical feature encodings"},{"location":"entsoe/#accessing-period-configurations","title":"Accessing Period Configurations","text":"<p>View the cyclical encoding periods:</p> <pre><code>from spotforecast2 import Config\n\nconfig = Config()\nfor period in config.periods:\n    print(f\"{period.name}: {period.n_periods} basis functions\")\n</code></pre>"},{"location":"entsoe/#feature-engineering","title":"Feature Engineering","text":""},{"location":"entsoe/#period-dataclass","title":"Period Dataclass","text":"<p>Periods define cyclical time features using radial basis functions:</p> <pre><code>from spotforecast2_safe.data import Period\n\ndaily = Period(\n    name='daily',\n    n_periods=12,\n    column='hour',\n    input_range=(1, 24)\n)\nprint(daily.name)        # 'daily'\nprint(daily.n_periods)   # 12\n</code></pre>"},{"location":"entsoe/#repeatingbasisfunction","title":"RepeatingBasisFunction","text":"<p>Transform time features into smooth cyclical encodings:</p> <pre><code>from spotforecast2_safe.preprocessing import RepeatingBasisFunction\nimport pandas as pd\n\nrbf = RepeatingBasisFunction(\n    n_periods=12,\n    column='hour',\n    input_range=(1, 24)\n)\n\ndf = pd.DataFrame({'hour': range(1, 25)})\nfeatures = rbf.transform(df)\nprint(features.shape)  # (24, 12)\n</code></pre>"},{"location":"entsoe/#exogbuilder","title":"ExogBuilder","text":"<p>Build complete exogenous feature sets including holidays and weekends:</p> <pre><code>from spotforecast2_safe.preprocessing import ExogBuilder\nfrom spotforecast2_safe.data import Period\nimport pandas as pd\n\nperiods = [\n    Period(name='daily', n_periods=12, column='hour', input_range=(1, 24)),\n    Period(name='weekly', n_periods=7, column='dayofweek', input_range=(0, 6)),\n]\n\nbuilder = ExogBuilder(periods=periods, country_code='DE')\nX = builder.build(\n    pd.Timestamp('2025-01-01', tz='UTC'),\n    pd.Timestamp('2025-01-02', tz='UTC')\n)\nprint(X.shape)  # (25, 21) - 12 + 7 + 2 (holiday, weekend)\n</code></pre>"},{"location":"entsoe/#using-config-with-exogbuilder","title":"Using Config with ExogBuilder","text":"<p>Combine configuration and feature building:</p> <pre><code>from spotforecast2 import Config\nfrom spotforecast2_safe.preprocessing import ExogBuilder\nimport pandas as pd\n\nconfig = Config()\nbuilder = ExogBuilder(\n    periods=config.periods,\n    country_code=config.API_COUNTRY_CODE\n)\nX = builder.build(\n    pd.Timestamp('2025-12-31', tz='UTC'),\n    pd.Timestamp('2026-01-01', tz='UTC')\n)\nprint(f\"Generated {X.shape[1]} features for {X.shape[0]} hours\")\n</code></pre>"},{"location":"entsoe/#data-preprocessing","title":"Data Preprocessing","text":""},{"location":"entsoe/#linear-interpolation","title":"Linear Interpolation","text":"<p>Handle missing values in time series data:</p> <pre><code>from spotforecast2_safe.preprocessing import LinearlyInterpolateTS\nimport pandas as pd\nimport numpy as np\n\nts = pd.Series(\n    [1.0, np.nan, 3.0, np.nan, 5.0],\n    index=pd.date_range('2025-01-01', periods=5, freq='h')\n)\n\ninterpolator = LinearlyInterpolateTS()\nts_clean = interpolator.fit_transform(ts)\n\nprint(ts_clean.values)  # [1.0, 2.0, 3.0, 4.0, 5.0]\n</code></pre>"},{"location":"entsoe/#forecaster-models","title":"Forecaster Models","text":""},{"location":"entsoe/#lightgbm-forecaster","title":"LightGBM Forecaster","text":"<p>Create a LightGBM-based recursive forecaster:</p> <pre><code>from spotforecast2.tasks.task_entsoe import ForecasterRecursiveLGBM, config\n\nmodel = ForecasterRecursiveLGBM(iteration=1)\n\nprint(model.name)             # 'lgbm'\nprint(model.random_state)     # 314159 (from config)\nprint(len(model.preprocessor.periods))  # 5 (from config)\n</code></pre>"},{"location":"entsoe/#xgboost-forecaster","title":"XGBoost Forecaster","text":"<p>Create an XGBoost-based recursive forecaster:</p> <pre><code>from spotforecast2.tasks.task_entsoe import ForecasterRecursiveXGB, config\n\nmodel = ForecasterRecursiveXGB(iteration=1, lags=24)\n\nprint(model.name)  # 'xgb'\n</code></pre>"},{"location":"entsoe/#custom-configuration-forecaster","title":"Custom Configuration Forecaster","text":"<p>Override default configuration values:</p> <pre><code>from spotforecast2.tasks.task_entsoe import ForecasterRecursiveLGBM\nfrom spotforecast2_safe.data import Period\n\ncustom_periods = [\n    Period(name='hourly', n_periods=24, column='hour', input_range=(1, 24)),\n]\n\nmodel = ForecasterRecursiveLGBM(\n    iteration=1,\n    lags=48,\n    periods=custom_periods,\n    country_code='FR',\n    random_state=42\n)\n\nprint(len(model.preprocessor.periods))  # 1\nprint(model.preprocessor.country_code)  # 'FR'\n</code></pre>"},{"location":"entsoe/#using-the-python-api-notebooks-quarto","title":"Using the Python API (Notebooks &amp; Quarto)","text":""},{"location":"entsoe/#full-prediction-pipeline","title":"Full Prediction Pipeline","text":"<p>For users working in Jupyter Notebooks or Quarto, the entire ENTSO-E pipeline can be executed using the Python API. This approach is highly recommended for safety-critical research as it allows for precise control over time windows and hyperparameters.</p> <pre><code>import pandas as pd\nimport os\nfrom spotforecast2_safe.downloader.entsoe import download_new_data\nfrom spotforecast2_safe.manager.trainer import handle_training as handle_training_safe\nfrom spotforecast2_safe.manager.predictor import get_model_prediction as get_model_prediction_safe\nfrom spotforecast2.manager.plotter import make_plot\nfrom spotforecast2.tasks.task_entsoe import ForecasterRecursiveLGBM\n\n# 1. Setup Time Windows (Last 3 years until last month)\nnow = pd.Timestamp.now(tz='UTC').floor('D')\ncurrent_month_start = now.replace(day=1)\nlast_month_start = (current_month_start - pd.Timedelta(days=1)).replace(day=1)\n\n# 2. Download Data (Optional, requires ENTSOE_API_KEY)\napi_key = os.environ.get(\"ENTSOE_API_KEY\")\nif api_key:\n    download_new_data(api_key=api_key, start=\"202301010000\")\n\n# 3. Configure and Train\n# Explicit parameters override global configuration for reproducibility\nmodel_class = ForecasterRecursiveLGBM\nmodel_name = \"lgbm_advanced\"\n\nhandle_training_safe(\n    model_class=model_class,\n    model_name=model_name,\n    train_size=pd.Timedelta(days=3 * 365),\n    end_dev=last_month_start.strftime(\"%Y-%m-%d %H:%M%z\"),\n)\n\n# 4. Generate Predictions for the forecast horizon\n# The predictor will automatically load the model trained above\npredictions = get_model_prediction_safe(\n    model_name=model_name,\n    predict_size=24 * 31\n)\n\n# 5. Visualize Results\nif predictions:\n    make_plot(predictions)\n</code></pre>"},{"location":"entsoe/#file-paths","title":"File Paths","text":""},{"location":"entsoe/#data-home-directory","title":"Data Home Directory","text":"<p>Access the data storage location:</p> <pre><code>from spotforecast2_safe.data import get_data_home\n\ndata_home = get_data_home()\nprint(data_home)  # ~/spotforecast2_data or SPOTFORECAST2_DATA\n</code></pre>"},{"location":"entsoe/#cli-commands","title":"CLI Commands","text":""},{"location":"entsoe/#download-data","title":"Download Data","text":"<pre><code># Download with API key\nuv run spotforecast2-entsoe download --api-key YOUR_API_KEY 202301010000\n\n# Download with date range\nuv run spotforecast2-entsoe download 202301010000 202312312300\n\n# Force re-download\nuv run spotforecast2-entsoe download --force 202301010000\n</code></pre>"},{"location":"entsoe/#train-models","title":"Train Models","text":"<pre><code># Train LightGBM model\nuv run spotforecast2-entsoe train lgbm\n\n# Train XGBoost model\nuv run spotforecast2-entsoe train xgb\n\n# Force retraining\nuv run spotforecast2-entsoe train lgbm --force\n</code></pre>"},{"location":"entsoe/#generate-predictions","title":"Generate Predictions","text":"<pre><code># Predict with default model (lgbm)\nuv run spotforecast2-entsoe predict\n\n# Predict with specific model\nuv run spotforecast2-entsoe predict lgbm\nuv run spotforecast2-entsoe predict xgb\n\n# Predict and generate plot\nuv run spotforecast2-entsoe predict --plot\n</code></pre>"},{"location":"entsoe/#merge-data-files","title":"Merge Data Files","text":"<pre><code>uv run spotforecast2-entsoe merge\n</code></pre>"},{"location":"entsoe/#environment-variables","title":"Environment Variables","text":"Variable Description <code>ENTSOE_API_KEY</code> ENTSO-E API key for data downloads <code>SPOTFORECAST2_DATA</code> Custom data directory (default: ~/spotforecast2_data)"},{"location":"entsoe/#testing","title":"Testing","text":"<p>All examples in this guide are validated by automated tests:</p> <pre><code># Run documentation example tests\nuv run pytest tests/test_docs_entsoe_examples.py -v\n\n# Run all ENTSO-E tests\nuv run pytest tests/test_tasks_entsoe.py -v\n</code></pre>"},{"location":"entsoe/#see-also","title":"See Also","text":"<ul> <li>Tasks Overview - All available CLI commands</li> <li>API Reference - Detailed API documentation</li> <li>Model Persistence - Saving and loading models</li> </ul>"},{"location":"tasks/","title":"Task Scripts","text":"<p><code>spotforecast2</code> provides command-line task scripts for common forecasting workflows. These scripts are registered as console entry points and can be invoked directly via <code>uv run</code> or after package installation.</p>"},{"location":"tasks/#available-commands","title":"Available Commands","text":"Command Description <code>spotforecast2-entsoe</code> ENTSO-E energy forecasting pipeline (download, train, predict) <code>spotforecast-demo</code> Demonstration task comparing baseline, covariate, and custom models <code>spotforecast-n2o1</code> N-to-1 forecasting with weighted aggregation <code>spotforecast-n2o1-df</code> N-to-1 forecasting using a DataFrame input <code>spotforecast-n2o1-cov</code> N-to-1 forecasting with exogenous covariates <code>spotforecast-n2o1-cov-df</code> N-to-1 forecasting with covariates and DataFrame input"},{"location":"tasks/#entso-e-task","title":"ENTSO-E Task","text":"<p>The <code>spotforecast2-entsoe</code> command provides a unified CLI for the ENTSO-E energy forecasting pipeline.</p>"},{"location":"tasks/#subcommands","title":"Subcommands","text":"<pre><code># Download data from ENTSO-E\nuv run spotforecast2-entsoe download --api-key YOUR_API_KEY 202301010000\n\n# Train a model (lgbm or xgb)\nuv run spotforecast2-entsoe train lgbm --force\n\n# Generate predictions and plot (defaults to lgbm)\nuv run spotforecast2-entsoe predict --plot\n\n# Generate predictions with explicit model selection\nuv run spotforecast2-entsoe predict lgbm --plot\nuv run spotforecast2-entsoe predict xgb --plot\n\n# Merge raw data files\nuv run spotforecast2-entsoe merge\n</code></pre>"},{"location":"tasks/#download-arguments-and-time-format","title":"Download arguments and time format","text":"<p>The positional argument 202301010000 is a UTC timestamp in the format YYYYMMDDHHMM. It represents the start of the download window. You can provide either one timestamp (start only) or two timestamps (start and end).</p> <pre><code># Start only (end defaults to now, UTC)\nuv run spotforecast2-entsoe download 202301010000\n\n# Start and end (UTC)\nuv run spotforecast2-entsoe download 202301010000 202312312300\n</code></pre> <p>Hidden arguments and defaults for download:</p> <ul> <li>--api-key or ENTSOE_API_KEY environment variable</li> <li>--force to re-download even if files already exist</li> <li>data home controlled by SPOTFORECAST2_DATA (default is ~/spotforecast2_data)</li> </ul>"},{"location":"tasks/#configuration","title":"Configuration","text":"<p>The ENTSO-E task uses a configuration class that can be customized programmatically. All configuration parameters have sensible defaults but can be overridden when needed.</p>"},{"location":"tasks/#using-default-configuration","title":"Using Default Configuration","text":"<pre><code>from spotforecast2 import Config\n\n# Create default configuration instance\nconfig = Config()\n\n# Access configuration values\nprint(config.API_COUNTRY_CODE)  # 'DE'\nprint(config.predict_size)      # 24\nprint(config.train_size)        # Timedelta(days=1095)\n</code></pre>"},{"location":"tasks/#custom-configuration","title":"Custom Configuration","text":"<pre><code>from spotforecast2 import Config\nimport pandas as pd\n\n# Create custom configuration\ncustom_config = Config(\n    api_country_code='DE',\n    predict_size=48,\n    refit_size=14,\n    train_size=pd.Timedelta(days=365),\n    random_state=42\n)\n\n# Use in your code\nprint(custom_config.API_COUNTRY_CODE)  # 'DE'\nprint(custom_config.predict_size)      # 48\n</code></pre>"},{"location":"tasks/#available-configuration-parameters","title":"Available Configuration Parameters","text":"Parameter Type Default Description <code>api_country_code</code> str \"DE\" ISO country code for ENTSO-E API <code>predict_size</code> int 24 Number of hours to predict ahead <code>refit_size</code> int 7 Number of days between model refits <code>train_size</code> Timedelta 3 years Training data window <code>end_train_default</code> str \"2025-12-31 00:00+00:00\" Default training end date <code>delta_val</code> Timedelta 10 weeks Validation window size <code>random_state</code> int 314159 Random seed for reproducibility <code>n_hyperparameters_trials</code> int 20 Hyperparameter tuning trials <code>lags_consider</code> List[int] [1..23] Lag values for features <code>periods</code> List[Period] 5 periods Cyclical feature encodings <p>For more details, see the ConfigEntsoe API documentation.</p>"},{"location":"tasks/#time-intervals-for-download-training-prediction-validation-and-testing","title":"Time intervals for download, training, prediction, validation, and testing","text":"<p>Download interval is defined by the start/end timestamps passed to the download command.</p> <p>Training, prediction, validation, and testing intervals are configured via the Config class. The CLI uses default configuration values which can be modified programmatically:</p> <ul> <li>training end time: config.end_train_default (defaults to \"2025-12-31 00:00+00:00\")</li> <li>training window size: config.train_size (defaults to 3 years)</li> <li>prediction window: config.predict_size * config.refit_size hours</li> </ul> <p>Validation and testing are derived from the prediction window:</p> <ul> <li>validation metrics use the first 24 hours of the prediction window</li> <li>testing metrics use the full prediction window</li> </ul> <p>Customizing Configuration</p> <p>To use custom configuration values, you'll need to modify the task script to create a Config instance with your desired parameters. See the Configuration section above for examples.</p> <p>API Key Management</p> <p>Store your ENTSO-E API key in the <code>ENTSOE_API_KEY</code> environment variable to avoid passing it on every command: <pre><code>export ENTSOE_API_KEY=\"your-api-key-here\"\necho $ENTSOE_API_KEY\nuv run spotforecast2-entsoe download 202301010000\n</code></pre></p>"},{"location":"tasks/#visualize-results","title":"Visualize Results","text":"<p>The prediction plot shows the following graphs:</p> <ul> <li>Total system load (actual): The real-time electricity demand (consumption) within the bidding zone. This includes network losses but excludes consumption for pumped storage and generating auxiliaries.</li> <li>Total system load (model prediction): The demand forecast generated by the <code>spotforecast2</code> machine learning model (e.g., LightGBM or XGBoost) based on historical data and exogenous features.</li> <li>Benchmark Forecast (e.g. ENTSOE): The reference forecast provided by the Transmission System Operators (TSOs) via the ENTSO-E Transparency Platform.</li> <li>Actual (last week): The actual system load from exactly one week ago at the same time, which serves as a seasonal baseline comparison.</li> </ul> <p>The prediction plot is saved as an HTML file named <code>index.html</code> in the data home directory. By default this is <code>~/spotforecast2_data/index.html</code> or the path defined by <code>SPOTFORECAST2_DATA</code>.</p> <pre><code># Default location on macOS/Linux\nopen ~/spotforecast2_data/index.html\n\n# If you use a custom data home\nopen \"$SPOTFORECAST2_DATA/index.html\"\n</code></pre> <p>Check the CLI logs for the exact path (look for \"Plot saved to ...\").</p>"},{"location":"tasks/#demo-task","title":"Demo Task","text":"<p>The <code>spotforecast-demo</code> command runs a comparison of three forecasting approaches:</p> <ol> <li>Baseline: Standard N-to-1 recursive forecaster</li> <li>Covariate-enhanced: Includes weather, holidays, and cyclical features</li> <li>Custom LightGBM: Optimized hyperparameters</li> </ol> <pre><code># Run with default settings\nuv run spotforecast-demo\n\n# Force retraining and save plot\nuv run spotforecast-demo --force_train true --html task_demo_plot.html\n</code></pre>"},{"location":"tasks/#n-to-1-forecasting-tasks","title":"N-to-1 Forecasting Tasks","text":"<p>These tasks implement multi-output time series forecasting with weighted aggregation.</p>"},{"location":"tasks/#basic-n-to-1","title":"Basic N-to-1","text":"<pre><code>uv run spotforecast-n2o1\n</code></pre>"},{"location":"tasks/#n-to-1-with-dataframe-input","title":"N-to-1 with DataFrame Input","text":"<pre><code>uv run spotforecast-n2o1-df\n</code></pre>"},{"location":"tasks/#n-to-1-with-covariates","title":"N-to-1 with Covariates","text":"<p>Includes weather data, holiday indicators, and cyclical time features.</p> <pre><code>uv run spotforecast-n2o1-cov\n</code></pre>"},{"location":"tasks/#n-to-1-with-covariates-and-dataframe","title":"N-to-1 with Covariates and DataFrame","text":"<pre><code>uv run spotforecast-n2o1-cov-df\n</code></pre>"},{"location":"tasks/#configuration_1","title":"Configuration","text":"<p>All tasks use sensible defaults but can be customized via:</p> <ul> <li>Environment variables (e.g., <code>ENTSOE_API_KEY</code>)</li> <li>Command-line arguments (use <code>--help</code> for details)</li> <li>Configuration files stored in <code>~/spotforecast2_models/</code></li> </ul> <pre><code># View available options for any command\nuv run spotforecast-demo --help\nuv run spotforecast2-entsoe predict --help\n</code></pre>"},{"location":"tasks/#model-persistence","title":"Model Persistence","text":"<p>Trained models are saved to <code>~/spotforecast2_models/&lt;task_name&gt;/</code> by default. This allows:</p> <ul> <li>Incremental retraining: Only retrain when models are stale</li> <li>Reproducibility: Models are versioned by task and timestamp</li> <li>Auditability: Full training logs are stored alongside models</li> </ul> <p>Safety-Critical Consideration</p> <p>In production environments, always verify model checksums and training timestamps before deployment.</p>"},{"location":"tasks/#testing","title":"Testing","text":"<p>The task scripts are covered by comprehensive safety-critical tests to ensure reliability in production environments.</p>"},{"location":"tasks/#running-tests","title":"Running Tests","text":"<p>Run all ENTSO-E task tests:</p> <pre><code>uv run pytest tests/test_tasks_entsoe.py -v\n</code></pre> <p>Run specific test categories:</p> <pre><code># Run only safety-critical tests\nuv run pytest tests/test_tasks_entsoe.py::TestSafetyCriticalEntsoe -v\n\n# Run parameter validation tests\nuv run pytest tests/test_tasks_entsoe.py::TestSafetyCriticalEntsoe::test_train_lgbm_model_parameter_correctness -v\n\n# Run with coverage\nuv run pytest tests/test_tasks_entsoe.py --cov=spotforecast2.tasks.task_entsoe --cov-report=html\n</code></pre>"},{"location":"tasks/#test-categories","title":"Test Categories","text":"<p>The test suite includes:</p> <ul> <li>Parameter Validation: Ensures correct parameter passing between CLI and internal functions</li> <li>Error Handling: Validates graceful degradation and meaningful error messages</li> <li>Data Validation: Tests boundary conditions and edge cases</li> <li>Integration Tests: Verifies end-to-end functionality</li> <li>Regression Tests: Protects against known historical bugs</li> <li>Model Selection Safety: Prevents model mismatch in production pipelines</li> </ul> <p>Continuous Testing</p> <p>Run tests before deployment in production environments: <pre><code>uv run pytest tests/ -v --tb=short\n</code></pre></p>"},{"location":"api/data/","title":"Data Module","text":""},{"location":"api/data/#data","title":"Data","text":""},{"location":"api/data/#spotforecast2_safe.data.data","title":"<code>spotforecast2_safe.data.data</code>","text":"<p>Data structures for input and processed data.</p>"},{"location":"api/data/#spotforecast2_safe.data.data.Data","title":"<code>Data</code>  <code>dataclass</code>","text":"<p>Container for input time series data.</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>DataFrame</code> <p>pandas DataFrame containing the input time series data.</p> Source code in <code>spotforecast2_safe/data/data.py</code> <pre><code>@dataclass\nclass Data:\n    \"\"\"Container for input time series data.\n\n    Attributes:\n        data: pandas DataFrame containing the input time series data.\n    \"\"\"\n\n    data: pd.DataFrame\n\n    @classmethod\n    def from_csv(\n        cls,\n        csv_path: Path,\n        timezone: Optional[str],\n        columns: Optional[List[str]] = None,\n        parse_dates=True,\n        index_col=0,\n        **kwargs,\n    ) -&gt; \"Data\":\n        \"\"\"Load data from a CSV file.\n\n        The CSV must contain a datetime column that becomes the DataFrame index.\n        The index is localized to the provided timezone if it is naive, and then\n        converted to UTC.\n\n        Args:\n            csv_path (Path): Path to the CSV file.\n            timezone (Optional[str]): Timezone to assign if the index has no\n                timezone. Must be provided if the index is naive.\n            columns (Optional[List[str]]): List of column names to include. If\n                provided, only these columns will be loaded from the CSV\n                (optimizes reading speed). If None, all columns are loaded.\n            parse_dates (bool or list, optional): Passed to ``pd.read_csv``.\n                Defaults to True.\n            index_col (int or str, optional): Column to use as index. Defaults to 0.\n            **kwargs (Any): Additional keyword arguments forwarded to ``pd.read_csv``.\n\n        Returns:\n            Data: Instance containing the loaded DataFrame.\n\n        Raises:\n            ValueError: If the CSV does not yield a DatetimeIndex.\n            ValueError: If the index is timezone-naive and no timezone is provided.\n\n        Examples:\n            &gt;&gt;&gt; from spotforecast2_safe.data import Data\n            &gt;&gt;&gt; data = Data.from_csv(\n            ...     Path(\"data.csv\"),\n            ...     timezone=\"UTC\",\n            ...     columns=[\"target_col\"]\n            ... )\n        \"\"\"\n        # If columns specified, add index column to usecols for efficient reading\n        usecols = None\n        if columns is not None:\n            # Get the index column name/number\n            if isinstance(index_col, int):\n                # Read header first to get column names\n                header_df = pd.read_csv(csv_path, nrows=0)\n                index_col_name = header_df.columns[index_col]\n            else:\n                index_col_name = index_col\n            usecols = [index_col_name] + columns\n\n        df = pd.read_csv(\n            csv_path,\n            parse_dates=parse_dates,\n            index_col=index_col,\n            usecols=usecols,\n            **kwargs,\n        )\n        df = convert_to_utc(df, timezone)\n        if df.index.freq is None:\n            try:\n                df.index.freq = pd.infer_freq(df.index)\n            except (ValueError, TypeError):\n                pass\n        return cls(data=df)\n\n    @classmethod\n    def from_dataframe(\n        cls,\n        df: pd.DataFrame,\n        timezone: Optional[str],\n        columns: Optional[List[str]] = None,\n    ) -&gt; \"Data\":\n        \"\"\"Create a new Data instance from an existing DataFrame.\n\n        The DataFrame must have a datetime index. The index is localized to the\n        provided timezone if it is naive, and then converted to UTC.\n\n        Args:\n            df (pd.DataFrame): Input DataFrame containing data.\n            timezone (Optional[str]): Timezone to assign if the index is naive.\n                Must be provided if the index has no timezone.\n            columns (Optional[List[str]]): List of column names to include.\n                If provided, only these columns will be selected from the\n                DataFrame. If None, all columns are used.\n\n        Returns:\n            Data: Instance containing the provided DataFrame.\n\n        Raises:\n            ValueError: If the DataFrame index is not a DatetimeIndex.\n            ValueError: If the index is timezone-naive and no timezone is provided.\n        \"\"\"\n        df = convert_to_utc(df, timezone)\n\n        # Select columns if specified\n        if columns is not None:\n            df = df[columns].copy()\n\n        if df.index.freq is None:\n            try:\n                df.index.freq = pd.infer_freq(df.index)\n            except (ValueError, TypeError):\n                pass\n\n        return cls(data=df)\n</code></pre>"},{"location":"api/data/#spotforecast2_safe.data.data.Data.from_csv","title":"<code>from_csv(csv_path, timezone, columns=None, parse_dates=True, index_col=0, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load data from a CSV file.</p> <p>The CSV must contain a datetime column that becomes the DataFrame index. The index is localized to the provided timezone if it is naive, and then converted to UTC.</p> <p>Parameters:</p> Name Type Description Default <code>csv_path</code> <code>Path</code> <p>Path to the CSV file.</p> required <code>timezone</code> <code>Optional[str]</code> <p>Timezone to assign if the index has no timezone. Must be provided if the index is naive.</p> required <code>columns</code> <code>Optional[List[str]]</code> <p>List of column names to include. If provided, only these columns will be loaded from the CSV (optimizes reading speed). If None, all columns are loaded.</p> <code>None</code> <code>parse_dates</code> <code>bool or list</code> <p>Passed to <code>pd.read_csv</code>. Defaults to True.</p> <code>True</code> <code>index_col</code> <code>int or str</code> <p>Column to use as index. Defaults to 0.</p> <code>0</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments forwarded to <code>pd.read_csv</code>.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Data</code> <code>Data</code> <p>Instance containing the loaded DataFrame.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the CSV does not yield a DatetimeIndex.</p> <code>ValueError</code> <p>If the index is timezone-naive and no timezone is provided.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data import Data\n&gt;&gt;&gt; data = Data.from_csv(\n...     Path(\"data.csv\"),\n...     timezone=\"UTC\",\n...     columns=[\"target_col\"]\n... )\n</code></pre> Source code in <code>spotforecast2_safe/data/data.py</code> <pre><code>@classmethod\ndef from_csv(\n    cls,\n    csv_path: Path,\n    timezone: Optional[str],\n    columns: Optional[List[str]] = None,\n    parse_dates=True,\n    index_col=0,\n    **kwargs,\n) -&gt; \"Data\":\n    \"\"\"Load data from a CSV file.\n\n    The CSV must contain a datetime column that becomes the DataFrame index.\n    The index is localized to the provided timezone if it is naive, and then\n    converted to UTC.\n\n    Args:\n        csv_path (Path): Path to the CSV file.\n        timezone (Optional[str]): Timezone to assign if the index has no\n            timezone. Must be provided if the index is naive.\n        columns (Optional[List[str]]): List of column names to include. If\n            provided, only these columns will be loaded from the CSV\n            (optimizes reading speed). If None, all columns are loaded.\n        parse_dates (bool or list, optional): Passed to ``pd.read_csv``.\n            Defaults to True.\n        index_col (int or str, optional): Column to use as index. Defaults to 0.\n        **kwargs (Any): Additional keyword arguments forwarded to ``pd.read_csv``.\n\n    Returns:\n        Data: Instance containing the loaded DataFrame.\n\n    Raises:\n        ValueError: If the CSV does not yield a DatetimeIndex.\n        ValueError: If the index is timezone-naive and no timezone is provided.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data import Data\n        &gt;&gt;&gt; data = Data.from_csv(\n        ...     Path(\"data.csv\"),\n        ...     timezone=\"UTC\",\n        ...     columns=[\"target_col\"]\n        ... )\n    \"\"\"\n    # If columns specified, add index column to usecols for efficient reading\n    usecols = None\n    if columns is not None:\n        # Get the index column name/number\n        if isinstance(index_col, int):\n            # Read header first to get column names\n            header_df = pd.read_csv(csv_path, nrows=0)\n            index_col_name = header_df.columns[index_col]\n        else:\n            index_col_name = index_col\n        usecols = [index_col_name] + columns\n\n    df = pd.read_csv(\n        csv_path,\n        parse_dates=parse_dates,\n        index_col=index_col,\n        usecols=usecols,\n        **kwargs,\n    )\n    df = convert_to_utc(df, timezone)\n    if df.index.freq is None:\n        try:\n            df.index.freq = pd.infer_freq(df.index)\n        except (ValueError, TypeError):\n            pass\n    return cls(data=df)\n</code></pre>"},{"location":"api/data/#spotforecast2_safe.data.data.Data.from_dataframe","title":"<code>from_dataframe(df, timezone, columns=None)</code>  <code>classmethod</code>","text":"<p>Create a new Data instance from an existing DataFrame.</p> <p>The DataFrame must have a datetime index. The index is localized to the provided timezone if it is naive, and then converted to UTC.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing data.</p> required <code>timezone</code> <code>Optional[str]</code> <p>Timezone to assign if the index is naive. Must be provided if the index has no timezone.</p> required <code>columns</code> <code>Optional[List[str]]</code> <p>List of column names to include. If provided, only these columns will be selected from the DataFrame. If None, all columns are used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Data</code> <code>Data</code> <p>Instance containing the provided DataFrame.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the DataFrame index is not a DatetimeIndex.</p> <code>ValueError</code> <p>If the index is timezone-naive and no timezone is provided.</p> Source code in <code>spotforecast2_safe/data/data.py</code> <pre><code>@classmethod\ndef from_dataframe(\n    cls,\n    df: pd.DataFrame,\n    timezone: Optional[str],\n    columns: Optional[List[str]] = None,\n) -&gt; \"Data\":\n    \"\"\"Create a new Data instance from an existing DataFrame.\n\n    The DataFrame must have a datetime index. The index is localized to the\n    provided timezone if it is naive, and then converted to UTC.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing data.\n        timezone (Optional[str]): Timezone to assign if the index is naive.\n            Must be provided if the index has no timezone.\n        columns (Optional[List[str]]): List of column names to include.\n            If provided, only these columns will be selected from the\n            DataFrame. If None, all columns are used.\n\n    Returns:\n        Data: Instance containing the provided DataFrame.\n\n    Raises:\n        ValueError: If the DataFrame index is not a DatetimeIndex.\n        ValueError: If the index is timezone-naive and no timezone is provided.\n    \"\"\"\n    df = convert_to_utc(df, timezone)\n\n    # Select columns if specified\n    if columns is not None:\n        df = df[columns].copy()\n\n    if df.index.freq is None:\n        try:\n            df.index.freq = pd.infer_freq(df.index)\n        except (ValueError, TypeError):\n            pass\n\n    return cls(data=df)\n</code></pre>"},{"location":"api/data/#spotforecast2_safe.data.data.Period","title":"<code>Period</code>  <code>dataclass</code>","text":"<p>Class abstraction for the information required to encode a period using RBF.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the period (e.g., 'hour', 'day').</p> <code>n_periods</code> <code>int</code> <p>Number of periods to encode (e.g., 24 for hours).</p> <code>column</code> <code>str</code> <p>Name of the column in the DataFrame containing the period information.</p> <code>input_range</code> <code>Tuple[int, int]</code> <p>Tuple of (min, max) values for the period (e.g., (0, 23) for hours).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data import Period\n&gt;&gt;&gt; period = Period(name=\"hour\", n_periods=24, column=\"hour\", input_range=(0, 23))\n&gt;&gt;&gt; period.name\n'hour'\n</code></pre> Source code in <code>spotforecast2_safe/data/data.py</code> <pre><code>@dataclass\nclass Period:\n    \"\"\"Class abstraction for the information required to encode a period using RBF.\n\n    Attributes:\n        name: Name of the period (e.g., 'hour', 'day').\n        n_periods: Number of periods to encode (e.g., 24 for hours).\n        column: Name of the column in the DataFrame containing the period information.\n        input_range: Tuple of (min, max) values for the period (e.g., (0, 23) for hours).\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data import Period\n        &gt;&gt;&gt; period = Period(name=\"hour\", n_periods=24, column=\"hour\", input_range=(0, 23))\n        &gt;&gt;&gt; period.name\n        'hour'\n    \"\"\"\n\n    name: str\n    n_periods: int\n    column: str\n    input_range: Tuple[int, int]\n\n    def __post_init__(self):\n        \"\"\"Validate the period configuration.\"\"\"\n        if self.n_periods &lt;= 0:\n            raise ValueError(f\"n_periods must be positive, got {self.n_periods}\")\n        if self.input_range[0] &gt;= self.input_range[1]:\n            raise ValueError(\n                f\"input_range[0] must be less than input_range[1], got {self.input_range}\"\n            )\n</code></pre>"},{"location":"api/data/#spotforecast2_safe.data.data.Period.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the period configuration.</p> Source code in <code>spotforecast2_safe/data/data.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate the period configuration.\"\"\"\n    if self.n_periods &lt;= 0:\n        raise ValueError(f\"n_periods must be positive, got {self.n_periods}\")\n    if self.input_range[0] &gt;= self.input_range[1]:\n        raise ValueError(\n            f\"input_range[0] must be less than input_range[1], got {self.input_range}\"\n        )\n</code></pre>"},{"location":"api/exceptions/","title":"Exceptions and Warnings","text":""},{"location":"api/exceptions/#spotforecast2.exceptions","title":"<code>spotforecast2.exceptions</code>","text":"<p>Custom exceptions and warnings for spotforecast2.</p> <p>This module contains all the custom warnings and error classes used across spotforecast2.</p> <p>Examples:</p> <p>Using custom warnings::</p> <pre><code>import warnings\nfrom spotforecast2.exceptions import MissingValuesWarning\n\n# Raise a warning\nwarnings.warn(\n    \"Missing values detected in input data.\",\n    MissingValuesWarning\n)\n\n# Suppress a specific warning\nwarnings.simplefilter('ignore', category=MissingValuesWarning)\n</code></pre>"},{"location":"api/exceptions/#spotforecast2.exceptions.DataTransformationWarning","title":"<code>DataTransformationWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for output data in transformed space.</p> <p>Used to notify that the output data is in the transformed space.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Output is in transformed space.\",\n...     DataTransformationWarning\n... )\n</code></pre> Source code in <code>spotforecast2_safe/exceptions.py</code> <pre><code>class DataTransformationWarning(UserWarning):\n    \"\"\"Warning for output data in transformed space.\n\n    Used to notify that the output data is in the transformed space.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Output is in transformed space.\",\n        ...     DataTransformationWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=DataTransformationWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2.exceptions.DataTypeWarning","title":"<code>DataTypeWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for incompatible data types in exogenous data.</p> <p>Used to notify there are dtypes in the exogenous data that are not 'int', 'float', 'bool' or 'category'. Most machine learning models do not accept other data types, therefore the forecaster <code>fit</code> and <code>predict</code> may fail.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Exogenous data contains unsupported dtypes.\",\n...     DataTypeWarning\n... )\n</code></pre> Source code in <code>spotforecast2_safe/exceptions.py</code> <pre><code>class DataTypeWarning(UserWarning):\n    \"\"\"Warning for incompatible data types in exogenous data.\n\n    Used to notify there are dtypes in the exogenous data that are not\n    'int', 'float', 'bool' or 'category'. Most machine learning models do not\n    accept other data types, therefore the forecaster `fit` and `predict` may fail.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Exogenous data contains unsupported dtypes.\",\n        ...     DataTypeWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=DataTypeWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2.exceptions.ExogenousInterpretationWarning","title":"<code>ExogenousInterpretationWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning about implications when using exogenous variables.</p> <p>Used to notify about important implications when using exogenous variables with models that use a two-step approach (e.g., regression + ARAR).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Exogenous variables may not be used as expected.\",\n...     ExogenousInterpretationWarning\n... )\n</code></pre> Source code in <code>spotforecast2_safe/exceptions.py</code> <pre><code>class ExogenousInterpretationWarning(UserWarning):\n    \"\"\"Warning about implications when using exogenous variables.\n\n    Used to notify about important implications when using exogenous\n    variables with models that use a two-step approach (e.g., regression + ARAR).\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Exogenous variables may not be used as expected.\",\n        ...     ExogenousInterpretationWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=ExogenousInterpretationWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2.exceptions.FeatureOutOfRangeWarning","title":"<code>FeatureOutOfRangeWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for features out of training range.</p> <p>Used to notify that a feature is out of the range seen during training.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Feature value exceeds training range.\",\n...     FeatureOutOfRangeWarning\n... )\n</code></pre> Source code in <code>spotforecast2_safe/exceptions.py</code> <pre><code>class FeatureOutOfRangeWarning(UserWarning):\n    \"\"\"Warning for features out of training range.\n\n    Used to notify that a feature is out of the range seen during training.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Feature value exceeds training range.\",\n        ...     FeatureOutOfRangeWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=FeatureOutOfRangeWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2.exceptions.IgnoredArgumentWarning","title":"<code>IgnoredArgumentWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for ignored arguments.</p> <p>Used to notify that an argument is ignored when using a method or a function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Argument 'x' is ignored in this context.\",\n...     IgnoredArgumentWarning\n... )\n</code></pre> Source code in <code>spotforecast2_safe/exceptions.py</code> <pre><code>class IgnoredArgumentWarning(UserWarning):\n    \"\"\"Warning for ignored arguments.\n\n    Used to notify that an argument is ignored when using a method\n    or a function.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Argument 'x' is ignored in this context.\",\n        ...     IgnoredArgumentWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=IgnoredArgumentWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2.exceptions.InputTypeWarning","title":"<code>InputTypeWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for inefficient input format.</p> <p>Used to notify that input format is not the most efficient or recommended for the forecaster.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Input format is not optimal for this forecaster.\",\n...     InputTypeWarning\n... )\n</code></pre> Source code in <code>spotforecast2_safe/exceptions.py</code> <pre><code>class InputTypeWarning(UserWarning):\n    \"\"\"Warning for inefficient input format.\n\n    Used to notify that input format is not the most efficient or\n    recommended for the forecaster.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Input format is not optimal for this forecaster.\",\n        ...     InputTypeWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=InputTypeWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2.exceptions.LongTrainingWarning","title":"<code>LongTrainingWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for potentially long training processes.</p> <p>Used to notify that a large number of models will be trained and the the process may take a while to run.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Training may take a long time.\",\n...     LongTrainingWarning\n... )\n</code></pre> Source code in <code>spotforecast2_safe/exceptions.py</code> <pre><code>class LongTrainingWarning(UserWarning):\n    \"\"\"Warning for potentially long training processes.\n\n    Used to notify that a large number of models will be trained and the\n    the process may take a while to run.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Training may take a long time.\",\n        ...     LongTrainingWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=LongTrainingWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2.exceptions.MissingExogWarning","title":"<code>MissingExogWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for missing exogenous variables.</p> <p>Used to indicate that there are missing exogenous variables in the data. Most machine learning models do not accept missing values, so the Forecaster's <code>fit' and</code>predict' methods may fail.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Missing exogenous variables detected.\",\n...     MissingExogWarning\n... )\n</code></pre> Source code in <code>spotforecast2_safe/exceptions.py</code> <pre><code>class MissingExogWarning(UserWarning):\n    \"\"\"Warning for missing exogenous variables.\n\n    Used to indicate that there are missing exogenous variables in the\n    data. Most machine learning models do not accept missing values, so the\n    Forecaster's `fit' and `predict' methods may fail.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Missing exogenous variables detected.\",\n        ...     MissingExogWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=MissingExogWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2.exceptions.MissingValuesWarning","title":"<code>MissingValuesWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for missing values in data.</p> <p>Used to indicate that there are missing values in the data. This warning occurs when the input data contains missing values, or the training matrix generates missing values. Most machine learning models do not accept missing values, so the Forecaster's <code>fit' and</code>predict' methods may fail.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Missing values detected in input data.\",\n...     MissingValuesWarning\n... )\n</code></pre> Source code in <code>spotforecast2_safe/exceptions.py</code> <pre><code>class MissingValuesWarning(UserWarning):\n    \"\"\"Warning for missing values in data.\n\n    Used to indicate that there are missing values in the data. This\n    warning occurs when the input data contains missing values, or the training\n    matrix generates missing values. Most machine learning models do not accept\n    missing values, so the Forecaster's `fit' and `predict' methods may fail.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Missing values detected in input data.\",\n        ...     MissingValuesWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=MissingValuesWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2.exceptions.NotFittedError","title":"<code>NotFittedError</code>","text":"<p>               Bases: <code>ValueError</code>, <code>AttributeError</code></p> <p>Exception class to raise if estimator is used before fitting.</p> <p>This class inherits from both ValueError and AttributeError to help with exception handling and backward compatibility.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.exceptions import NotFittedError\n&gt;&gt;&gt; try:\n...     raise NotFittedError(\"Forecaster not fitted\")\n... except NotFittedError as e:\n...     print(e)\nForecaster not fitted\n</code></pre> Source code in <code>spotforecast2_safe/exceptions.py</code> <pre><code>class NotFittedError(ValueError, AttributeError):\n    \"\"\"Exception class to raise if estimator is used before fitting.\n\n    This class inherits from both ValueError and AttributeError to help with\n    exception handling and backward compatibility.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.exceptions import NotFittedError\n        &gt;&gt;&gt; try:\n        ...     raise NotFittedError(\"Forecaster not fitted\")\n        ... except NotFittedError as e:\n        ...     print(e)\n        Forecaster not fitted\n    \"\"\"\n</code></pre>"},{"location":"api/exceptions/#spotforecast2.exceptions.OneStepAheadValidationWarning","title":"<code>OneStepAheadValidationWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for one-step-ahead validation usage.</p> <p>Used to notify that the one-step-ahead validation is being used.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Using one-step-ahead validation.\",\n...     OneStepAheadValidationWarning\n... )\n</code></pre> Source code in <code>spotforecast2_safe/exceptions.py</code> <pre><code>class OneStepAheadValidationWarning(UserWarning):\n    \"\"\"Warning for one-step-ahead validation usage.\n\n    Used to notify that the one-step-ahead validation is being used.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Using one-step-ahead validation.\",\n        ...     OneStepAheadValidationWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=OneStepAheadValidationWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2.exceptions.ResidualsUsageWarning","title":"<code>ResidualsUsageWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for incorrect residuals usage.</p> <p>Used to notify that a residual are not correctly used in the probabilistic forecasting process.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Residuals are not properly used.\",\n...     ResidualsUsageWarning\n... )\n</code></pre> Source code in <code>spotforecast2_safe/exceptions.py</code> <pre><code>class ResidualsUsageWarning(UserWarning):\n    \"\"\"Warning for incorrect residuals usage.\n\n    Used to notify that a residual are not correctly used in the\n    probabilistic forecasting process.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Residuals are not properly used.\",\n        ...     ResidualsUsageWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=ResidualsUsageWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2.exceptions.SaveLoadSkforecastWarning","title":"<code>SaveLoadSkforecastWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for save/load operations.</p> <p>Used to notify any issues that may arise when saving or loading a forecaster.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Issues detected when saving forecaster.\",\n...     SaveLoadSkforecastWarning\n... )\n</code></pre> Source code in <code>spotforecast2_safe/exceptions.py</code> <pre><code>class SaveLoadSkforecastWarning(UserWarning):\n    \"\"\"Warning for save/load operations.\n\n    Used to notify any issues that may arise when saving or loading\n    a forecaster.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Issues detected when saving forecaster.\",\n        ...     SaveLoadSkforecastWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=SaveLoadSkforecastWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2.exceptions.SpotforecastVersionWarning","title":"<code>SpotforecastVersionWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for version mismatch.</p> <p>Used to notify that the version installed in the environment differs from the version used to initialize the forecaster.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Version mismatch detected.\",\n...     SpotforecastVersionWarning\n... )\n</code></pre> Source code in <code>spotforecast2_safe/exceptions.py</code> <pre><code>class SpotforecastVersionWarning(UserWarning):\n    \"\"\"Warning for version mismatch.\n\n    Used to notify that the version installed in the\n    environment differs from the version used to initialize the forecaster.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Version mismatch detected.\",\n        ...     SpotforecastVersionWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=SpotforecastVersionWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2.exceptions.UnknownLevelWarning","title":"<code>UnknownLevelWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for unknown levels in prediction.</p> <p>Used to notify that a level being predicted was not part of the training data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Predicting for an unknown level.\",\n...     UnknownLevelWarning\n... )\n</code></pre> Source code in <code>spotforecast2_safe/exceptions.py</code> <pre><code>class UnknownLevelWarning(UserWarning):\n    \"\"\"Warning for unknown levels in prediction.\n\n    Used to notify that a level being predicted was not part of the\n    training data.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Predicting for an unknown level.\",\n        ...     UnknownLevelWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=UnknownLevelWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/exceptions/#spotforecast2.exceptions.format_warning_handler","title":"<code>format_warning_handler(message, category, filename, lineno, file=None, line=None)</code>","text":"<p>Custom warning handler to format warnings in a box.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Warning message.</p> required <code>category</code> <code>str</code> <p>Warning category.</p> required <code>filename</code> <code>str</code> <p>Filename where the warning was raised.</p> required <code>lineno</code> <code>str</code> <p>Line number where the warning was raised.</p> required <code>file</code> <code>object</code> <p>File where the warning was raised.</p> <code>None</code> <code>line</code> <code>str</code> <p>Line where the warning was raised.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/spotforecast2/exceptions.py</code> <pre><code>def format_warning_handler(\n    message: str,\n    category: str,\n    filename: str,\n    lineno: str,\n    file: object = None,\n    line: str = None,\n) -&gt; None:\n    \"\"\"Custom warning handler to format warnings in a box.\n\n    Args:\n        message: Warning message.\n        category: Warning category.\n        filename: Filename where the warning was raised.\n        lineno: Line number where the warning was raised.\n        file: File where the warning was raised.\n        line: Line where the warning was raised.\n\n    Returns:\n        None\n    \"\"\"\n\n    if isinstance(message, tuple(warn_skforecast_categories)):\n        width = 88\n        title = type(message).__name__\n        output_text = [\"\\n\"]\n\n        wrapped_message = textwrap.fill(\n            str(message), width=width - 2, expand_tabs=True, replace_whitespace=True\n        )\n        title_top_border = f\"\u256d{'\u2500' * ((width - len(title) - 2) // 2)} {title} {'\u2500' * ((width - len(title) - 2) // 2)}\u256e\"\n        if len(title) % 2 != 0:\n            title_top_border = title_top_border[:-1] + \"\u2500\" + \"\u256e\"\n        bottom_border = f\"\u2570{'\u2500' * width}\u256f\"\n        output_text.append(title_top_border)\n\n        for line in wrapped_message.split(\"\\n\"):\n            output_text.append(f\"\u2502 {line.ljust(width - 2)} \u2502\")\n\n        output_text.append(bottom_border)\n        output_text = \"\\n\".join(output_text)\n        color = \"\\033[38;5;208m\"\n        reset = \"\\033[0m\"\n        output_text = f\"{color}{output_text}{reset}\"\n        print(output_text)\n    else:\n        # Fallback to default Python warning formatting\n        warnings._original_showwarning(message, category, filename, lineno, file, line)\n</code></pre>"},{"location":"api/exceptions/#spotforecast2.exceptions.rich_warning_handler","title":"<code>rich_warning_handler(message, category, filename, lineno, file=None, line=None)</code>","text":"<p>Custom handler for warnings that uses rich to display formatted panels.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Warning message.</p> required <code>category</code> <code>str</code> <p>Warning category.</p> required <code>filename</code> <code>str</code> <p>Filename where the warning was raised.</p> required <code>lineno</code> <code>str</code> <p>Line number where the warning was raised.</p> required <code>file</code> <code>object</code> <p>File where the warning was raised.</p> <code>None</code> <code>line</code> <code>str</code> <p>Line where the warning was raised.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/spotforecast2/exceptions.py</code> <pre><code>def rich_warning_handler(\n    message: str,\n    category: str,\n    filename: str,\n    lineno: str,\n    file: object = None,\n    line: str = None,\n) -&gt; None:\n    \"\"\"Custom handler for warnings that uses rich to display formatted panels.\n\n    Args:\n        message: Warning message.\n        category: Warning category.\n        filename: Filename where the warning was raised.\n        lineno: Line number where the warning was raised.\n        file: File where the warning was raised.\n        line: Line where the warning was raised.\n\n    Returns:\n        None\n    \"\"\"\n\n    if isinstance(message, tuple(warn_skforecast_categories)):\n        if not HAS_RICH:\n            # Fallback to format_warning_handler if rich is not available\n            format_warning_handler(message, category, filename, lineno, file, line)\n            return\n\n        console = Console()\n\n        category_name = category.__name__\n        text = (\n            f\"{message.message}\\n\\n\"\n            f\"Category : spotforecast2.exceptions.{category_name}\\n\"\n            f\"Location : {filename}:{lineno}\\n\"\n            f\"Suppress : warnings.simplefilter('ignore', category={category_name})\"\n        )\n\n        panel = Panel(\n            Text(text, justify=\"left\"),\n            title=category_name,\n            title_align=\"center\",\n            border_style=\"color(214)\",\n            width=88,\n        )\n\n        console.print(panel)\n    else:\n        # Fallback to default Python warning formatting\n        warnings._original_showwarning(message, category, filename, lineno, file, line)\n</code></pre>"},{"location":"api/forecaster/","title":"Forecaster Module","text":""},{"location":"api/forecaster/#metrics","title":"Metrics","text":""},{"location":"api/forecaster/#spotforecast2.forecaster.metrics","title":"<code>spotforecast2.forecaster.metrics</code>","text":"<p>Metrics for evaluating forecasting models.</p> <p>This module provides various metric functions for evaluating forecasting performance, including custom metrics like MASE, RMSSE, and probabilistic metrics like CRPS. These metrics are imported from the spotforecast2_safe package.</p>"},{"location":"api/forecaster/#spotforecast2.forecaster.metrics.add_y_train_argument","title":"<code>add_y_train_argument(func)</code>","text":"<p>Add <code>y_train</code> argument to a function if it is not already present.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>Function to which the argument is added.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>Function with <code>y_train</code> argument added.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; def my_metric(y_true, y_pred):\n...     return np.mean(np.abs(y_true - y_pred))\n&gt;&gt;&gt; enhanced_metric = add_y_train_argument(my_metric)\n&gt;&gt;&gt; # Now the function accepts y_train parameter\n&gt;&gt;&gt; result = enhanced_metric(np.array([1,2,3]), np.array([1,2,3]), y_train=None)\n</code></pre> Source code in <code>spotforecast2_safe/forecaster/metrics.py</code> <pre><code>def add_y_train_argument(func: Callable) -&gt; Callable:\n    \"\"\"Add `y_train` argument to a function if it is not already present.\n\n    Args:\n        func: Function to which the argument is added.\n\n    Returns:\n        Function with `y_train` argument added.\n\n    Examples:\n        &gt;&gt;&gt; def my_metric(y_true, y_pred):\n        ...     return np.mean(np.abs(y_true - y_pred))\n        &gt;&gt;&gt; enhanced_metric = add_y_train_argument(my_metric)\n        &gt;&gt;&gt; # Now the function accepts y_train parameter\n        &gt;&gt;&gt; result = enhanced_metric(np.array([1,2,3]), np.array([1,2,3]), y_train=None)\n    \"\"\"\n\n    sig = inspect.signature(func)\n\n    if \"y_train\" in sig.parameters:\n        return func\n\n    new_params = list(sig.parameters.values()) + [\n        inspect.Parameter(\"y_train\", inspect.Parameter.KEYWORD_ONLY, default=None)\n    ]\n    new_sig = sig.replace(parameters=new_params)\n\n    @wraps(func)\n    def wrapper(*args, y_train=None, **kwargs):\n        return func(*args, **kwargs)\n\n    wrapper.__signature__ = new_sig\n\n    return wrapper\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.metrics.calculate_coverage","title":"<code>calculate_coverage(y_true, lower_bound, upper_bound)</code>","text":"<p>Calculate coverage of a given interval.</p> <p>Coverage is the proportion of true values that fall within the interval.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray | Series</code> <p>True values of the target variable.</p> required <code>lower_bound</code> <code>ndarray | Series</code> <p>Lower bound of the interval.</p> required <code>upper_bound</code> <code>ndarray | Series</code> <p>Upper bound of the interval.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Coverage of the interval.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.metrics import calculate_coverage\n&gt;&gt;&gt; y_true = np.array([1, 2, 3, 4, 5])\n&gt;&gt;&gt; lower_bound = np.array([0.5, 1.5, 2.5, 3.5, 4.5])\n&gt;&gt;&gt; upper_bound = np.array([1.5, 2.5, 3.5, 4.5, 5.5])\n&gt;&gt;&gt; coverage = calculate_coverage(y_true, lower_bound, upper_bound)\n&gt;&gt;&gt; coverage == 1.0  # All values within bounds\nTrue\n</code></pre> Source code in <code>spotforecast2_safe/forecaster/metrics.py</code> <pre><code>def calculate_coverage(\n    y_true: np.ndarray | pd.Series,\n    lower_bound: np.ndarray | pd.Series,\n    upper_bound: np.ndarray | pd.Series,\n) -&gt; float:\n    \"\"\"Calculate coverage of a given interval.\n\n    Coverage is the proportion of true values that fall within the interval.\n\n    Args:\n        y_true: True values of the target variable.\n        lower_bound: Lower bound of the interval.\n        upper_bound: Upper bound of the interval.\n\n    Returns:\n        Coverage of the interval.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.metrics import calculate_coverage\n        &gt;&gt;&gt; y_true = np.array([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; lower_bound = np.array([0.5, 1.5, 2.5, 3.5, 4.5])\n        &gt;&gt;&gt; upper_bound = np.array([1.5, 2.5, 3.5, 4.5, 5.5])\n        &gt;&gt;&gt; coverage = calculate_coverage(y_true, lower_bound, upper_bound)\n        &gt;&gt;&gt; coverage == 1.0  # All values within bounds\n        True\n    \"\"\"\n    if not isinstance(y_true, (np.ndarray, pd.Series)) or y_true.ndim != 1:\n        raise TypeError(\"`y_true` must be a 1D numpy array or pandas Series.\")\n\n    if not isinstance(lower_bound, (np.ndarray, pd.Series)) or lower_bound.ndim != 1:\n        raise TypeError(\"`lower_bound` must be a 1D numpy array or pandas Series.\")\n\n    if not isinstance(upper_bound, (np.ndarray, pd.Series)) or upper_bound.ndim != 1:\n        raise TypeError(\"`upper_bound` must be a 1D numpy array or pandas Series.\")\n\n    y_true = np.asarray(y_true)\n    lower_bound = np.asarray(lower_bound)\n    upper_bound = np.asarray(upper_bound)\n\n    if y_true.shape != lower_bound.shape or y_true.shape != upper_bound.shape:\n        raise ValueError(\n            \"`y_true`, `lower_bound` and `upper_bound` must have the same shape.\"\n        )\n\n    coverage = np.mean(np.logical_and(y_true &gt;= lower_bound, y_true &lt;= upper_bound))\n\n    return coverage\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.metrics.create_mean_pinball_loss","title":"<code>create_mean_pinball_loss(alpha)</code>","text":"<p>Create pinball loss for a given quantile.</p> <p>Also known as quantile loss. Internally, it uses the <code>mean_pinball_loss</code> function from scikit-learn.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>Quantile for which the Pinball loss is calculated. Must be between 0 and 1, inclusive.</p> required <p>Returns:</p> Type Description <code>callable</code> <p>Mean Pinball loss function for the given quantile.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.metrics import create_mean_pinball_loss\n&gt;&gt;&gt; pinball_loss_50 = create_mean_pinball_loss(alpha=0.5)\n&gt;&gt;&gt; y_true = np.array([1, 2, 3, 4, 5])\n&gt;&gt;&gt; y_pred = np.array([1.1, 1.9, 3.2, 3.8, 5.1])\n&gt;&gt;&gt; loss = pinball_loss_50(y_true, y_pred)\n&gt;&gt;&gt; loss &gt;= 0\nTrue\n</code></pre> Source code in <code>spotforecast2_safe/forecaster/metrics.py</code> <pre><code>def create_mean_pinball_loss(alpha: float) -&gt; callable:\n    \"\"\"Create pinball loss for a given quantile.\n\n    Also known as quantile loss. Internally, it uses the `mean_pinball_loss`\n    function from scikit-learn.\n\n    Args:\n        alpha: Quantile for which the Pinball loss is calculated.\n            Must be between 0 and 1, inclusive.\n\n    Returns:\n        Mean Pinball loss function for the given quantile.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.metrics import create_mean_pinball_loss\n        &gt;&gt;&gt; pinball_loss_50 = create_mean_pinball_loss(alpha=0.5)\n        &gt;&gt;&gt; y_true = np.array([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; y_pred = np.array([1.1, 1.9, 3.2, 3.8, 5.1])\n        &gt;&gt;&gt; loss = pinball_loss_50(y_true, y_pred)\n        &gt;&gt;&gt; loss &gt;= 0\n        True\n    \"\"\"\n    if not (0 &lt;= alpha &lt;= 1):\n        raise ValueError(\"alpha must be between 0 and 1, both inclusive.\")\n\n    def mean_pinball_loss_q(y_true, y_pred):\n        return mean_pinball_loss(y_true, y_pred, alpha=alpha)\n\n    return mean_pinball_loss_q\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.metrics.crps_from_predictions","title":"<code>crps_from_predictions(y_true, y_pred)</code>","text":"<p>Compute the Continuous Ranked Probability Score (CRPS) from predictions.</p> <p>The CRPS compares the empirical distribution of a set of forecasted values to a scalar observation. The smaller the CRPS, the better.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>float</code> <p>The true value of the random variable.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted values of the random variable. These are the multiple forecasted values for a single observation.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The CRPS score.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.metrics import crps_from_predictions\n&gt;&gt;&gt; y_true = 5.0\n&gt;&gt;&gt; y_pred = np.array([4.5, 5.1, 4.9, 5.3, 4.7])\n&gt;&gt;&gt; crps = crps_from_predictions(y_true, y_pred)\n&gt;&gt;&gt; crps &gt;= 0\nTrue\n</code></pre> Source code in <code>spotforecast2_safe/forecaster/metrics.py</code> <pre><code>def crps_from_predictions(y_true: float, y_pred: np.ndarray) -&gt; float:\n    \"\"\"Compute the Continuous Ranked Probability Score (CRPS) from predictions.\n\n    The CRPS compares the empirical distribution of a set of forecasted values\n    to a scalar observation. The smaller the CRPS, the better.\n\n    Args:\n        y_true: The true value of the random variable.\n        y_pred: The predicted values of the random variable. These are the multiple\n            forecasted values for a single observation.\n\n    Returns:\n        The CRPS score.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.metrics import crps_from_predictions\n        &gt;&gt;&gt; y_true = 5.0\n        &gt;&gt;&gt; y_pred = np.array([4.5, 5.1, 4.9, 5.3, 4.7])\n        &gt;&gt;&gt; crps = crps_from_predictions(y_true, y_pred)\n        &gt;&gt;&gt; crps &gt;= 0\n        True\n    \"\"\"\n    if not isinstance(y_pred, np.ndarray) or y_pred.ndim != 1:\n        raise TypeError(\"`y_pred` must be a 1D numpy array.\")\n\n    if not isinstance(y_true, (float, int)):\n        raise TypeError(\"`y_true` must be a float or integer.\")\n\n    y_pred = np.sort(y_pred)\n    # Define the grid for integration including the true value\n    grid = np.concatenate(([y_true], y_pred))\n    grid = np.sort(grid)\n    cdf_values = np.searchsorted(y_pred, grid, side=\"right\") / len(y_pred)\n    indicator = grid &gt;= y_true\n    diffs = np.diff(grid)\n    crps = np.sum(diffs * (cdf_values[:-1] - indicator[:-1]) ** 2)\n\n    return crps\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.metrics.crps_from_quantiles","title":"<code>crps_from_quantiles(y_true, pred_quantiles, quantile_levels)</code>","text":"<p>Calculate the Continuous Ranked Probability Score (CRPS) from quantiles.</p> <p>The empirical cdf is approximated using linear interpolation between the predicted quantiles.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>float</code> <p>The true value of the random variable.</p> required <code>pred_quantiles</code> <code>ndarray</code> <p>The predicted quantile values.</p> required <code>quantile_levels</code> <code>ndarray</code> <p>The quantile levels corresponding to the predicted quantiles.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The CRPS score.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.metrics import crps_from_quantiles\n&gt;&gt;&gt; y_true = 5.0\n&gt;&gt;&gt; pred_quantiles = np.array([4.0, 4.5, 5.0, 5.5, 6.0])\n&gt;&gt;&gt; quantile_levels = np.array([0.1, 0.25, 0.5, 0.75, 0.9])\n&gt;&gt;&gt; crps = crps_from_quantiles(y_true, pred_quantiles, quantile_levels)\n&gt;&gt;&gt; crps &gt;= 0\nTrue\n</code></pre> Source code in <code>spotforecast2_safe/forecaster/metrics.py</code> <pre><code>def crps_from_quantiles(\n    y_true: float,\n    pred_quantiles: np.ndarray,\n    quantile_levels: np.ndarray,\n) -&gt; float:\n    \"\"\"Calculate the Continuous Ranked Probability Score (CRPS) from quantiles.\n\n    The empirical cdf is approximated using linear interpolation\n    between the predicted quantiles.\n\n    Args:\n        y_true: The true value of the random variable.\n        pred_quantiles: The predicted quantile values.\n        quantile_levels: The quantile levels corresponding to the predicted quantiles.\n\n    Returns:\n        The CRPS score.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.metrics import crps_from_quantiles\n        &gt;&gt;&gt; y_true = 5.0\n        &gt;&gt;&gt; pred_quantiles = np.array([4.0, 4.5, 5.0, 5.5, 6.0])\n        &gt;&gt;&gt; quantile_levels = np.array([0.1, 0.25, 0.5, 0.75, 0.9])\n        &gt;&gt;&gt; crps = crps_from_quantiles(y_true, pred_quantiles, quantile_levels)\n        &gt;&gt;&gt; crps &gt;= 0\n        True\n    \"\"\"\n    if not isinstance(y_true, (float, int)):\n        raise TypeError(\"`y_true` must be a float or integer.\")\n\n    if not isinstance(pred_quantiles, np.ndarray) or pred_quantiles.ndim != 1:\n        raise TypeError(\"`pred_quantiles` must be a 1D numpy array.\")\n\n    if not isinstance(quantile_levels, np.ndarray) or quantile_levels.ndim != 1:\n        raise TypeError(\"`quantile_levels` must be a 1D numpy array.\")\n\n    if len(pred_quantiles) != len(quantile_levels):\n        raise ValueError(\n            \"The number of predicted quantiles and quantile levels must be equal.\"\n        )\n\n    sorted_indices = np.argsort(pred_quantiles)\n    pred_quantiles = pred_quantiles[sorted_indices]\n    quantile_levels = quantile_levels[sorted_indices]\n\n    # Define the empirical CDF function using interpolation\n    def empirical_cdf(x):\n        return np.interp(x, pred_quantiles, quantile_levels, left=0.0, right=1.0)\n\n    # Define the CRPS integrand\n    def crps_integrand(x):\n        return (empirical_cdf(x) - (x &gt;= y_true)) ** 2\n\n    # Integration bounds: Extend slightly beyond predicted quantiles\n    xmin = np.min(pred_quantiles) * 0.9\n    xmax = np.max(pred_quantiles) * 1.1\n\n    # Create a fine grid of x values for integration\n    x_values = np.linspace(xmin, xmax, 1000)\n\n    # Compute the integrand values and integrate using the trapezoidal rule\n    integrand_values = crps_integrand(x_values)\n    if np.__version__ &gt;= \"2.0.0\":\n        crps = np.trapezoid(integrand_values, x=x_values)\n    else:\n        crps = np.trapz(integrand_values, x_values)\n\n    return crps\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.metrics.mean_absolute_scaled_error","title":"<code>mean_absolute_scaled_error(y_true, y_pred, y_train)</code>","text":"<p>Mean Absolute Scaled Error (MASE).</p> <p>MASE is a scale-independent error metric that measures the accuracy of a forecast. It is the mean absolute error of the forecast divided by the mean absolute error of a naive forecast in the training set. The naive forecast is the one obtained by shifting the time series by one period. If y_train is a list of numpy arrays or pandas Series, it is considered that each element is the true value of the target variable in the training set for each time series. In this case, the naive forecast is calculated for each time series separately.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray | Series</code> <p>True values of the target variable.</p> required <code>y_pred</code> <code>ndarray | Series</code> <p>Predicted values of the target variable.</p> required <code>y_train</code> <code>list[float] | ndarray | Series</code> <p>True values of the target variable in the training set. If <code>list</code>, it is consider that each element is the true value of the target variable in the training set for each time series.</p> required <p>Returns:</p> Type Description <code>float</code> <p>MASE value.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.metrics import mean_absolute_scaled_error\n&gt;&gt;&gt; y_train = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n&gt;&gt;&gt; y_true = np.array([9, 10, 11])\n&gt;&gt;&gt; y_pred = np.array([8.8, 10.2, 10.9])\n&gt;&gt;&gt; mase = mean_absolute_scaled_error(y_true, y_pred, y_train)\n&gt;&gt;&gt; mase &lt; 1.0  # Good forecast\nTrue\n</code></pre> Source code in <code>spotforecast2_safe/forecaster/metrics.py</code> <pre><code>def mean_absolute_scaled_error(\n    y_true: np.ndarray | pd.Series,\n    y_pred: np.ndarray | pd.Series,\n    y_train: list[float] | np.ndarray | pd.Series,\n) -&gt; float:\n    \"\"\"Mean Absolute Scaled Error (MASE).\n\n    MASE is a scale-independent error metric that measures the accuracy of\n    a forecast. It is the mean absolute error of the forecast divided by the\n    mean absolute error of a naive forecast in the training set. The naive\n    forecast is the one obtained by shifting the time series by one period.\n    If y_train is a list of numpy arrays or pandas Series, it is considered\n    that each element is the true value of the target variable in the training\n    set for each time series. In this case, the naive forecast is calculated\n    for each time series separately.\n\n    Args:\n        y_true: True values of the target variable.\n        y_pred: Predicted values of the target variable.\n        y_train: True values of the target variable in the training set. If `list`, it\n            is consider that each element is the true value of the target variable\n            in the training set for each time series.\n\n    Returns:\n        MASE value.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.metrics import mean_absolute_scaled_error\n        &gt;&gt;&gt; y_train = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n        &gt;&gt;&gt; y_true = np.array([9, 10, 11])\n        &gt;&gt;&gt; y_pred = np.array([8.8, 10.2, 10.9])\n        &gt;&gt;&gt; mase = mean_absolute_scaled_error(y_true, y_pred, y_train)\n        &gt;&gt;&gt; mase &lt; 1.0  # Good forecast\n        True\n    \"\"\"\n\n    # NOTE: When using this metric in validation, `y_train` doesn't include\n    # the first window_size observations used to create the predictors and/or\n    # rolling features.\n\n    if not isinstance(y_true, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_true` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_pred, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_pred` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_train, (list, pd.Series, np.ndarray)):\n        raise TypeError(\"`y_train` must be a list, pandas Series or numpy ndarray.\")\n    if isinstance(y_train, list):\n        for x in y_train:\n            if not isinstance(x, (pd.Series, np.ndarray)):\n                raise TypeError(\n                    \"When `y_train` is a list, each element must be a pandas Series \"\n                    \"or numpy ndarray.\"\n                )\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"`y_true` and `y_pred` must have the same length.\")\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError(\"`y_true` and `y_pred` must have at least one element.\")\n\n    if isinstance(y_train, list):\n        # Flatten list of arrays for naive forecast if meaningful, but MASE usually assumes\n        # naive forecast on single series. If list, we might be doing something else.\n        # Original code does: np.concatenate([np.diff(x) for x in y_train])\n        # This assumes independent series and we average error over all of them.\n        naive_forecast = np.concatenate([np.diff(x) for x in y_train])\n    else:\n        naive_forecast = np.diff(y_train)\n\n    mase = np.mean(np.abs(y_true - y_pred)) / np.nanmean(np.abs(naive_forecast))\n\n    return mase\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.metrics.root_mean_squared_scaled_error","title":"<code>root_mean_squared_scaled_error(y_true, y_pred, y_train)</code>","text":"<p>Root Mean Squared Scaled Error (RMSSE).</p> <p>RMSSE is a scale-independent error metric that measures the accuracy of a forecast. It is the root mean squared error of the forecast divided by the root mean squared error of a naive forecast in the training set. The naive forecast is the one obtained by shifting the time series by one period. If y_train is a list of numpy arrays or pandas Series, it is considered that each element is the true value of the target variable in the training set for each time series. In this case, the naive forecast is calculated for each time series separately.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray | Series</code> <p>True values of the target variable.</p> required <code>y_pred</code> <code>ndarray | Series</code> <p>Predicted values of the target variable.</p> required <code>y_train</code> <code>list[float] | ndarray | Series</code> <p>True values of the target variable in the training set. If list, it is consider that each element is the true value of the target variable in the training set for each time series.</p> required <p>Returns:</p> Type Description <code>float</code> <p>RMSSE value.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.metrics import root_mean_squared_scaled_error\n&gt;&gt;&gt; y_train = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n&gt;&gt;&gt; y_true = np.array([9, 10, 11])\n&gt;&gt;&gt; y_pred = np.array([8.8, 10.2, 10.9])\n&gt;&gt;&gt; rmsse = root_mean_squared_scaled_error(y_true, y_pred, y_train)\n&gt;&gt;&gt; rmsse &lt; 1.0  # Good forecast\nTrue\n</code></pre> Source code in <code>spotforecast2_safe/forecaster/metrics.py</code> <pre><code>def root_mean_squared_scaled_error(\n    y_true: np.ndarray | pd.Series,\n    y_pred: np.ndarray | pd.Series,\n    y_train: list[float] | np.ndarray | pd.Series,\n) -&gt; float:\n    \"\"\"Root Mean Squared Scaled Error (RMSSE).\n\n    RMSSE is a scale-independent error metric that measures the accuracy of\n    a forecast. It is the root mean squared error of the forecast divided by\n    the root mean squared error of a naive forecast in the training set. The\n    naive forecast is the one obtained by shifting the time series by one period.\n    If y_train is a list of numpy arrays or pandas Series, it is considered\n    that each element is the true value of the target variable in the training\n    set for each time series. In this case, the naive forecast is calculated\n    for each time series separately.\n\n    Args:\n        y_true: True values of the target variable.\n        y_pred: Predicted values of the target variable.\n        y_train: True values of the target variable in the training set. If list, it\n            is consider that each element is the true value of the target variable\n            in the training set for each time series.\n\n    Returns:\n        RMSSE value.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.metrics import root_mean_squared_scaled_error\n        &gt;&gt;&gt; y_train = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n        &gt;&gt;&gt; y_true = np.array([9, 10, 11])\n        &gt;&gt;&gt; y_pred = np.array([8.8, 10.2, 10.9])\n        &gt;&gt;&gt; rmsse = root_mean_squared_scaled_error(y_true, y_pred, y_train)\n        &gt;&gt;&gt; rmsse &lt; 1.0  # Good forecast\n        True\n    \"\"\"\n\n    # NOTE: When using this metric in validation, `y_train` doesn't include\n    # the first window_size observations used to create the predictors and/or\n    # rolling features.\n\n    if not isinstance(y_true, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_true` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_pred, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_pred` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_train, (list, pd.Series, np.ndarray)):\n        raise TypeError(\"`y_train` must be a list, pandas Series or numpy ndarray.\")\n    if isinstance(y_train, list):\n        for x in y_train:\n            if not isinstance(x, (pd.Series, np.ndarray)):\n                raise TypeError(\n                    \"When `y_train` is a list, each element must be a pandas Series \"\n                    \"or numpy ndarray.\"\n                )\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"`y_true` and `y_pred` must have the same length.\")\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError(\"`y_true` and `y_pred` must have at least one element.\")\n\n    if isinstance(y_train, list):\n        naive_forecast = np.concatenate([np.diff(x) for x in y_train])\n    else:\n        naive_forecast = np.diff(y_train)\n\n    rmsse = np.sqrt(np.mean((y_true - y_pred) ** 2)) / np.sqrt(\n        np.nanmean(naive_forecast**2)\n    )\n\n    return rmsse\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.metrics.symmetric_mean_absolute_percentage_error","title":"<code>symmetric_mean_absolute_percentage_error(y_true, y_pred)</code>","text":"<p>Compute the Symmetric Mean Absolute Percentage Error (SMAPE).</p> <p>SMAPE is a relative error metric used to measure the accuracy of forecasts. Unlike MAPE, it is symmetric and prevents division by zero by averaging the absolute values of actual and predicted values.</p> <p>The result is expressed as a percentage and ranges from 0% (perfect prediction) to 200% (maximum error).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray | Series</code> <p>True values of the target variable.</p> required <code>y_pred</code> <code>ndarray | Series</code> <p>Predicted values of the target variable.</p> required <p>Returns:</p> Type Description <code>float</code> <p>SMAPE value as a percentage.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.metrics import symmetric_mean_absolute_percentage_error\n&gt;&gt;&gt; y_true = np.array([100, 200, 0])\n&gt;&gt;&gt; y_pred = np.array([110, 180, 10])\n&gt;&gt;&gt; result = symmetric_mean_absolute_percentage_error(y_true, y_pred)\n&gt;&gt;&gt; 0 &lt;= result &lt;= 200\nTrue\n</code></pre> Source code in <code>spotforecast2_safe/forecaster/metrics.py</code> <pre><code>def symmetric_mean_absolute_percentage_error(\n    y_true: np.ndarray | pd.Series, y_pred: np.ndarray | pd.Series\n) -&gt; float:\n    \"\"\"Compute the Symmetric Mean Absolute Percentage Error (SMAPE).\n\n    SMAPE is a relative error metric used to measure the accuracy\n    of forecasts. Unlike MAPE, it is symmetric and prevents division\n    by zero by averaging the absolute values of actual and predicted values.\n\n    The result is expressed as a percentage and ranges from 0%\n    (perfect prediction) to 200% (maximum error).\n\n    Args:\n        y_true: True values of the target variable.\n        y_pred: Predicted values of the target variable.\n\n    Returns:\n        SMAPE value as a percentage.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.metrics import symmetric_mean_absolute_percentage_error\n        &gt;&gt;&gt; y_true = np.array([100, 200, 0])\n        &gt;&gt;&gt; y_pred = np.array([110, 180, 10])\n        &gt;&gt;&gt; result = symmetric_mean_absolute_percentage_error(y_true, y_pred)\n        &gt;&gt;&gt; 0 &lt;= result &lt;= 200\n        True\n    \"\"\"\n\n    if not isinstance(y_true, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_true` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_pred, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_pred` must be a pandas Series or numpy ndarray.\")\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"`y_true` and `y_pred` must have the same length.\")\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError(\"`y_true` and `y_pred` must have at least one element.\")\n\n    numerator = np.abs(y_true - y_pred)\n    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n\n    # NOTE: Avoid division by zero\n    mask = denominator != 0\n    smape_values = np.zeros_like(denominator)\n    smape_values[mask] = numerator[mask] / denominator[mask]\n\n    smape = 100 * np.mean(smape_values)\n\n    return smape\n</code></pre>"},{"location":"api/forecaster/#utils","title":"Utils","text":""},{"location":"api/forecaster/#spotforecast2.forecaster.utils","title":"<code>spotforecast2.forecaster.utils</code>","text":""},{"location":"api/forecaster/#spotforecast2.forecaster.utils.check_exog","title":"<code>check_exog(exog, allow_nan=True, series_id='`exog`')</code>","text":"<p>Validate that exog is a pandas Series or DataFrame.</p> <p>This function ensures that exogenous variables meet basic requirements: - Must be a pandas Series or DataFrame - If Series, must have a name - Optionally warns if NaN values are present</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variable/s included as predictor/s.</p> required <code>allow_nan</code> <code>bool</code> <p>If True, allows NaN values but issues a warning. If False, raises no warning about NaN values. Defaults to True.</p> <code>True</code> <code>series_id</code> <code>str</code> <p>Identifier of the series used in error messages. Defaults to \"<code>exog</code>\".</p> <code>'`exog`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If exog is not a pandas Series or DataFrame.</p> <code>ValueError</code> <p>If exog is a Series without a name.</p> <p>Warns:</p> Type Description <code>MissingValuesWarning</code> <p>If allow_nan=True and exog contains NaN values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_exog\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid DataFrame\n&gt;&gt;&gt; exog_df = pd.DataFrame({\"temp\": [20, 21, 22], \"humidity\": [50, 55, 60]})\n&gt;&gt;&gt; check_exog(exog_df)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid Series with name\n&gt;&gt;&gt; exog_series = pd.Series([1, 2, 3], name=\"temperature\")\n&gt;&gt;&gt; check_exog(exog_series)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: Series without name\n&gt;&gt;&gt; exog_no_name = pd.Series([1, 2, 3])\n&gt;&gt;&gt; try:\n...     check_exog(exog_no_name)\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: When `exog` is a pandas Series, it must have a name.\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not a Series/DataFrame\n&gt;&gt;&gt; try:\n...     check_exog([1, 2, 3])\n... except TypeError as e:\n...     print(f\"Error: {e}\")\nError: `exog` must be a pandas Series or DataFrame. Got &lt;class 'list'&gt;.\n</code></pre> Source code in <code>src/spotforecast2/utils/validation.py</code> <pre><code>def check_exog(\n    exog: Union[pd.Series, pd.DataFrame],\n    allow_nan: bool = True,\n    series_id: str = \"`exog`\",\n) -&gt; None:\n    \"\"\"\n    Validate that exog is a pandas Series or DataFrame.\n\n    This function ensures that exogenous variables meet basic requirements:\n    - Must be a pandas Series or DataFrame\n    - If Series, must have a name\n    - Optionally warns if NaN values are present\n\n    Args:\n        exog: Exogenous variable/s included as predictor/s.\n        allow_nan: If True, allows NaN values but issues a warning. If False,\n            raises no warning about NaN values. Defaults to True.\n        series_id: Identifier of the series used in error messages. Defaults to \"`exog`\".\n\n    Raises:\n        TypeError: If exog is not a pandas Series or DataFrame.\n        ValueError: If exog is a Series without a name.\n\n    Warnings:\n        MissingValuesWarning: If allow_nan=True and exog contains NaN values.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_exog\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid DataFrame\n        &gt;&gt;&gt; exog_df = pd.DataFrame({\"temp\": [20, 21, 22], \"humidity\": [50, 55, 60]})\n        &gt;&gt;&gt; check_exog(exog_df)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid Series with name\n        &gt;&gt;&gt; exog_series = pd.Series([1, 2, 3], name=\"temperature\")\n        &gt;&gt;&gt; check_exog(exog_series)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: Series without name\n        &gt;&gt;&gt; exog_no_name = pd.Series([1, 2, 3])\n        &gt;&gt;&gt; try:\n        ...     check_exog(exog_no_name)\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: When `exog` is a pandas Series, it must have a name.\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not a Series/DataFrame\n        &gt;&gt;&gt; try:\n        ...     check_exog([1, 2, 3])\n        ... except TypeError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `exog` must be a pandas Series or DataFrame. Got &lt;class 'list'&gt;.\n    \"\"\"\n    if not isinstance(exog, (pd.Series, pd.DataFrame)):\n        raise TypeError(\n            f\"{series_id} must be a pandas Series or DataFrame. Got {type(exog)}.\"\n        )\n\n    if isinstance(exog, pd.Series) and exog.name is None:\n        raise ValueError(f\"When {series_id} is a pandas Series, it must have a name.\")\n\n    if not allow_nan:\n        if exog.isna().to_numpy().any():\n            warnings.warn(\n                f\"{series_id} has missing values. Most machine learning models \"\n                f\"do not allow missing values. Fitting the forecaster may fail.\",\n                MissingValuesWarning,\n            )\n\n    return\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.utils.check_exog_dtypes","title":"<code>check_exog_dtypes(exog, call_check_exog=True, series_id='`exog`')</code>","text":"<p>Check that exogenous variables have valid data types (int, float, category).</p> <p>This function validates that the exogenous variables (Series or DataFrame) contain only supported data types: integer, float, or category. It issues a warning if other types (like object/string) are found, as these may cause issues with some machine learning estimators.</p> <p>It also strictly enforces that categorical columns must have integer categories.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variables to check.</p> required <code>call_check_exog</code> <code>bool</code> <p>If True, calls check_exog() first to ensure basic validity. Defaults to True.</p> <code>True</code> <code>series_id</code> <code>str</code> <p>Identifier used in warning/error messages. Defaults to \"<code>exog</code>\".</p> <code>'`exog`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If categorical columns contain non-integer categories.</p> <p>Warns:</p> Type Description <code>DataTypeWarning</code> <p>If columns with unsupported data types (not int, float, category) are found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_exog_dtypes\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid types (float, int)\n&gt;&gt;&gt; df_valid = pd.DataFrame({\n...     \"a\": [1.0, 2.0, 3.0],\n...     \"b\": [1, 2, 3]\n... })\n&gt;&gt;&gt; check_exog_dtypes(df_valid)  # No warning\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid type (object/string)\n&gt;&gt;&gt; df_invalid = pd.DataFrame({\n...     \"a\": [1, 2, 3],\n...     \"b\": [\"x\", \"y\", \"z\"]\n... })\n&gt;&gt;&gt; check_exog_dtypes(df_invalid)\n... # Issues DataTypeWarning about column 'b'\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid categorical (with integer categories)\n&gt;&gt;&gt; df_cat = pd.DataFrame({\"a\": [1, 2, 1]})\n&gt;&gt;&gt; df_cat[\"a\"] = df_cat[\"a\"].astype(\"category\")\n&gt;&gt;&gt; check_exog_dtypes(df_cat)  # No warning\n</code></pre> Source code in <code>src/spotforecast2/utils/validation.py</code> <pre><code>def check_exog_dtypes(\n    exog: Union[pd.Series, pd.DataFrame],\n    call_check_exog: bool = True,\n    series_id: str = \"`exog`\",\n) -&gt; None:\n    \"\"\"\n    Check that exogenous variables have valid data types (int, float, category).\n\n    This function validates that the exogenous variables (Series or DataFrame)\n    contain only supported data types: integer, float, or category. It issues a\n    warning if other types (like object/string) are found, as these may cause\n    issues with some machine learning estimators.\n\n    It also strictly enforces that categorical columns must have integer categories.\n\n    Args:\n        exog: Exogenous variables to check.\n        call_check_exog: If True, calls check_exog() first to ensure basic validity.\n            Defaults to True.\n        series_id: Identifier used in warning/error messages. Defaults to \"`exog`\".\n\n    Raises:\n        TypeError: If categorical columns contain non-integer categories.\n\n    Warnings:\n        DataTypeWarning: If columns with unsupported data types (not int, float, category)\n            are found.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_exog_dtypes\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid types (float, int)\n        &gt;&gt;&gt; df_valid = pd.DataFrame({\n        ...     \"a\": [1.0, 2.0, 3.0],\n        ...     \"b\": [1, 2, 3]\n        ... })\n        &gt;&gt;&gt; check_exog_dtypes(df_valid)  # No warning\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid type (object/string)\n        &gt;&gt;&gt; df_invalid = pd.DataFrame({\n        ...     \"a\": [1, 2, 3],\n        ...     \"b\": [\"x\", \"y\", \"z\"]\n        ... })\n        &gt;&gt;&gt; check_exog_dtypes(df_invalid)\n        ... # Issues DataTypeWarning about column 'b'\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid categorical (with integer categories)\n        &gt;&gt;&gt; df_cat = pd.DataFrame({\"a\": [1, 2, 1]})\n        &gt;&gt;&gt; df_cat[\"a\"] = df_cat[\"a\"].astype(\"category\")\n        &gt;&gt;&gt; check_exog_dtypes(df_cat)  # No warning\n    \"\"\"\n    if call_check_exog:\n        check_exog(exog=exog, allow_nan=False, series_id=series_id)\n\n    valid_dtypes = (\"int\", \"Int\", \"float\", \"Float\", \"uint\")\n\n    if isinstance(exog, pd.DataFrame):\n        unique_dtypes = set(exog.dtypes)\n        has_invalid_dtype = False\n        for dtype in unique_dtypes:\n            if isinstance(dtype, pd.CategoricalDtype):\n                try:\n                    is_integer = np.issubdtype(dtype.categories.dtype, np.integer)\n                except TypeError:\n                    # Pandas StringDtype and other non-numpy dtypes will raise TypeError\n                    is_integer = False\n\n                if not is_integer:\n                    raise TypeError(\n                        \"Categorical dtypes in exog must contain only integer values. \"\n                    )\n            elif not dtype.name.startswith(valid_dtypes):\n                has_invalid_dtype = True\n\n        if has_invalid_dtype:\n            warnings.warn(\n                f\"{series_id} may contain only `int`, `float` or `category` dtypes. \"\n                f\"Most machine learning models do not allow other types of values. \"\n                f\"Fitting the forecaster may fail.\",\n                DataTypeWarning,\n            )\n\n    else:\n        dtype_name = str(exog.dtypes)\n        if not (dtype_name.startswith(valid_dtypes) or dtype_name == \"category\"):\n            warnings.warn(\n                f\"{series_id} may contain only `int`, `float` or `category` dtypes. Most \"\n                f\"machine learning models do not allow other types of values. \"\n                f\"Fitting the forecaster may fail.\",\n                DataTypeWarning,\n            )\n\n        if isinstance(exog.dtype, pd.CategoricalDtype):\n            if not np.issubdtype(exog.cat.categories.dtype, np.integer):\n                raise TypeError(\n                    \"Categorical dtypes in exog must contain only integer values. \"\n                )\n    return\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.utils.check_extract_values_and_index","title":"<code>check_extract_values_and_index(data, data_label='`y`', ignore_freq=False, return_values=True)</code>","text":"<p>Extract values and index from a pandas Series or DataFrame, ensuring they are valid.</p> <p>Validates that the input data has a proper DatetimeIndex or RangeIndex and extracts its values and index for use in forecasting operations. Optionally checks for index frequency consistency.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Series, DataFrame]</code> <p>Input data (pandas Series or DataFrame) to extract values and index from.</p> required <code>data_label</code> <code>str</code> <p>Label used in exception messages for better error reporting. Defaults to \"<code>y</code>\".</p> <code>'`y`'</code> <code>ignore_freq</code> <code>bool</code> <p>If True, the frequency of the index is not checked. Defaults to False.</p> <code>False</code> <code>return_values</code> <code>bool</code> <p>If True, the values of the data are returned. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[Optional[ndarray], Index]</code> <p>A tuple containing: - values (numpy.ndarray or None): Values of the data as numpy array,   or None if return_values is False. - index (pandas.Index): Index of the data.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If data is not a pandas Series or DataFrame.</p> <code>TypeError</code> <p>If data index is not a DatetimeIndex or RangeIndex.</p> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If DatetimeIndex has no frequency (inferred automatically).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; dates = pd.date_range('2020-01-01', periods=10, freq='D')\n&gt;&gt;&gt; series = pd.Series(np.arange(10), index=dates)\n&gt;&gt;&gt; values, index = check_extract_values_and_index(series)\n&gt;&gt;&gt; print(values.shape)\n(10,)\n&gt;&gt;&gt; print(type(index))\n&lt;class 'pandas.core.indexes.datetimes.DatetimeIndex'&gt;\n</code></pre> <p>Extract index only:</p> <pre><code>&gt;&gt;&gt; _, index = check_extract_values_and_index(series, return_values=False)\n&gt;&gt;&gt; print(index[0])\n2020-01-01 00:00:00\n</code></pre> Source code in <code>spotforecast2_safe/forecaster/utils.py</code> <pre><code>def check_extract_values_and_index(\n    data: Union[pd.Series, pd.DataFrame],\n    data_label: str = \"`y`\",\n    ignore_freq: bool = False,\n    return_values: bool = True,\n) -&gt; Tuple[Optional[np.ndarray], pd.Index]:\n    \"\"\"Extract values and index from a pandas Series or DataFrame, ensuring they are valid.\n\n    Validates that the input data has a proper DatetimeIndex or RangeIndex and extracts\n    its values and index for use in forecasting operations. Optionally checks for\n    index frequency consistency.\n\n    Args:\n        data: Input data (pandas Series or DataFrame) to extract values and index from.\n        data_label: Label used in exception messages for better error reporting.\n            Defaults to \"`y`\".\n        ignore_freq: If True, the frequency of the index is not checked.\n            Defaults to False.\n        return_values: If True, the values of the data are returned.\n            Defaults to True.\n\n    Returns:\n        tuple: A tuple containing:\n            - values (numpy.ndarray or None): Values of the data as numpy array,\n              or None if return_values is False.\n            - index (pandas.Index): Index of the data.\n\n    Raises:\n        TypeError: If data is not a pandas Series or DataFrame.\n        TypeError: If data index is not a DatetimeIndex or RangeIndex.\n\n    Warnings:\n        UserWarning: If DatetimeIndex has no frequency (inferred automatically).\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; dates = pd.date_range('2020-01-01', periods=10, freq='D')\n        &gt;&gt;&gt; series = pd.Series(np.arange(10), index=dates)\n        &gt;&gt;&gt; values, index = check_extract_values_and_index(series)\n        &gt;&gt;&gt; print(values.shape)\n        (10,)\n        &gt;&gt;&gt; print(type(index))\n        &lt;class 'pandas.core.indexes.datetimes.DatetimeIndex'&gt;\n\n        Extract index only:\n        &gt;&gt;&gt; _, index = check_extract_values_and_index(series, return_values=False)\n        &gt;&gt;&gt; print(index[0])\n        2020-01-01 00:00:00\n    \"\"\"\n\n    if not isinstance(data, (pd.Series, pd.DataFrame)):\n        raise TypeError(f\"{data_label} must be a pandas Series or DataFrame.\")\n\n    if not isinstance(data.index, (pd.DatetimeIndex, pd.RangeIndex)):\n        raise TypeError(f\"{data_label} must have a pandas DatetimeIndex or RangeIndex.\")\n\n    if isinstance(data.index, pd.DatetimeIndex) and not ignore_freq:\n        if data.index.freq is None:\n            warnings.warn(\n                f\"{data_label} has a DatetimeIndex but no frequency. \"\n                \"The frequency has been inferred from the index.\",\n                UserWarning,\n            )\n\n    values = data.to_numpy() if return_values else None\n\n    return values, data.index\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.utils.check_interval","title":"<code>check_interval(interval=None, ensure_symmetric_intervals=False, quantiles=None, alpha=None, alpha_literal='alpha')</code>","text":"<p>Validate that a confidence interval specification is valid.</p> <p>This function checks that interval values are properly formatted and within valid ranges for confidence interval prediction.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>Union[List[float], Tuple[float], None]</code> <p>Confidence interval percentiles (0-100 inclusive). Should be [lower_bound, upper_bound]. Example: [2.5, 97.5] for 95% interval.</p> <code>None</code> <code>ensure_symmetric_intervals</code> <code>bool</code> <p>If True, ensure intervals are symmetric (lower + upper = 100).</p> <code>False</code> <code>quantiles</code> <code>Union[List[float], Tuple[float], None]</code> <p>Sequence of quantiles (0-1 inclusive). Currently not validated, reserved for future use.</p> <code>None</code> <code>alpha</code> <code>Optional[float]</code> <p>Confidence level (1-alpha). Currently not validated, reserved for future use.</p> <code>None</code> <code>alpha_literal</code> <code>Optional[str]</code> <p>Name used in error messages for alpha parameter.</p> <code>'alpha'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If interval is not a list or tuple.</p> <code>ValueError</code> <p>If interval doesn't have exactly 2 values, values out of range (0-100), lower &gt;= upper, or intervals not symmetric when required.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.utils.validation import check_interval\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid 95% confidence interval\n&gt;&gt;&gt; check_interval(interval=[2.5, 97.5])  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid symmetric interval\n&gt;&gt;&gt; check_interval(interval=[2.5, 97.5], ensure_symmetric_intervals=True)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not symmetric\n&gt;&gt;&gt; try:\n...     check_interval(interval=[5, 90], ensure_symmetric_intervals=True)\n... except ValueError as e:\n...     print(\"Error: Interval not symmetric\")\nError: Interval not symmetric\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: wrong number of values\n&gt;&gt;&gt; try:\n...     check_interval(interval=[2.5, 50, 97.5])\n... except ValueError as e:\n...     print(\"Error: Must have exactly 2 values\")\nError: Must have exactly 2 values\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: out of range\n&gt;&gt;&gt; try:\n...     check_interval(interval=[-5, 105])\n... except ValueError as e:\n...     print(\"Error: Values out of range\")\nError: Values out of range\n</code></pre> Source code in <code>src/spotforecast2/utils/validation.py</code> <pre><code>def check_interval(\n    interval: Union[List[float], Tuple[float], None] = None,\n    ensure_symmetric_intervals: bool = False,\n    quantiles: Union[List[float], Tuple[float], None] = None,\n    alpha: Optional[float] = None,\n    alpha_literal: Optional[str] = \"alpha\",\n) -&gt; None:\n    \"\"\"\n    Validate that a confidence interval specification is valid.\n\n    This function checks that interval values are properly formatted and within\n    valid ranges for confidence interval prediction.\n\n    Args:\n        interval: Confidence interval percentiles (0-100 inclusive).\n            Should be [lower_bound, upper_bound]. Example: [2.5, 97.5] for 95% interval.\n        ensure_symmetric_intervals: If True, ensure intervals are symmetric\n            (lower + upper = 100).\n        quantiles: Sequence of quantiles (0-1 inclusive). Currently not validated,\n            reserved for future use.\n        alpha: Confidence level (1-alpha). Currently not validated, reserved for future use.\n        alpha_literal: Name used in error messages for alpha parameter.\n\n    Raises:\n        TypeError: If interval is not a list or tuple.\n        ValueError: If interval doesn't have exactly 2 values, values out of range (0-100),\n            lower &gt;= upper, or intervals not symmetric when required.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_interval\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid 95% confidence interval\n        &gt;&gt;&gt; check_interval(interval=[2.5, 97.5])  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid symmetric interval\n        &gt;&gt;&gt; check_interval(interval=[2.5, 97.5], ensure_symmetric_intervals=True)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not symmetric\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[5, 90], ensure_symmetric_intervals=True)\n        ... except ValueError as e:\n        ...     print(\"Error: Interval not symmetric\")\n        Error: Interval not symmetric\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: wrong number of values\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[2.5, 50, 97.5])\n        ... except ValueError as e:\n        ...     print(\"Error: Must have exactly 2 values\")\n        Error: Must have exactly 2 values\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: out of range\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[-5, 105])\n        ... except ValueError as e:\n        ...     print(\"Error: Values out of range\")\n        Error: Values out of range\n    \"\"\"\n    if interval is not None:\n        if not isinstance(interval, (list, tuple)):\n            raise TypeError(\n                \"`interval` must be a `list` or `tuple`. For example, interval of 95% \"\n                \"should be as `interval = [2.5, 97.5]`.\"\n            )\n\n        if len(interval) != 2:\n            raise ValueError(\n                \"`interval` must contain exactly 2 values, respectively the \"\n                \"lower and upper interval bounds. For example, interval of 95% \"\n                \"should be as `interval = [2.5, 97.5]`.\"\n            )\n\n        if (interval[0] &lt; 0.0) or (interval[0] &gt;= 100.0):\n            raise ValueError(\n                f\"Lower interval bound ({interval[0]}) must be &gt;= 0 and &lt; 100.\"\n            )\n\n        if (interval[1] &lt;= 0.0) or (interval[1] &gt; 100.0):\n            raise ValueError(\n                f\"Upper interval bound ({interval[1]}) must be &gt; 0 and &lt;= 100.\"\n            )\n\n        if interval[0] &gt;= interval[1]:\n            raise ValueError(\n                f\"Lower interval bound ({interval[0]}) must be less than the \"\n                f\"upper interval bound ({interval[1]}).\"\n            )\n\n        if ensure_symmetric_intervals and interval[0] + interval[1] != 100:\n            raise ValueError(\n                f\"Interval must be symmetric, the sum of the lower, ({interval[0]}), \"\n                f\"and upper, ({interval[1]}), interval bounds must be equal to \"\n                f\"100. Got {interval[0] + interval[1]}.\"\n            )\n\n    return\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.utils.check_optional_dependency","title":"<code>check_optional_dependency(package_name)</code>","text":"<p>Check if an optional dependency is installed, if not raise an ImportError with installation instructions.</p> <p>Parameters:</p> Name Type Description Default <code>package_name</code> <code>str</code> <p>Name of the package to check.</p> required <p>Raises:</p> Type Description <code>ImportError</code> <p>If the package is not installed.</p> Source code in <code>src/spotforecast2/forecaster/utils.py</code> <pre><code>def check_optional_dependency(package_name: str) -&gt; None:\n    \"\"\"\n    Check if an optional dependency is installed, if not raise an ImportError\n    with installation instructions.\n\n    Args:\n        package_name (str): Name of the package to check.\n\n    Raises:\n        ImportError: If the package is not installed.\n    \"\"\"\n\n    if find_spec(package_name) is None:\n        try:\n            extra, package_version = _find_optional_dependency(\n                package_name=package_name\n            )\n            msg = f\"\\n'{package_name}' is an optional dependency not included in the default spotforecast installation.\"\n        except Exception:\n            msg = f\"\\n'{package_name}' is needed but not installed. Please install it.\"\n\n        raise ImportError(msg)\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.utils.check_predict_input","title":"<code>check_predict_input(forecaster_name, steps, is_fitted, exog_in_, index_type_, index_freq_, window_size, last_window, last_window_exog=None, exog=None, exog_names_in_=None, interval=None, alpha=None, max_step=None, levels=None, levels_forecaster=None, series_names_in_=None, encoding=None)</code>","text":"<p>Check all inputs of predict method. This is a helper function to validate that inputs used in predict method match attributes of a forecaster already trained.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>str Forecaster name.</p> required <code>steps</code> <code>Union[int, List[int]]</code> <p>int, list Number of future steps predicted.</p> required <code>is_fitted</code> <code>bool</code> <p>bool Tag to identify if the estimator has been fitted (trained).</p> required <code>exog_in_</code> <code>bool</code> <p>bool If the forecaster has been trained using exogenous variable/s.</p> required <code>index_type_</code> <code>type</code> <p>type Type of index of the input used in training.</p> required <code>index_freq_</code> <code>str</code> <p>str Frequency of Index of the input used in training.</p> required <code>window_size</code> <code>int</code> <p>int Size of the window needed to create the predictors. It is equal to <code>max_lag</code>.</p> required <code>last_window</code> <code>Optional[Union[Series, DataFrame]]</code> <p>pandas Series, pandas DataFrame, None Values of the series used to create the predictors (lags) need in the first iteration of prediction (t + 1).</p> required <code>last_window_exog</code> <code>Optional[Union[Series, DataFrame]]</code> <p>pandas Series, pandas DataFrame, default None Values of the exogenous variables aligned with <code>last_window</code> in ForecasterStats predictions.</p> <code>None</code> <code>exog</code> <code>Optional[Union[Series, DataFrame, Dict[str, Union[Series, DataFrame]]]]</code> <p>pandas Series, pandas DataFrame, dict, default None Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>exog_names_in_</code> <code>Optional[List[str]]</code> <p>list, default None Names of the exogenous variables used during training.</p> <code>None</code> <code>interval</code> <code>Optional[List[float]]</code> <p>list, tuple, default None Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. For example, interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>None</code> <code>alpha</code> <code>Optional[float]</code> <p>float, default None The confidence intervals used in ForecasterStats are (1 - alpha) %.</p> <code>None</code> <code>max_step</code> <code>Optional[int]</code> <p>int, default None Maximum number of steps allowed (<code>ForecasterDirect</code> and <code>ForecasterDirectMultiVariate</code>).</p> <code>None</code> <code>levels</code> <code>Optional[Union[str, List[str]]]</code> <p>str, list, default None Time series to be predicted (<code>ForecasterRecursiveMultiSeries</code> and `ForecasterRnn).</p> <code>None</code> <code>levels_forecaster</code> <code>Optional[Union[str, List[str]]]</code> <p>str, list, default None Time series used as output data of a multiseries problem in a RNN problem (<code>ForecasterRnn</code>).</p> <code>None</code> <code>series_names_in_</code> <code>Optional[List[str]]</code> <p>list, default None Names of the columns used during fit (<code>ForecasterRecursiveMultiSeries</code>, <code>ForecasterDirectMultiVariate</code> and <code>ForecasterRnn</code>).</p> <code>None</code> <code>encoding</code> <code>Optional[str]</code> <p>str, default None Encoding used to identify the different series (<code>ForecasterRecursiveMultiSeries</code>).</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spotforecast2_safe/utils/validation.py</code> <pre><code>def check_predict_input(\n    forecaster_name: str,\n    steps: Union[int, List[int]],\n    is_fitted: bool,\n    exog_in_: bool,\n    index_type_: type,\n    index_freq_: str,\n    window_size: int,\n    last_window: Optional[Union[pd.Series, pd.DataFrame]],\n    last_window_exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    exog: Optional[\n        Union[pd.Series, pd.DataFrame, Dict[str, Union[pd.Series, pd.DataFrame]]]\n    ] = None,\n    exog_names_in_: Optional[List[str]] = None,\n    interval: Optional[List[float]] = None,\n    alpha: Optional[float] = None,\n    max_step: Optional[int] = None,\n    levels: Optional[Union[str, List[str]]] = None,\n    levels_forecaster: Optional[Union[str, List[str]]] = None,\n    series_names_in_: Optional[List[str]] = None,\n    encoding: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Check all inputs of predict method. This is a helper function to validate\n    that inputs used in predict method match attributes of a forecaster already\n    trained.\n\n    Args:\n        forecaster_name: str\n            Forecaster name.\n        steps: int, list\n            Number of future steps predicted.\n        is_fitted: bool\n            Tag to identify if the estimator has been fitted (trained).\n        exog_in_: bool\n            If the forecaster has been trained using exogenous variable/s.\n        index_type_: type\n            Type of index of the input used in training.\n        index_freq_: str\n            Frequency of Index of the input used in training.\n        window_size: int\n            Size of the window needed to create the predictors. It is equal to\n            `max_lag`.\n        last_window: pandas Series, pandas DataFrame, None\n            Values of the series used to create the predictors (lags) need in the\n            first iteration of prediction (t + 1).\n        last_window_exog: pandas Series, pandas DataFrame, default None\n            Values of the exogenous variables aligned with `last_window` in\n            ForecasterStats predictions.\n        exog: pandas Series, pandas DataFrame, dict, default None\n            Exogenous variable/s included as predictor/s.\n        exog_names_in_: list, default None\n            Names of the exogenous variables used during training.\n        interval: list, tuple, default None\n            Confidence of the prediction interval estimated. Sequence of percentiles\n            to compute, which must be between 0 and 100 inclusive. For example,\n            interval of 95% should be as `interval = [2.5, 97.5]`.\n        alpha: float, default None\n            The confidence intervals used in ForecasterStats are (1 - alpha) %.\n        max_step: int, default None\n            Maximum number of steps allowed (`ForecasterDirect` and\n            `ForecasterDirectMultiVariate`).\n        levels: str, list, default None\n            Time series to be predicted (`ForecasterRecursiveMultiSeries`\n            and `ForecasterRnn).\n        levels_forecaster: str, list, default None\n            Time series used as output data of a multiseries problem in a RNN problem\n            (`ForecasterRnn`).\n        series_names_in_: list, default None\n            Names of the columns used during fit (`ForecasterRecursiveMultiSeries`,\n            `ForecasterDirectMultiVariate` and `ForecasterRnn`).\n        encoding: str, default None\n            Encoding used to identify the different series (`ForecasterRecursiveMultiSeries`).\n\n    Returns:\n        None\n    \"\"\"\n\n    if not is_fitted:\n        raise RuntimeError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `predict`.\"\n        )\n\n    if isinstance(steps, (int, np.integer)) and steps &lt; 1:\n        raise ValueError(\n            f\"`steps` must be an integer greater than or equal to 1. Got {steps}.\"\n        )\n\n    if isinstance(steps, list) and min(steps) &lt; 1:\n        raise ValueError(\n            f\"`steps` must be a list of integers greater than or equal to 1. Got {steps}.\"\n        )\n\n    if max_step is not None:\n        if isinstance(steps, (int, np.integer)):\n            if steps &gt; max_step:\n                raise ValueError(\n                    f\"The maximum step that can be predicted is {max_step}. \"\n                    f\"Got {steps}.\"\n                )\n        elif isinstance(steps, list):\n            if max(steps) &gt; max_step:\n                raise ValueError(\n                    f\"The maximum step that can be predicted is {max_step}. \"\n                    f\"Got {max(steps)}.\"\n                )\n\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n\n    if exog_in_ and exog is None:\n        raise ValueError(\n            \"Forecaster trained with exogenous variable/s. \"\n            \"Same variable/s must be provided when predicting.\"\n        )\n\n    if not exog_in_ and exog is not None:\n        raise ValueError(\n            \"Forecaster trained without exogenous variable/s. \"\n            \"`exog` must be `None` when predicting.\"\n        )\n\n    if exog is not None:\n        # If exog is a dictionary, it is assumed that it contains the exogenous\n        # variables for each series.\n        if isinstance(exog, dict):\n            # Check that all series have the exogenous variables\n            if levels is None and series_names_in_ is not None:\n                levels = series_names_in_\n\n            if isinstance(levels, str):\n                levels = [levels]\n\n            if levels is not None:\n                for level in levels:\n                    if level not in exog:\n                        raise ValueError(\n                            f\"Exogenous variables for series '{level}' are missing.\"\n                        )\n                    check_exog(\n                        exog=exog[level],\n                        allow_nan=False,\n                        series_id=f\"`exog` for series '{level}'\",\n                    )\n                    check_exog_dtypes(\n                        exog=exog[level],\n                        call_check_exog=False,\n                        series_id=f\"`exog` for series '{level}'\",\n                    )\n\n                    # Check that exogenous variables are the same as used in training\n                    # Get the name of columns\n                    if isinstance(exog[level], pd.Series):\n                        exog_names = [exog[level].name]\n                    else:\n                        exog_names = exog[level].columns.tolist()\n\n                    if len(set(exog_names) - set(exog_names_in_)) &gt; 0:\n                        raise ValueError(\n                            f\"Exogenous variables must be: {exog_names_in_}. \"\n                            f\"Got {exog_names} for series '{level}'.\"\n                        )\n        else:\n            check_exog(exog=exog, allow_nan=False)\n            check_exog_dtypes(exog=exog, call_check_exog=False)\n\n            # Check that exogenous variables are the same as used in training\n            # Get the name of columns\n            if isinstance(exog, pd.Series):\n                exog_names = [exog.name]\n            else:\n                exog_names = exog.columns.tolist()\n\n            if len(set(exog_names) - set(exog_names_in_)) &gt; 0:\n                raise ValueError(\n                    f\"Exogenous variables must be: {exog_names_in_}. Got {exog_names}.\"\n                )\n\n    # Check last_window\n    if last_window is not None:\n        if isinstance(last_window, pd.DataFrame):\n            if last_window.isna().to_numpy().any():\n                raise ValueError(\"`last_window` has missing values.\")\n        else:\n            check_y(last_window, series_id=\"`last_window`\")\n\n    return\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.utils.check_preprocess_series","title":"<code>check_preprocess_series(series)</code>","text":"<p>Check and preprocess <code>series</code> argument in <code>ForecasterRecursiveMultiSeries</code> class.</p> <pre><code>- If `series` is a wide-format pandas DataFrame, each column represents a\ndifferent time series, and the index must be either a `DatetimeIndex` or\na `RangeIndex` with frequency or step size, as appropriate\n- If `series` is a long-format pandas DataFrame with a MultiIndex, the\nfirst level of the index must contain the series IDs, and the second\nlevel must be a `DatetimeIndex` with the same frequency across all series.\n- If series is a dictionary, each key must be a series ID, and each value\nmust be a named pandas Series. All series must have the same index, which\nmust be either a `DatetimeIndex` or a `RangeIndex`, and they must share the\nsame frequency or step size, as appropriate.\n</code></pre> <p>When <code>series</code> is a pandas DataFrame, it is converted to a dictionary of pandas Series, where the keys are the series IDs and the values are the Series with the same index as the original DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>DataFrame | dict[str, Series | DataFrame]</code> <p>pandas DataFrame or dictionary of pandas Series/DataFrames</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Series], dict[str, Index]]</code> <p>tuple[dict[str, pd.Series], dict[str, pd.Index]]: - series_dict: Dictionary where keys are series IDs and values are pandas Series. - series_indexes: Dictionary where keys are series IDs and values are the index of each series.</p> <p>Raises:     TypeError:         If <code>series</code> is not a pandas DataFrame or a dictionary of pandas Series/DataFrames.     TypeError:         If the index of <code>series</code> is not a DatetimeIndex or RangeIndex with frequency/step size.     ValueError:         If the series in <code>series</code> have different frequencies or step sizes.     ValueError:         If all values of any series are NaN.     UserWarning:         If <code>series</code> is a wide-format DataFrame, only the first column will be used as series values.     UserWarning:         If <code>series</code> is a DataFrame (either wide or long format), additional internal transformations are required, which can increase computational time.         It is recommended to use a dictionary of pandas Series instead.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.utils import check_preprocess_series\n&gt;&gt;&gt; # Example with wide-format DataFrame\n&gt;&gt;&gt; dates = pd.date_range('2020-01-01', periods=5, freq='D')\n&gt;&gt;&gt; df_wide = pd.DataFrame({\n...     'series_1': [1, 2, 3, 4, 5],\n...     'series_2': [5, 4, 3, 2, 1],\n... }, index=dates)\n&gt;&gt;&gt; series_dict, series_indexes = check_preprocess_series(df_wide)\nUserWarning: `series` DataFrame has multiple columns. Only the values of first column, 'series_1', will be used as series values. All other columns will be ignored.\nUserWarning: Passing a DataFrame (either wide or long format) as `series` requires additional internal transformations, which can increase computational time.\nIt is recommended to use a dictionary of pandas Series instead.\n&gt;&gt;&gt; print(series_dict['series_1'])\n2020-01-01    1\n2020-01-02    2\n2020-01-03    3\n2020-01-04    4\n2020-01-05    5\nName: series_1, dtype: int64\n&gt;&gt;&gt; print(series_indexes['series_1'])\nDatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n               '2020-01-05'],\n              dtype='datetime64[ns]', freq='D')\n&gt;&gt;&gt; # Example with long-format DataFrame\n&gt;&gt;&gt; df_long = pd.DataFrame({\n...     'series_id': ['series_1'] * 5 + ['series_2'] * 5,\n...     'value': [1, 2, 3, 4, 5, 5, 4, 3, 2, 1],\n... }, index=pd.MultiIndex.from_product([['series_1', 'series_2'], dates], names=['series_id', 'date']))\n&gt;&gt;&gt; series_dict, series_indexes = check_preprocess_series(df_long)\nUserWarning: `series` DataFrame has multiple columns. Only the values of first column, 'value', will be used as series values. All other columns will be ignored.\nUserWarning: Passing a DataFrame (either wide or long format) as `series` requires additional internal transformations, which can increase computational time.\nIt is recommended to use a dictionary of pandas Series instead.\n&gt;&gt;&gt; print(series_dict['series_1'])\n2020-01-01    1\n2020-01-02    2\n2020-01-03    3\n2020-01-04    4\n2020-01-05    5\nName: series_1, dtype: int64\n&gt;&gt;&gt; print(series_indexes['series_1'])\nDatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n                  '2020-01-05'],\n                 dtype='datetime64[ns]', freq='D')\n</code></pre> <pre><code>&gt;&gt;&gt; # Example with dictionary of Series\n&gt;&gt;&gt; series_dict_input = {\n...     'series_1': pd.Series([1, 2, 3, 4, 5], index=dates),\n...     'series_2': pd.Series([5, 4, 3, 2, 1], index=dates),\n... }\n&gt;&gt;&gt; series_dict, series_indexes = check_preprocess_series(series_dict_input)\n&gt;&gt;&gt; print(series_dict['series_1'])\n2020-01-01    1\n2020-01-02    2\n2020-01-03    3\n2020-01-04    4\n2020-01-05    5\nName: series_1, dtype: int64\n&gt;&gt;&gt; print(series_indexes['series_1'])\nDatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n               '2020-01-05'],\n              dtype='datetime64[ns]', freq='D')\n    &gt;&gt;&gt; # Example with dictionary of DataFrames\n    &gt;&gt;&gt; df_series_1 = pd.DataFrame({'value': [1, 2, 3, 4, 5]}, index=dates)\n    &gt;&gt;&gt; df_series_2 = pd.DataFrame({'value': [5, 4, 3, 2, 1]}, index=dates)\n    &gt;&gt;&gt; series_dict_input = {\n    ...     'series_1': df_series_1,\n    ...     'series_2': df_series_2,\n    ... }\n    &gt;&gt;&gt; series_dict, series_indexes = check_preprocess_series(series_dict_input)\n    &gt;&gt;&gt; print(series_dict['series_1'])\n2020-01-01    1\n2020-01-02    2\n2020-01-03    3\n2020-01-04    4\n2020-01-05    5\nName: series_1, dtype: int64\n&gt;&gt;&gt; print(series_indexes['series_1'])\nDatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n               '2020-01-05'],\n              dtype='datetime64[ns]', freq='D')\n</code></pre> Source code in <code>spotforecast2_safe/forecaster/utils.py</code> <pre><code>def check_preprocess_series(\n    series: pd.DataFrame | dict[str, pd.Series | pd.DataFrame],\n) -&gt; tuple[dict[str, pd.Series], dict[str, pd.Index]]:\n    \"\"\"\n    Check and preprocess `series` argument in `ForecasterRecursiveMultiSeries` class.\n\n        - If `series` is a wide-format pandas DataFrame, each column represents a\n        different time series, and the index must be either a `DatetimeIndex` or\n        a `RangeIndex` with frequency or step size, as appropriate\n        - If `series` is a long-format pandas DataFrame with a MultiIndex, the\n        first level of the index must contain the series IDs, and the second\n        level must be a `DatetimeIndex` with the same frequency across all series.\n        - If series is a dictionary, each key must be a series ID, and each value\n        must be a named pandas Series. All series must have the same index, which\n        must be either a `DatetimeIndex` or a `RangeIndex`, and they must share the\n        same frequency or step size, as appropriate.\n\n    When `series` is a pandas DataFrame, it is converted to a dictionary of pandas\n    Series, where the keys are the series IDs and the values are the Series with\n    the same index as the original DataFrame.\n\n    Args:\n        series: pandas DataFrame or dictionary of pandas Series/DataFrames\n\n    Returns:\n        tuple[dict[str, pd.Series], dict[str, pd.Index]]:\n            - series_dict: Dictionary where keys are series IDs and values are pandas Series.\n            - series_indexes: Dictionary where keys are series IDs and values are the index of each series.\n    Raises:\n        TypeError:\n            If `series` is not a pandas DataFrame or a dictionary of pandas Series/DataFrames.\n        TypeError:\n            If the index of `series` is not a DatetimeIndex or RangeIndex with frequency/step size.\n        ValueError:\n            If the series in `series` have different frequencies or step sizes.\n        ValueError:\n            If all values of any series are NaN.\n        UserWarning:\n            If `series` is a wide-format DataFrame, only the first column will be used as series values.\n        UserWarning:\n            If `series` is a DataFrame (either wide or long format), additional internal transformations are required, which can increase computational time.\n            It is recommended to use a dictionary of pandas Series instead.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.utils import check_preprocess_series\n        &gt;&gt;&gt; # Example with wide-format DataFrame\n        &gt;&gt;&gt; dates = pd.date_range('2020-01-01', periods=5, freq='D')\n        &gt;&gt;&gt; df_wide = pd.DataFrame({\n        ...     'series_1': [1, 2, 3, 4, 5],\n        ...     'series_2': [5, 4, 3, 2, 1],\n        ... }, index=dates)\n        &gt;&gt;&gt; series_dict, series_indexes = check_preprocess_series(df_wide)\n        UserWarning: `series` DataFrame has multiple columns. Only the values of first column, 'series_1', will be used as series values. All other columns will be ignored.\n        UserWarning: Passing a DataFrame (either wide or long format) as `series` requires additional internal transformations, which can increase computational time.\n        It is recommended to use a dictionary of pandas Series instead.\n        &gt;&gt;&gt; print(series_dict['series_1'])\n        2020-01-01    1\n        2020-01-02    2\n        2020-01-03    3\n        2020-01-04    4\n        2020-01-05    5\n        Name: series_1, dtype: int64\n        &gt;&gt;&gt; print(series_indexes['series_1'])\n        DatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n                       '2020-01-05'],\n                      dtype='datetime64[ns]', freq='D')\n        &gt;&gt;&gt; # Example with long-format DataFrame\n        &gt;&gt;&gt; df_long = pd.DataFrame({\n        ...     'series_id': ['series_1'] * 5 + ['series_2'] * 5,\n        ...     'value': [1, 2, 3, 4, 5, 5, 4, 3, 2, 1],\n        ... }, index=pd.MultiIndex.from_product([['series_1', 'series_2'], dates], names=['series_id', 'date']))\n        &gt;&gt;&gt; series_dict, series_indexes = check_preprocess_series(df_long)\n        UserWarning: `series` DataFrame has multiple columns. Only the values of first column, 'value', will be used as series values. All other columns will be ignored.\n        UserWarning: Passing a DataFrame (either wide or long format) as `series` requires additional internal transformations, which can increase computational time.\n        It is recommended to use a dictionary of pandas Series instead.\n        &gt;&gt;&gt; print(series_dict['series_1'])\n        2020-01-01    1\n        2020-01-02    2\n        2020-01-03    3\n        2020-01-04    4\n        2020-01-05    5\n        Name: series_1, dtype: int64\n        &gt;&gt;&gt; print(series_indexes['series_1'])\n        DatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n                          '2020-01-05'],\n                         dtype='datetime64[ns]', freq='D')\n\n        &gt;&gt;&gt; # Example with dictionary of Series\n        &gt;&gt;&gt; series_dict_input = {\n        ...     'series_1': pd.Series([1, 2, 3, 4, 5], index=dates),\n        ...     'series_2': pd.Series([5, 4, 3, 2, 1], index=dates),\n        ... }\n        &gt;&gt;&gt; series_dict, series_indexes = check_preprocess_series(series_dict_input)\n        &gt;&gt;&gt; print(series_dict['series_1'])\n        2020-01-01    1\n        2020-01-02    2\n        2020-01-03    3\n        2020-01-04    4\n        2020-01-05    5\n        Name: series_1, dtype: int64\n        &gt;&gt;&gt; print(series_indexes['series_1'])\n        DatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n                       '2020-01-05'],\n                      dtype='datetime64[ns]', freq='D')\n            &gt;&gt;&gt; # Example with dictionary of DataFrames\n            &gt;&gt;&gt; df_series_1 = pd.DataFrame({'value': [1, 2, 3, 4, 5]}, index=dates)\n            &gt;&gt;&gt; df_series_2 = pd.DataFrame({'value': [5, 4, 3, 2, 1]}, index=dates)\n            &gt;&gt;&gt; series_dict_input = {\n            ...     'series_1': df_series_1,\n            ...     'series_2': df_series_2,\n            ... }\n            &gt;&gt;&gt; series_dict, series_indexes = check_preprocess_series(series_dict_input)\n            &gt;&gt;&gt; print(series_dict['series_1'])\n        2020-01-01    1\n        2020-01-02    2\n        2020-01-03    3\n        2020-01-04    4\n        2020-01-05    5\n        Name: series_1, dtype: int64\n        &gt;&gt;&gt; print(series_indexes['series_1'])\n        DatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n                       '2020-01-05'],\n                      dtype='datetime64[ns]', freq='D')\n    \"\"\"\n    if not isinstance(series, (pd.DataFrame, dict)):\n        raise TypeError(\n            f\"`series` must be a pandas DataFrame or a dict of DataFrames or Series. \"\n            f\"Got {type(series)}.\"\n        )\n\n    if isinstance(series, pd.DataFrame):\n\n        if not isinstance(series.index, pd.MultiIndex):\n            _, _ = check_extract_values_and_index(\n                data=series, data_label=\"`series`\", return_values=False\n            )\n            series = series.copy()\n            series.index.name = None\n            series_dict = series.to_dict(orient=\"series\")\n        else:\n            if not isinstance(series.index.levels[1], pd.DatetimeIndex):\n                raise TypeError(\n                    f\"The second level of the MultiIndex in `series` must be a \"\n                    f\"pandas DatetimeIndex with the same frequency for each series. \"\n                    f\"Found {type(series.index.levels[1])}.\"\n                )\n\n            first_col = series.columns[0]\n            if len(series.columns) != 1:\n                warnings.warn(\n                    f\"`series` DataFrame has multiple columns. Only the values of \"\n                    f\"first column, '{first_col}', will be used as series values. \"\n                    f\"All other columns will be ignored.\",\n                    IgnoredArgumentWarning,\n                )\n\n            series = series.copy()\n            series.index = series.index.set_names([series.index.names[0], None])\n            series_dict = {\n                series_id: series.loc[series_id][first_col].rename(series_id)\n                for series_id in series.index.levels[0]\n            }\n\n        warnings.warn(\n            \"Passing a DataFrame (either wide or long format) as `series` requires \"\n            \"additional internal transformations, which can increase computational \"\n            \"time. It is recommended to use a dictionary of pandas Series instead. \",\n            InputTypeWarning,\n        )\n\n    else:\n\n        not_valid_series = [\n            k for k, v in series.items() if not isinstance(v, (pd.Series, pd.DataFrame))\n        ]\n        if not_valid_series:\n            raise TypeError(\n                f\"If `series` is a dictionary, all series must be a named \"\n                f\"pandas Series or a pandas DataFrame with a single column. \"\n                f\"Review series: {not_valid_series}\"\n            )\n\n        series_dict = {k: v.copy() for k, v in series.items()}\n\n    not_valid_index = []\n    indexes_freq = set()\n    series_indexes = {}\n    for k, v in series_dict.items():\n        if isinstance(v, pd.DataFrame):\n            if v.shape[1] != 1:\n                raise ValueError(\n                    f\"If `series` is a dictionary, all series must be a named \"\n                    f\"pandas Series or a pandas DataFrame with a single column. \"\n                    f\"Review series: '{k}'\"\n                )\n            series_dict[k] = v.iloc[:, 0]\n\n        series_dict[k].name = k\n        idx = v.index\n        if isinstance(idx, pd.DatetimeIndex):\n            indexes_freq.add(idx.freq)\n        elif isinstance(idx, pd.RangeIndex):\n            indexes_freq.add(idx.step)\n        else:\n            not_valid_index.append(k)\n\n        if v.isna().to_numpy().all():\n            raise ValueError(f\"All values of series '{k}' are NaN.\")\n\n        series_indexes[k] = idx\n\n    if not_valid_index:\n        raise TypeError(\n            f\"If `series` is a dictionary, all series must have a Pandas \"\n            f\"RangeIndex or DatetimeIndex with the same step/frequency. \"\n            f\"Review series: {not_valid_index}\"\n        )\n    if None in indexes_freq:\n        raise ValueError(\n            \"If `series` is a dictionary, all series must have a Pandas \"\n            \"RangeIndex or DatetimeIndex with the same step/frequency. \"\n            \"If it a MultiIndex DataFrame, the second level must be a DatetimeIndex \"\n            \"with the same frequency for each series. Found series with no \"\n            \"frequency or step.\"\n        )\n    if not len(indexes_freq) == 1:\n        raise ValueError(\n            f\"If `series` is a dictionary, all series must have a Pandas \"\n            f\"RangeIndex or DatetimeIndex with the same step/frequency. \"\n            f\"If it a MultiIndex DataFrame, the second level must be a DatetimeIndex \"\n            f\"with the same frequency for each series. \"\n            f\"Found frequencies: {sorted(indexes_freq)}\"\n        )\n\n    return series_dict, series_indexes\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.utils.check_residuals_input","title":"<code>check_residuals_input(forecaster_name, use_in_sample_residuals, in_sample_residuals_, out_sample_residuals_, use_binned_residuals, in_sample_residuals_by_bin_, out_sample_residuals_by_bin_, levels=None, encoding=None)</code>","text":"<p>Check residuals input arguments in Forecasters.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>str Forecaster name.</p> required <code>use_in_sample_residuals</code> <code>bool</code> <p>bool Indicates if in sample or out sample residuals are used.</p> required <code>in_sample_residuals_</code> <code>ndarray | dict[str, ndarray] | None</code> <p>numpy ndarray, dict Residuals of the model when predicting training data.</p> required <code>out_sample_residuals_</code> <code>ndarray | dict[str, ndarray] | None</code> <p>numpy ndarray, dict Residuals of the model when predicting non training data.</p> required <code>use_binned_residuals</code> <code>bool</code> <p>bool Indicates if residuals are binned.</p> required <code>in_sample_residuals_by_bin_</code> <code>dict[str | int, ndarray | dict[int, ndarray]] | None</code> <p>dict In sample residuals binned according to the predicted value each residual is associated with.</p> required <code>out_sample_residuals_by_bin_</code> <code>dict[str | int, ndarray | dict[int, ndarray]] | None</code> <p>dict Out of sample residuals binned according to the predicted value each residual is associated with.</p> required <code>levels</code> <code>list[str] | None</code> <p>list, default None Names of the series (levels) to be predicted (Forecasters multiseries).</p> <code>None</code> <code>encoding</code> <code>str | None</code> <p>str, default None Encoding used to identify the different series (ForecasterRecursiveMultiSeries).</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/spotforecast2/forecaster/utils.py</code> <pre><code>def check_residuals_input(\n    forecaster_name: str,\n    use_in_sample_residuals: bool,\n    in_sample_residuals_: np.ndarray | dict[str, np.ndarray] | None,\n    out_sample_residuals_: np.ndarray | dict[str, np.ndarray] | None,\n    use_binned_residuals: bool,\n    in_sample_residuals_by_bin_: (\n        dict[str | int, np.ndarray | dict[int, np.ndarray]] | None\n    ),\n    out_sample_residuals_by_bin_: (\n        dict[str | int, np.ndarray | dict[int, np.ndarray]] | None\n    ),\n    levels: list[str] | None = None,\n    encoding: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Check residuals input arguments in Forecasters.\n\n    Args:\n        forecaster_name: str\n            Forecaster name.\n        use_in_sample_residuals: bool\n            Indicates if in sample or out sample residuals are used.\n        in_sample_residuals_: numpy ndarray, dict\n            Residuals of the model when predicting training data.\n        out_sample_residuals_: numpy ndarray, dict\n            Residuals of the model when predicting non training data.\n        use_binned_residuals: bool\n            Indicates if residuals are binned.\n        in_sample_residuals_by_bin_: dict\n            In sample residuals binned according to the predicted value each residual\n            is associated with.\n        out_sample_residuals_by_bin_: dict\n            Out of sample residuals binned according to the predicted value each residual\n            is associated with.\n        levels: list, default None\n            Names of the series (levels) to be predicted (Forecasters multiseries).\n        encoding: str, default None\n            Encoding used to identify the different series (ForecasterRecursiveMultiSeries).\n\n    Returns:\n        None\n\n    \"\"\"\n\n    forecasters_multiseries = (\n        \"ForecasterRecursiveMultiSeries\",\n        \"ForecasterDirectMultiVariate\",\n        \"ForecasterRnn\",\n    )\n\n    if use_in_sample_residuals:\n        if use_binned_residuals:\n            residuals = in_sample_residuals_by_bin_\n            literal = \"in_sample_residuals_by_bin_\"\n        else:\n            residuals = in_sample_residuals_\n            literal = \"in_sample_residuals_\"\n\n        # Check if residuals are empty or None\n        is_empty = (\n            residuals is None\n            or (isinstance(residuals, dict) and not residuals)\n            or (isinstance(residuals, np.ndarray) and residuals.size == 0)\n        )\n        if is_empty:\n            raise ValueError(\n                f\"`forecaster.{literal}` is either None or empty. Use \"\n                f\"`store_in_sample_residuals = True` when fitting the forecaster \"\n                f\"or use the `set_in_sample_residuals()` method before predicting.\"\n            )\n\n        if forecaster_name in forecasters_multiseries:\n            if encoding is not None:\n                unknown_levels = set(levels) - set(residuals.keys())\n                if unknown_levels:\n                    warnings.warn(\n                        f\"`levels` {unknown_levels} are not present in `forecaster.{literal}`, \"\n                        f\"most likely because they were not present in the training data. \"\n                        f\"A random sample of the residuals from other levels will be used. \"\n                        f\"This can lead to inaccurate intervals for the unknown levels.\",\n                        UnknownLevelWarning,\n                    )\n    else:\n        if use_binned_residuals:\n            residuals = out_sample_residuals_by_bin_\n            literal = \"out_sample_residuals_by_bin_\"\n        else:\n            residuals = out_sample_residuals_\n            literal = \"out_sample_residuals_\"\n\n        is_empty = (\n            residuals is None\n            or (isinstance(residuals, dict) and not residuals)\n            or (isinstance(residuals, np.ndarray) and residuals.size == 0)\n        )\n        if is_empty:\n            raise ValueError(\n                f\"`forecaster.{literal}` is either None or empty. Use \"\n                f\"`set_out_sample_residuals()` method before predicting.\"\n            )\n\n        if forecaster_name in forecasters_multiseries:\n            if encoding is not None:\n                unknown_levels = set(levels) - set(residuals.keys())\n                if unknown_levels:\n                    warnings.warn(\n                        f\"`levels` {unknown_levels} are not present in `forecaster.{literal}`, \"\n                        f\"most likely because they were not present in the training data. \"\n                        f\"A random sample of the residuals from other levels will be used. \"\n                        f\"This can lead to inaccurate intervals for the unknown levels.\",\n                        UnknownLevelWarning,\n                    )\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.utils.check_select_fit_kwargs","title":"<code>check_select_fit_kwargs(estimator, fit_kwargs=None)</code>","text":"<p>Check if <code>fit_kwargs</code> is a dict and select only keys used by estimator's <code>fit</code>.</p> <p>This function validates that fit_kwargs is a dictionary, warns about unused arguments, removes 'sample_weight' (which should be handled via weight_func), and returns a dictionary containing only the arguments accepted by the estimator's fit method.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>Any</code> <p>Scikit-learn compatible estimator.</p> required <code>fit_kwargs</code> <code>Optional[dict]</code> <p>Dictionary of arguments to pass to the estimator's fit method.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with only the arguments accepted by the estimator's fit method.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If fit_kwargs is not a dict.</p> <p>Warns:</p> Type Description <code>IgnoredArgumentWarning</code> <p>If fit_kwargs contains keys not used by fit method, or if 'sample_weight' is present (it gets removed).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; from spotforecast2.utils.forecaster_config import check_select_fit_kwargs\n&gt;&gt;&gt;\n&gt;&gt;&gt; estimator = Ridge()\n&gt;&gt;&gt; # Valid argument for Ridge.fit\n&gt;&gt;&gt; kwargs = {\"sample_weight\": [1, 1], \"invalid_arg\": 10}\n&gt;&gt;&gt; # sample_weight is removed (should be passed via weight_func in forecaster)\n&gt;&gt;&gt; # invalid_arg is ignored\n&gt;&gt;&gt; filtered = check_select_fit_kwargs(estimator, kwargs)\n&gt;&gt;&gt; filtered\n{}\n</code></pre> Source code in <code>src/spotforecast2/utils/forecaster_config.py</code> <pre><code>def check_select_fit_kwargs(estimator: Any, fit_kwargs: Optional[dict] = None) -&gt; dict:\n    \"\"\"\n    Check if `fit_kwargs` is a dict and select only keys used by estimator's `fit`.\n\n    This function validates that fit_kwargs is a dictionary, warns about unused arguments,\n    removes 'sample_weight' (which should be handled via weight_func), and returns\n    a dictionary containing only the arguments accepted by the estimator's fit method.\n\n    Args:\n        estimator: Scikit-learn compatible estimator.\n        fit_kwargs: Dictionary of arguments to pass to the estimator's fit method.\n\n    Returns:\n        Dictionary with only the arguments accepted by the estimator's fit method.\n\n    Raises:\n        TypeError: If fit_kwargs is not a dict.\n\n    Warnings:\n        IgnoredArgumentWarning: If fit_kwargs contains keys not used by fit method,\n            or if 'sample_weight' is present (it gets removed).\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; from spotforecast2.utils.forecaster_config import check_select_fit_kwargs\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; estimator = Ridge()\n        &gt;&gt;&gt; # Valid argument for Ridge.fit\n        &gt;&gt;&gt; kwargs = {\"sample_weight\": [1, 1], \"invalid_arg\": 10}\n        &gt;&gt;&gt; # sample_weight is removed (should be passed via weight_func in forecaster)\n        &gt;&gt;&gt; # invalid_arg is ignored\n        &gt;&gt;&gt; filtered = check_select_fit_kwargs(estimator, kwargs)\n        &gt;&gt;&gt; filtered\n        {}\n    \"\"\"\n    import inspect\n    import warnings\n\n    # Import IgnoredArgumentWarning if available, otherwise define locally\n    try:\n        from spotforecast2.exceptions import IgnoredArgumentWarning\n    except ImportError:\n\n        class IgnoredArgumentWarning(UserWarning):\n            \"\"\"Warning for ignored arguments.\"\"\"\n\n            pass\n\n    if fit_kwargs is None:\n        fit_kwargs = {}\n    else:\n        if not isinstance(fit_kwargs, dict):\n            raise TypeError(\n                f\"Argument `fit_kwargs` must be a dict. Got {type(fit_kwargs)}.\"\n            )\n\n        # Get parameters accepted by estimator.fit\n        fit_params = inspect.signature(estimator.fit).parameters\n\n        # Identify unused keys\n        non_used_keys = [k for k in fit_kwargs.keys() if k not in fit_params]\n        if non_used_keys:\n            warnings.warn(\n                f\"Argument/s {non_used_keys} ignored since they are not used by the \"\n                f\"estimator's `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n\n        # Handle sample_weight specially\n        if \"sample_weight\" in fit_kwargs.keys():\n            warnings.warn(\n                \"The `sample_weight` argument is ignored. Use `weight_func` to pass \"\n                \"a function that defines the individual weights for each sample \"\n                \"based on its index.\",\n                IgnoredArgumentWarning,\n            )\n            del fit_kwargs[\"sample_weight\"]\n\n        # Select only the keyword arguments allowed by the estimator's `fit` method.\n        # Note: We need to re-check keys because sample_weight might have been deleted but it might be in fit_params\n        # If it was deleted, it is no longer in fit_kwargs, so this comprehension is safe\n        fit_kwargs = {k: v for k, v in fit_kwargs.items() if k in fit_params}\n\n    return fit_kwargs\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.utils.check_y","title":"<code>check_y(y, series_id='`y`')</code>","text":"<p>Validate that y is a pandas Series without missing values.</p> <p>This function ensures that the input time series meets the basic requirements for forecasting: it must be a pandas Series and must not contain any NaN values.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Any</code> <p>Time series values to validate.</p> required <code>series_id</code> <code>str</code> <p>Identifier of the series used in error messages. Defaults to \"<code>y</code>\".</p> <code>'`y`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If y is not a pandas Series.</p> <code>ValueError</code> <p>If y contains missing (NaN) values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_y\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid series\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n&gt;&gt;&gt; check_y(y)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not a Series\n&gt;&gt;&gt; try:\n...     check_y([1, 2, 3])\n... except TypeError as e:\n...     print(f\"Error: {e}\")\nError: `y` must be a pandas Series with a DatetimeIndex or a RangeIndex. Found &lt;class 'list'&gt;.\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: contains NaN\n&gt;&gt;&gt; y_with_nan = pd.Series([1, 2, np.nan, 4])\n&gt;&gt;&gt; try:\n...     check_y(y_with_nan)\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: `y` has missing values.\n</code></pre> Source code in <code>src/spotforecast2/utils/validation.py</code> <pre><code>def check_y(y: Any, series_id: str = \"`y`\") -&gt; None:\n    \"\"\"\n    Validate that y is a pandas Series without missing values.\n\n    This function ensures that the input time series meets the basic requirements\n    for forecasting: it must be a pandas Series and must not contain any NaN values.\n\n    Args:\n        y: Time series values to validate.\n        series_id: Identifier of the series used in error messages. Defaults to \"`y`\".\n\n    Raises:\n        TypeError: If y is not a pandas Series.\n        ValueError: If y contains missing (NaN) values.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_y\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid series\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; check_y(y)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not a Series\n        &gt;&gt;&gt; try:\n        ...     check_y([1, 2, 3])\n        ... except TypeError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `y` must be a pandas Series with a DatetimeIndex or a RangeIndex. Found &lt;class 'list'&gt;.\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: contains NaN\n        &gt;&gt;&gt; y_with_nan = pd.Series([1, 2, np.nan, 4])\n        &gt;&gt;&gt; try:\n        ...     check_y(y_with_nan)\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `y` has missing values.\n    \"\"\"\n    if not isinstance(y, pd.Series):\n        raise TypeError(\n            f\"{series_id} must be a pandas Series with a DatetimeIndex or a RangeIndex. \"\n            f\"Found {type(y)}.\"\n        )\n\n    if y.isna().to_numpy().any():\n        raise ValueError(f\"{series_id} has missing values.\")\n\n    return\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.utils.date_to_index_position","title":"<code>date_to_index_position(index, date_input, method='prediction', date_literal='steps', kwargs_pd_to_datetime={})</code>","text":"<p>Transform a datetime string or pandas Timestamp to an integer. The integer represents the position of the datetime in the index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Index</code> <p>pandas Index Original datetime index (must be a pandas DatetimeIndex if <code>date_input</code> is not an int).</p> required <code>date_input</code> <code>int | str | Timestamp</code> <p>int, str, pandas Timestamp Datetime to transform to integer.</p> <ul> <li>If int, returns the same integer.</li> <li>If str or pandas Timestamp, it is converted and expanded into the index.</li> </ul> required <code>method</code> <code>str</code> <p>str, default 'prediction' Can be 'prediction' or 'validation'.</p> <ul> <li>If 'prediction', the date must be later than the last date in the index.</li> <li>If 'validation', the date must be within the index range.</li> </ul> <code>'prediction'</code> <code>date_literal</code> <code>str</code> <p>str, default 'steps' Variable name used in error messages.</p> <code>'steps'</code> <code>kwargs_pd_to_datetime</code> <code>dict</code> <p>dict, default {} Additional keyword arguments to pass to <code>pd.to_datetime()</code>.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p><code>date_input</code> transformed to integer position in the <code>index</code>.</p> <code>int</code> <ul> <li>If <code>date_input</code> is an integer, it returns the same integer.</li> </ul> <code>int</code> <ul> <li>If method is 'prediction', number of steps to predict from the last</li> </ul> <code>int</code> <p>date in the index.</p> <code>int</code> <ul> <li>If method is 'validation', position plus one of the date in the index,</li> </ul> <code>int</code> <p>this is done to include the target date in the training set when using</p> <code>int</code> <p>pandas iloc with slices.</p> Source code in <code>src/spotforecast2/forecaster/utils.py</code> <pre><code>def date_to_index_position(\n    index: pd.Index,\n    date_input: int | str | pd.Timestamp,\n    method: str = \"prediction\",\n    date_literal: str = \"steps\",\n    kwargs_pd_to_datetime: dict = {},\n) -&gt; int:\n    \"\"\"\n    Transform a datetime string or pandas Timestamp to an integer. The integer\n    represents the position of the datetime in the index.\n\n    Args:\n        index: pandas Index\n            Original datetime index (must be a pandas DatetimeIndex if `date_input`\n            is not an int).\n        date_input: int, str, pandas Timestamp\n            Datetime to transform to integer.\n\n            - If int, returns the same integer.\n            - If str or pandas Timestamp, it is converted and expanded into the index.\n        method: str, default 'prediction'\n            Can be 'prediction' or 'validation'.\n\n            - If 'prediction', the date must be later than the last date in the index.\n            - If 'validation', the date must be within the index range.\n        date_literal: str, default 'steps'\n            Variable name used in error messages.\n        kwargs_pd_to_datetime: dict, default {}\n            Additional keyword arguments to pass to `pd.to_datetime()`.\n\n    Returns:\n        int:\n            `date_input` transformed to integer position in the `index`.\n\n        + If `date_input` is an integer, it returns the same integer.\n        + If method is 'prediction', number of steps to predict from the last\n        date in the index.\n        + If method is 'validation', position plus one of the date in the index,\n        this is done to include the target date in the training set when using\n        pandas iloc with slices.\n\n    \"\"\"\n\n    if method not in [\"prediction\", \"validation\"]:\n        raise ValueError(\"`method` must be 'prediction' or 'validation'.\")\n\n    if isinstance(date_input, (str, pd.Timestamp)):\n        if not isinstance(index, pd.DatetimeIndex):\n            raise TypeError(\n                f\"Index must be a pandas DatetimeIndex when `{date_literal}` is \"\n                f\"not an integer. Check input series or last window.\"\n            )\n\n        target_date = pd.to_datetime(date_input, **kwargs_pd_to_datetime)\n        last_date = pd.to_datetime(index[-1])\n\n        if method == \"prediction\":\n            if target_date &lt;= last_date:\n                raise ValueError(\n                    \"If `steps` is a date, it must be greater than the last date \"\n                    \"in the index.\"\n                )\n            span_index = pd.date_range(\n                start=last_date, end=target_date, freq=index.freq\n            )\n            output = len(span_index) - 1\n        elif method == \"validation\":\n            first_date = pd.to_datetime(index[0])\n            if target_date &lt; first_date or target_date &gt; last_date:\n                raise ValueError(\n                    \"If `initial_train_size` is a date, it must be greater than \"\n                    \"the first date in the index and less than the last date.\"\n                )\n            span_index = pd.date_range(\n                start=first_date, end=target_date, freq=index.freq\n            )\n            output = len(span_index)\n\n    elif isinstance(date_input, (int, np.integer)):\n        output = date_input\n\n    else:\n        raise TypeError(\n            f\"`{date_literal}` must be an integer, string, or pandas Timestamp.\"\n        )\n\n    return output\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.utils.exog_to_direct","title":"<code>exog_to_direct(exog, steps)</code>","text":"<p>Transforms <code>exog</code> to a pandas DataFrame with the shape needed for Direct forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Series | DataFrame</code> <p>pandas Series, pandas DataFrame Exogenous variables.</p> required <code>steps</code> <code>int</code> <p>int Number of steps that will be predicted using exog.</p> required <p>Returns:</p> Type Description <code>tuple[DataFrame, list[str]]</code> <p>tuple[pd.DataFrame, list[str]]: exog_direct: pandas DataFrame     Exogenous variables transformed. exog_direct_names: list     Names of the columns of the exogenous variables transformed. Only     created if <code>exog</code> is a pandas Series or DataFrame.</p> Source code in <code>spotforecast2_safe/forecaster/utils.py</code> <pre><code>def exog_to_direct(\n    exog: pd.Series | pd.DataFrame, steps: int\n) -&gt; tuple[pd.DataFrame, list[str]]:\n    \"\"\"\n    Transforms `exog` to a pandas DataFrame with the shape needed for Direct\n    forecasting.\n\n    Args:\n        exog: pandas Series, pandas DataFrame\n            Exogenous variables.\n        steps: int\n            Number of steps that will be predicted using exog.\n\n    Returns:\n        tuple[pd.DataFrame, list[str]]:\n            exog_direct: pandas DataFrame\n                Exogenous variables transformed.\n            exog_direct_names: list\n                Names of the columns of the exogenous variables transformed. Only\n                created if `exog` is a pandas Series or DataFrame.\n    \"\"\"\n\n    if not isinstance(exog, (pd.Series, pd.DataFrame)):\n        raise TypeError(\n            f\"`exog` must be a pandas Series or DataFrame. Got {type(exog)}.\"\n        )\n\n    if isinstance(exog, pd.Series):\n        exog = exog.to_frame()\n\n    n_rows = len(exog)\n    exog_idx = exog.index\n    exog_cols = exog.columns\n    exog_direct = []\n    for i in range(steps):\n        exog_step = exog.iloc[i : n_rows - (steps - 1 - i),]\n        exog_step.index = pd.RangeIndex(len(exog_step))\n        exog_step.columns = [f\"{col}_step_{i + 1}\" for col in exog_cols]\n        exog_direct.append(exog_step)\n\n    exog_direct = pd.concat(exog_direct, axis=1) if steps &gt; 1 else exog_direct[0]\n\n    exog_direct_names = exog_direct.columns.to_list()\n    exog_direct.index = exog_idx[-len(exog_direct) :]\n\n    return exog_direct, exog_direct_names\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.utils.exog_to_direct_numpy","title":"<code>exog_to_direct_numpy(exog, steps)</code>","text":"<p>Transforms <code>exog</code> to numpy ndarray with the shape needed for Direct forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>ndarray | Series | DataFrame</code> <p>numpy ndarray, pandas Series, pandas DataFrame Exogenous variables, shape(samples,). If exog is a pandas format, the direct exog names are created.</p> required <code>steps</code> <code>int</code> <p>int Number of steps that will be predicted using exog.</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, list[str] | None]</code> <p>tuple[np.ndarray, list[str] | None]: exog_direct: numpy ndarray     Exogenous variables transformed. exog_direct_names: list, None     Names of the columns of the exogenous variables transformed. Only     created if <code>exog</code> is a pandas Series or DataFrame.</p> Source code in <code>src/spotforecast2/forecaster/utils.py</code> <pre><code>def exog_to_direct_numpy(\n    exog: np.ndarray | pd.Series | pd.DataFrame, steps: int\n) -&gt; tuple[np.ndarray, list[str] | None]:\n    \"\"\"\n    Transforms `exog` to numpy ndarray with the shape needed for Direct\n    forecasting.\n\n    Args:\n        exog: numpy ndarray, pandas Series, pandas DataFrame\n            Exogenous variables, shape(samples,). If exog is a pandas format, the\n            direct exog names are created.\n        steps: int\n            Number of steps that will be predicted using exog.\n\n    Returns:\n        tuple[np.ndarray, list[str] | None]:\n            exog_direct: numpy ndarray\n                Exogenous variables transformed.\n            exog_direct_names: list, None\n                Names of the columns of the exogenous variables transformed. Only\n                created if `exog` is a pandas Series or DataFrame.\n    \"\"\"\n\n    if isinstance(exog, (pd.Series, pd.DataFrame)):\n        exog_cols = exog.columns if isinstance(exog, pd.DataFrame) else [exog.name]\n        exog_direct_names = [\n            f\"{col}_step_{i + 1}\" for i in range(steps) for col in exog_cols\n        ]\n        exog = exog.to_numpy()\n    else:\n        exog_direct_names = None\n        if not isinstance(exog, np.ndarray):\n            raise TypeError(\n                f\"`exog` must be a numpy ndarray, pandas Series or DataFrame. \"\n                f\"Got {type(exog)}.\"\n            )\n\n    if exog.ndim == 1:\n        exog = np.expand_dims(exog, axis=1)\n\n    n_rows = len(exog)\n    exog_direct = [exog[i : n_rows - (steps - 1 - i)] for i in range(steps)]\n    exog_direct = np.concatenate(exog_direct, axis=1) if steps &gt; 1 else exog_direct[0]\n\n    return exog_direct, exog_direct_names\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.utils.expand_index","title":"<code>expand_index(index, steps)</code>","text":"<p>Create a new index extending from the end of the original index.</p> <p>This function generates future indices for forecasting by extending the time series index by a specified number of steps. Handles both DatetimeIndex and RangeIndex appropriately.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Union[Index, None]</code> <p>Original pandas Index (DatetimeIndex or RangeIndex). If None, creates a RangeIndex starting from 0.</p> required <code>steps</code> <code>int</code> <p>Number of future steps to generate.</p> required <p>Returns:</p> Type Description <code>Index</code> <p>New pandas Index with <code>steps</code> future periods.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If steps is not an integer, or if index is neither DatetimeIndex nor RangeIndex.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2.utils.data_transform import expand_index\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DatetimeIndex\n&gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=5, freq=\"D\")\n&gt;&gt;&gt; new_index = expand_index(dates, 3)\n&gt;&gt;&gt; new_index\nDatetimeIndex(['2023-01-06', '2023-01-07', '2023-01-08'], dtype='datetime64[ns]', freq='D')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # RangeIndex\n&gt;&gt;&gt; range_idx = pd.RangeIndex(start=0, stop=10)\n&gt;&gt;&gt; new_index = expand_index(range_idx, 5)\n&gt;&gt;&gt; new_index\nRangeIndex(start=10, stop=15, step=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # None index (creates new RangeIndex)\n&gt;&gt;&gt; new_index = expand_index(None, 3)\n&gt;&gt;&gt; new_index\nRangeIndex(start=0, stop=3, step=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: steps not an integer\n&gt;&gt;&gt; try:\n...     expand_index(dates, 3.5)\n... except TypeError as e:\n...     print(\"Error: steps must be an integer\")\nError: steps must be an integer\n</code></pre> Source code in <code>src/spotforecast2/utils/data_transform.py</code> <pre><code>def expand_index(index: Union[pd.Index, None], steps: int) -&gt; pd.Index:\n    \"\"\"\n    Create a new index extending from the end of the original index.\n\n    This function generates future indices for forecasting by extending the time\n    series index by a specified number of steps. Handles both DatetimeIndex and\n    RangeIndex appropriately.\n\n    Args:\n        index: Original pandas Index (DatetimeIndex or RangeIndex). If None,\n            creates a RangeIndex starting from 0.\n        steps: Number of future steps to generate.\n\n    Returns:\n        New pandas Index with `steps` future periods.\n\n    Raises:\n        TypeError: If steps is not an integer, or if index is neither DatetimeIndex\n            nor RangeIndex.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2.utils.data_transform import expand_index\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DatetimeIndex\n        &gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=5, freq=\"D\")\n        &gt;&gt;&gt; new_index = expand_index(dates, 3)\n        &gt;&gt;&gt; new_index\n        DatetimeIndex(['2023-01-06', '2023-01-07', '2023-01-08'], dtype='datetime64[ns]', freq='D')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # RangeIndex\n        &gt;&gt;&gt; range_idx = pd.RangeIndex(start=0, stop=10)\n        &gt;&gt;&gt; new_index = expand_index(range_idx, 5)\n        &gt;&gt;&gt; new_index\n        RangeIndex(start=10, stop=15, step=1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # None index (creates new RangeIndex)\n        &gt;&gt;&gt; new_index = expand_index(None, 3)\n        &gt;&gt;&gt; new_index\n        RangeIndex(start=0, stop=3, step=1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: steps not an integer\n        &gt;&gt;&gt; try:\n        ...     expand_index(dates, 3.5)\n        ... except TypeError as e:\n        ...     print(\"Error: steps must be an integer\")\n        Error: steps must be an integer\n    \"\"\"\n    if not isinstance(steps, (int, np.integer)):\n        raise TypeError(f\"`steps` must be an integer. Got {type(steps)}.\")\n\n    # Convert numpy integer to Python int if needed\n    if isinstance(steps, np.integer):\n        steps = int(steps)\n\n    if isinstance(index, pd.Index):\n        if isinstance(index, pd.DatetimeIndex):\n            new_index = pd.date_range(\n                start=index[-1] + index.freq, periods=steps, freq=index.freq\n            )\n        elif isinstance(index, pd.RangeIndex):\n            new_index = pd.RangeIndex(start=index[-1] + 1, stop=index[-1] + 1 + steps)\n        else:\n            raise TypeError(\n                \"Argument `index` must be a pandas DatetimeIndex or RangeIndex.\"\n            )\n    else:\n        new_index = pd.RangeIndex(start=0, stop=steps)\n\n    return new_index\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.utils.get_exog_dtypes","title":"<code>get_exog_dtypes(exog)</code>","text":"<p>Extract and store the data types of exogenous variables.</p> <p>This function returns a dictionary mapping column names to their data types. For Series, uses the series name as the key. For DataFrames, uses all column names.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variable/s (Series or DataFrame).</p> required <p>Returns:</p> Type Description <code>Dict[str, type]</code> <p>Dictionary mapping variable names to their pandas dtypes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import get_exog_dtypes\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DataFrame with mixed types\n&gt;&gt;&gt; exog_df = pd.DataFrame({\n...     \"temp\": pd.Series([20.5, 21.3, 22.1], dtype='float64'),\n...     \"day\": pd.Series([1, 2, 3], dtype='int64'),\n...     \"is_weekend\": pd.Series([False, False, True], dtype='bool')\n... })\n&gt;&gt;&gt; dtypes = get_exog_dtypes(exog_df)\n&gt;&gt;&gt; dtypes['temp']\ndtype('float64')\n&gt;&gt;&gt; dtypes['day']\ndtype('int64')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series\n&gt;&gt;&gt; exog_series = pd.Series([1.0, 2.0, 3.0], name=\"temperature\", dtype='float64')\n&gt;&gt;&gt; dtypes = get_exog_dtypes(exog_series)\n&gt;&gt;&gt; dtypes\n{'temperature': dtype('float64')}\n</code></pre> Source code in <code>src/spotforecast2/utils/validation.py</code> <pre><code>def get_exog_dtypes(exog: Union[pd.Series, pd.DataFrame]) -&gt; Dict[str, type]:\n    \"\"\"\n    Extract and store the data types of exogenous variables.\n\n    This function returns a dictionary mapping column names to their data types.\n    For Series, uses the series name as the key. For DataFrames, uses all column names.\n\n    Args:\n        exog: Exogenous variable/s (Series or DataFrame).\n\n    Returns:\n        Dictionary mapping variable names to their pandas dtypes.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import get_exog_dtypes\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DataFrame with mixed types\n        &gt;&gt;&gt; exog_df = pd.DataFrame({\n        ...     \"temp\": pd.Series([20.5, 21.3, 22.1], dtype='float64'),\n        ...     \"day\": pd.Series([1, 2, 3], dtype='int64'),\n        ...     \"is_weekend\": pd.Series([False, False, True], dtype='bool')\n        ... })\n        &gt;&gt;&gt; dtypes = get_exog_dtypes(exog_df)\n        &gt;&gt;&gt; dtypes['temp']\n        dtype('float64')\n        &gt;&gt;&gt; dtypes['day']\n        dtype('int64')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series\n        &gt;&gt;&gt; exog_series = pd.Series([1.0, 2.0, 3.0], name=\"temperature\", dtype='float64')\n        &gt;&gt;&gt; dtypes = get_exog_dtypes(exog_series)\n        &gt;&gt;&gt; dtypes\n        {'temperature': dtype('float64')}\n    \"\"\"\n    if isinstance(exog, pd.Series):\n        exog_dtypes = {exog.name: exog.dtypes}\n    else:\n        exog_dtypes = exog.dtypes.to_dict()\n\n    return exog_dtypes\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.utils.get_style_repr_html","title":"<code>get_style_repr_html(is_fitted=False)</code>","text":"<p>Generate CSS style for HTML representation of the Forecaster.</p> <p>Creates a unique CSS style block with a container ID for rendering forecaster objects in Jupyter notebooks or HTML documents. The styling provides a clean, monospace display with a light gray background.</p> <p>Parameters:</p> Name Type Description Default <code>is_fitted</code> <code>bool</code> <p>Parameter to indicate if the Forecaster has been fitted. Currently not used in styling but reserved for future extensions.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[str, str]</code> <p>A tuple containing: - style (str): CSS style block as a string with unique container class. - unique_id (str): Unique 8-character ID for the container element.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; style, uid = get_style_repr_html(is_fitted=True)\n&gt;&gt;&gt; print(f\"Container ID: {uid}\")\nContainer ID: a1b2c3d4\n&gt;&gt;&gt; print(f\"Style contains CSS: {'container-' in style}\")\nStyle contains CSS: True\n</code></pre> <p>Using in HTML rendering:</p> <pre><code>&gt;&gt;&gt; style, uid = get_style_repr_html(is_fitted=False)\n&gt;&gt;&gt; html = f\"{style}&lt;div class='container-{uid}'&gt;Forecaster Info&lt;/div&gt;\"\n&gt;&gt;&gt; print(\"background-color\" in html)\nTrue\n</code></pre> Source code in <code>spotforecast2_safe/forecaster/utils.py</code> <pre><code>def get_style_repr_html(is_fitted: bool = False) -&gt; Tuple[str, str]:\n    \"\"\"Generate CSS style for HTML representation of the Forecaster.\n\n    Creates a unique CSS style block with a container ID for rendering\n    forecaster objects in Jupyter notebooks or HTML documents. The styling\n    provides a clean, monospace display with a light gray background.\n\n    Args:\n        is_fitted: Parameter to indicate if the Forecaster has been fitted.\n            Currently not used in styling but reserved for future extensions.\n\n    Returns:\n        tuple: A tuple containing:\n            - style (str): CSS style block as a string with unique container class.\n            - unique_id (str): Unique 8-character ID for the container element.\n\n    Examples:\n        &gt;&gt;&gt; style, uid = get_style_repr_html(is_fitted=True)\n        &gt;&gt;&gt; print(f\"Container ID: {uid}\")\n        Container ID: a1b2c3d4\n        &gt;&gt;&gt; print(f\"Style contains CSS: {'container-' in style}\")\n        Style contains CSS: True\n\n        Using in HTML rendering:\n        &gt;&gt;&gt; style, uid = get_style_repr_html(is_fitted=False)\n        &gt;&gt;&gt; html = f\"{style}&lt;div class='container-{uid}'&gt;Forecaster Info&lt;/div&gt;\"\n        &gt;&gt;&gt; print(\"background-color\" in html)\n        True\n    \"\"\"\n\n    unique_id = str(uuid.uuid4())[:8]\n    style = f\"\"\"\n    &lt;style&gt;\n        .container-{unique_id} {{\n            font-family: monospace;\n            background-color: #f0f0f0;\n            padding: 10px;\n            border-radius: 5px;\n        }}\n    &lt;/style&gt;\n    \"\"\"\n    return style, unique_id\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.utils.initialize_estimator","title":"<code>initialize_estimator(estimator=None, regressor=None)</code>","text":"<p>Helper to handle the deprecation of 'regressor' in favor of 'estimator'. Returns the valid estimator object.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>object | None</code> <p>estimator or pipeline compatible with the scikit-learn API, default None An instance of a estimator or pipeline compatible with the scikit-learn API.</p> <code>None</code> <code>regressor</code> <code>object | None</code> <p>estimator or pipeline compatible with the scikit-learn API, default None Deprecated. An instance of a estimator or pipeline compatible with the scikit-learn API.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>estimator or pipeline compatible with the scikit-learn API The valid estimator object.</p> Source code in <code>src/spotforecast2/forecaster/utils.py</code> <pre><code>def initialize_estimator(\n    estimator: object | None = None, regressor: object | None = None\n) -&gt; None:\n    \"\"\"\n    Helper to handle the deprecation of 'regressor' in favor of 'estimator'.\n    Returns the valid estimator object.\n\n    Args:\n        estimator: estimator or pipeline compatible with the scikit-learn API, default None\n            An instance of a estimator or pipeline compatible with the scikit-learn API.\n        regressor: estimator or pipeline compatible with the scikit-learn API, default None\n            Deprecated. An instance of a estimator or pipeline compatible with the\n            scikit-learn API.\n\n    Returns:\n        estimator or pipeline compatible with the scikit-learn API\n            The valid estimator object.\n\n    \"\"\"\n\n    if regressor is not None:\n        warnings.warn(\n            \"The `regressor` argument is deprecated and will be removed in a future \"\n            \"version. Please use `estimator` instead.\",\n            FutureWarning,\n            stacklevel=3,  # Important: to point to the user's code\n        )\n        if estimator is not None:\n            raise ValueError(\n                \"Both `estimator` and `regressor` were provided. Use only `estimator`.\"\n            )\n        return regressor\n\n    return estimator\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.utils.initialize_lags","title":"<code>initialize_lags(forecaster_name, lags)</code>","text":"<p>Validate and normalize lag specification for forecasting.</p> <p>This function converts various lag specifications (int, list, tuple, range, ndarray) into a standardized format: sorted numpy array, lag names, and maximum lag value.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Name of the forecaster class for error messages.</p> required <code>lags</code> <code>Any</code> <p>Lag specification in one of several formats: - int: Creates lags from 1 to lags (e.g., 5 \u2192 [1,2,3,4,5]) - list/tuple/range: Converted to numpy array - numpy.ndarray: Validated and used directly - None: Returns (None, None, None)</p> required <p>Returns:</p> Type Description <code>Optional[ndarray]</code> <p>Tuple containing:</p> <code>Optional[List[str]]</code> <ul> <li>lags: Sorted numpy array of lag values (or None)</li> </ul> <code>Optional[int]</code> <ul> <li>lags_names: List of lag names like ['lag_1', 'lag_2', ...] (or None)</li> </ul> <code>Tuple[Optional[ndarray], Optional[List[str]], Optional[int]]</code> <ul> <li>max_lag: Maximum lag value (or None)</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If lags &lt; 1, empty array, or not 1-dimensional.</p> <code>TypeError</code> <p>If lags is not an integer, not in the right format for the forecaster, or array contains non-integer values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_lags\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Integer input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", 3)\n&gt;&gt;&gt; lags\narray([1, 2, 3])\n&gt;&gt;&gt; names\n['lag_1', 'lag_2', 'lag_3']\n&gt;&gt;&gt; max_lag\n3\n&gt;&gt;&gt;\n&gt;&gt;&gt; # List input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", [1, 3, 5])\n&gt;&gt;&gt; lags\narray([1, 3, 5])\n&gt;&gt;&gt; names\n['lag_1', 'lag_3', 'lag_5']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Range input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", range(1, 4))\n&gt;&gt;&gt; lags\narray([1, 2, 3])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # None input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", None)\n&gt;&gt;&gt; lags is None\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: lags &lt; 1\n&gt;&gt;&gt; try:\n...     initialize_lags(\"ForecasterRecursive\", 0)\n... except ValueError as e:\n...     print(\"Error: Minimum value of lags allowed is 1\")\nError: Minimum value of lags allowed is 1\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: negative lags\n&gt;&gt;&gt; try:\n...     initialize_lags(\"ForecasterRecursive\", [1, -2, 3])\n... except ValueError as e:\n...     print(\"Error: Minimum value of lags allowed is 1\")\nError: Minimum value of lags allowed is 1\n</code></pre> Source code in <code>src/spotforecast2/utils/forecaster_config.py</code> <pre><code>def initialize_lags(\n    forecaster_name: str, lags: Any\n) -&gt; Tuple[Optional[np.ndarray], Optional[List[str]], Optional[int]]:\n    \"\"\"\n    Validate and normalize lag specification for forecasting.\n\n    This function converts various lag specifications (int, list, tuple, range, ndarray)\n    into a standardized format: sorted numpy array, lag names, and maximum lag value.\n\n    Args:\n        forecaster_name: Name of the forecaster class for error messages.\n        lags: Lag specification in one of several formats:\n            - int: Creates lags from 1 to lags (e.g., 5 \u2192 [1,2,3,4,5])\n            - list/tuple/range: Converted to numpy array\n            - numpy.ndarray: Validated and used directly\n            - None: Returns (None, None, None)\n\n    Returns:\n        Tuple containing:\n        - lags: Sorted numpy array of lag values (or None)\n        - lags_names: List of lag names like ['lag_1', 'lag_2', ...] (or None)\n        - max_lag: Maximum lag value (or None)\n\n    Raises:\n        ValueError: If lags &lt; 1, empty array, or not 1-dimensional.\n        TypeError: If lags is not an integer, not in the right format for the forecaster,\n            or array contains non-integer values.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_lags\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Integer input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", 3)\n        &gt;&gt;&gt; lags\n        array([1, 2, 3])\n        &gt;&gt;&gt; names\n        ['lag_1', 'lag_2', 'lag_3']\n        &gt;&gt;&gt; max_lag\n        3\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # List input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", [1, 3, 5])\n        &gt;&gt;&gt; lags\n        array([1, 3, 5])\n        &gt;&gt;&gt; names\n        ['lag_1', 'lag_3', 'lag_5']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Range input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", range(1, 4))\n        &gt;&gt;&gt; lags\n        array([1, 2, 3])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # None input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", None)\n        &gt;&gt;&gt; lags is None\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: lags &lt; 1\n        &gt;&gt;&gt; try:\n        ...     initialize_lags(\"ForecasterRecursive\", 0)\n        ... except ValueError as e:\n        ...     print(\"Error: Minimum value of lags allowed is 1\")\n        Error: Minimum value of lags allowed is 1\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: negative lags\n        &gt;&gt;&gt; try:\n        ...     initialize_lags(\"ForecasterRecursive\", [1, -2, 3])\n        ... except ValueError as e:\n        ...     print(\"Error: Minimum value of lags allowed is 1\")\n        Error: Minimum value of lags allowed is 1\n    \"\"\"\n    lags_names = None\n    max_lag = None\n\n    if lags is not None:\n        if isinstance(lags, int):\n            if lags &lt; 1:\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n            lags = np.arange(1, lags + 1)\n\n        if isinstance(lags, (list, tuple, range)):\n            lags = np.array(lags)\n\n        if isinstance(lags, np.ndarray):\n            if lags.size == 0:\n                return None, None, None\n            if lags.ndim != 1:\n                raise ValueError(\"`lags` must be a 1-dimensional array.\")\n            if not np.issubdtype(lags.dtype, np.integer):\n                raise TypeError(\"All values in `lags` must be integers.\")\n            if np.any(lags &lt; 1):\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n        else:\n            if forecaster_name == \"ForecasterDirectMultiVariate\":\n                raise TypeError(\n                    f\"`lags` argument must be a dict, int, 1d numpy ndarray, range, \"\n                    f\"tuple or list. Got {type(lags)}.\"\n                )\n            else:\n                raise TypeError(\n                    f\"`lags` argument must be an int, 1d numpy ndarray, range, \"\n                    f\"tuple or list. Got {type(lags)}.\"\n                )\n\n        lags = np.sort(lags)\n        lags_names = [f\"lag_{i}\" for i in lags]\n        max_lag = int(max(lags))\n\n    return lags, lags_names, max_lag\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.utils.initialize_transformer_series","title":"<code>initialize_transformer_series(forecaster_name, series_names_in_, encoding=None, transformer_series=None)</code>","text":"<p>Initialize transformer_series_ attribute for multivariate/multiseries forecasters.</p> <p>Creates a dictionary of transformers for each time series in multivariate or multiseries forecasting. Handles three cases: no transformation (None), same transformer for all series (single object), or different transformers per series (dictionary). Clones transformer objects to avoid overwriting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Name of the forecaster using this function. Special handling is applied for 'ForecasterRecursiveMultiSeries'.</p> required <code>series_names_in_</code> <code>list[str]</code> <p>Names of the time series (levels) used during training. These will be the keys in the returned transformer dictionary.</p> required <code>encoding</code> <code>str | None</code> <p>Encoding used to identify different series. Only used for ForecasterRecursiveMultiSeries. If None, creates a single '_unknown_level' entry. Defaults to None.</p> <code>None</code> <code>transformer_series</code> <code>object | dict[str, object | None] | None</code> <p>Transformer(s) to apply to series. Can be: - None: No transformation applied - Single transformer object: Same transformer cloned for all series - Dict mapping series names to transformers: Different transformer per series Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, object | None]</code> <p>Dictionary with series names as keys and transformer objects (or None) as values. Transformers are cloned to prevent overwriting.</p> <p>Warns:</p> Type Description <code>IgnoredArgumentWarning</code> <p>If transformer_series is a dict and some series_names_in_ are not present in the dict keys (those series get no transformation).</p> <p>Examples:</p> <p>No transformation:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.utils import initialize_transformer_series\n&gt;&gt;&gt; series = ['series1', 'series2', 'series3']\n&gt;&gt;&gt; result = initialize_transformer_series(\n...     forecaster_name='ForecasterDirectMultiVariate',\n...     series_names_in_=series,\n...     transformer_series=None\n... )\n&gt;&gt;&gt; print(result)\n{'series1': None, 'series2': None, 'series3': None}\n</code></pre> <p>Same transformer for all series:</p> <pre><code>&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n&gt;&gt;&gt; scaler = StandardScaler()\n&gt;&gt;&gt; result = initialize_transformer_series(\n...     forecaster_name='ForecasterDirectMultiVariate',\n...     series_names_in_=['series1', 'series2'],\n...     transformer_series=scaler\n... )\n&gt;&gt;&gt; len(result)\n2\n&gt;&gt;&gt; all(isinstance(v, StandardScaler) for v in result.values())\nTrue\n&gt;&gt;&gt; result['series1'] is result['series2']  # Different clones\nFalse\n</code></pre> <p>Different transformer per series:</p> <pre><code>&gt;&gt;&gt; from sklearn.preprocessing import MinMaxScaler\n&gt;&gt;&gt; transformers = {\n...     'series1': StandardScaler(),\n...     'series2': MinMaxScaler()\n... }\n&gt;&gt;&gt; result = initialize_transformer_series(\n...     forecaster_name='ForecasterDirectMultiVariate',\n...     series_names_in_=['series1', 'series2'],\n...     transformer_series=transformers\n... )\n&gt;&gt;&gt; isinstance(result['series1'], StandardScaler)\nTrue\n&gt;&gt;&gt; isinstance(result['series2'], MinMaxScaler)\nTrue\n</code></pre> Source code in <code>src/spotforecast2/forecaster/utils.py</code> <pre><code>def initialize_transformer_series(\n    forecaster_name: str,\n    series_names_in_: list[str],\n    encoding: str | None = None,\n    transformer_series: object | dict[str, object | None] | None = None,\n) -&gt; dict[str, object | None]:\n    \"\"\"Initialize transformer_series_ attribute for multivariate/multiseries forecasters.\n\n    Creates a dictionary of transformers for each time series in multivariate or\n    multiseries forecasting. Handles three cases: no transformation (None), same\n    transformer for all series (single object), or different transformers per series\n    (dictionary). Clones transformer objects to avoid overwriting.\n\n    Args:\n        forecaster_name: Name of the forecaster using this function. Special handling\n            is applied for 'ForecasterRecursiveMultiSeries'.\n        series_names_in_: Names of the time series (levels) used during training.\n            These will be the keys in the returned transformer dictionary.\n        encoding: Encoding used to identify different series. Only used for\n            ForecasterRecursiveMultiSeries. If None, creates a single '_unknown_level'\n            entry. Defaults to None.\n        transformer_series: Transformer(s) to apply to series. Can be:\n            - None: No transformation applied\n            - Single transformer object: Same transformer cloned for all series\n            - Dict mapping series names to transformers: Different transformer per series\n            Defaults to None.\n\n    Returns:\n        dict: Dictionary with series names as keys and transformer objects (or None)\n            as values. Transformers are cloned to prevent overwriting.\n\n    Warnings:\n        IgnoredArgumentWarning: If transformer_series is a dict and some series_names_in_\n            are not present in the dict keys (those series get no transformation).\n\n    Examples:\n        No transformation:\n        &gt;&gt;&gt; from spotforecast2.forecaster.utils import initialize_transformer_series\n        &gt;&gt;&gt; series = ['series1', 'series2', 'series3']\n        &gt;&gt;&gt; result = initialize_transformer_series(\n        ...     forecaster_name='ForecasterDirectMultiVariate',\n        ...     series_names_in_=series,\n        ...     transformer_series=None\n        ... )\n        &gt;&gt;&gt; print(result)\n        {'series1': None, 'series2': None, 'series3': None}\n\n        Same transformer for all series:\n        &gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n        &gt;&gt;&gt; scaler = StandardScaler()\n        &gt;&gt;&gt; result = initialize_transformer_series(\n        ...     forecaster_name='ForecasterDirectMultiVariate',\n        ...     series_names_in_=['series1', 'series2'],\n        ...     transformer_series=scaler\n        ... )\n        &gt;&gt;&gt; len(result)\n        2\n        &gt;&gt;&gt; all(isinstance(v, StandardScaler) for v in result.values())\n        True\n        &gt;&gt;&gt; result['series1'] is result['series2']  # Different clones\n        False\n\n        Different transformer per series:\n        &gt;&gt;&gt; from sklearn.preprocessing import MinMaxScaler\n        &gt;&gt;&gt; transformers = {\n        ...     'series1': StandardScaler(),\n        ...     'series2': MinMaxScaler()\n        ... }\n        &gt;&gt;&gt; result = initialize_transformer_series(\n        ...     forecaster_name='ForecasterDirectMultiVariate',\n        ...     series_names_in_=['series1', 'series2'],\n        ...     transformer_series=transformers\n        ... )\n        &gt;&gt;&gt; isinstance(result['series1'], StandardScaler)\n        True\n        &gt;&gt;&gt; isinstance(result['series2'], MinMaxScaler)\n        True\n    \"\"\"\n    from copy import deepcopy\n    from sklearn.base import clone\n    from spotforecast2.exceptions import IgnoredArgumentWarning\n\n    if forecaster_name == \"ForecasterRecursiveMultiSeries\":\n        if encoding is None:\n            series_names_in_ = [\"_unknown_level\"]\n        else:\n            series_names_in_ = series_names_in_ + [\"_unknown_level\"]\n\n    if transformer_series is None:\n        transformer_series_ = {serie: None for serie in series_names_in_}\n    elif not isinstance(transformer_series, dict):\n        transformer_series_ = {\n            serie: clone(transformer_series) for serie in series_names_in_\n        }\n    else:\n        transformer_series_ = {serie: None for serie in series_names_in_}\n        # Only elements already present in transformer_series_ are updated\n        transformer_series_.update(\n            {\n                k: deepcopy(v)\n                for k, v in transformer_series.items()\n                if k in transformer_series_\n            }\n        )\n\n        series_not_in_transformer_series = (\n            set(series_names_in_) - set(transformer_series.keys())\n        ) - {\"_unknown_level\"}\n        if series_not_in_transformer_series:\n            warnings.warn(\n                f\"{series_not_in_transformer_series} not present in `transformer_series`.\"\n                f\" No transformation is applied to these series.\",\n                IgnoredArgumentWarning,\n            )\n\n    return transformer_series_\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.utils.initialize_weights","title":"<code>initialize_weights(forecaster_name, estimator, weight_func, series_weights)</code>","text":"<p>Validate and initialize weight function configuration for forecasting.</p> <p>This function validates weight_func and series_weights, extracts source code from weight functions for serialization, and checks if the estimator supports sample weights in its fit method.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Name of the forecaster class.</p> required <code>estimator</code> <code>Any</code> <p>Scikit-learn compatible estimator or pipeline.</p> required <code>weight_func</code> <code>Any</code> <p>Weight function specification: - Callable: Single weight function - dict: Dictionary of weight functions (for MultiSeries forecasters) - None: No weighting</p> required <code>series_weights</code> <code>Any</code> <p>Dictionary of series-level weights (for MultiSeries forecasters). - dict: Maps series names to weight values - None: No series weighting</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Tuple containing:</p> <code>Optional[Union[str, dict]]</code> <ul> <li>weight_func: Validated weight function (or None if invalid)</li> </ul> <code>Any</code> <ul> <li>source_code_weight_func: Source code of weight function(s) for serialization (or None)</li> </ul> <code>Tuple[Any, Optional[Union[str, dict]], Any]</code> <ul> <li>series_weights: Validated series weights (or None if invalid)</li> </ul> <p>Raises:</p> Type Description <code>TypeError</code> <p>If weight_func is not Callable/dict (depending on forecaster type), or if series_weights is not a dict.</p> <p>Warns:</p> Type Description <code>IgnoredArgumentWarning</code> <p>If estimator doesn't support sample_weight.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_weights\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Simple weight function\n&gt;&gt;&gt; def custom_weights(index):\n...     return np.ones(len(index))\n&gt;&gt;&gt;\n&gt;&gt;&gt; estimator = Ridge()\n&gt;&gt;&gt; wf, source, sw = initialize_weights(\n...     \"ForecasterRecursive\", estimator, custom_weights, None\n... )\n&gt;&gt;&gt; wf is not None\nTrue\n&gt;&gt;&gt; isinstance(source, str)\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # No weight function\n&gt;&gt;&gt; wf, source, sw = initialize_weights(\n...     \"ForecasterRecursive\", estimator, None, None\n... )\n&gt;&gt;&gt; wf is None\nTrue\n&gt;&gt;&gt; source is None\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid type for non-MultiSeries forecaster\n&gt;&gt;&gt; try:\n...     initialize_weights(\"ForecasterRecursive\", estimator, \"invalid\", None)\n... except TypeError as e:\n...     print(\"Error: weight_func must be Callable\")\nError: weight_func must be Callable\n</code></pre> Source code in <code>src/spotforecast2/utils/forecaster_config.py</code> <pre><code>def initialize_weights(\n    forecaster_name: str, estimator: Any, weight_func: Any, series_weights: Any\n) -&gt; Tuple[Any, Optional[Union[str, dict]], Any]:\n    \"\"\"\n    Validate and initialize weight function configuration for forecasting.\n\n    This function validates weight_func and series_weights, extracts source code\n    from weight functions for serialization, and checks if the estimator supports\n    sample weights in its fit method.\n\n    Args:\n        forecaster_name: Name of the forecaster class.\n        estimator: Scikit-learn compatible estimator or pipeline.\n        weight_func: Weight function specification:\n            - Callable: Single weight function\n            - dict: Dictionary of weight functions (for MultiSeries forecasters)\n            - None: No weighting\n        series_weights: Dictionary of series-level weights (for MultiSeries forecasters).\n            - dict: Maps series names to weight values\n            - None: No series weighting\n\n    Returns:\n        Tuple containing:\n        - weight_func: Validated weight function (or None if invalid)\n        - source_code_weight_func: Source code of weight function(s) for serialization (or None)\n        - series_weights: Validated series weights (or None if invalid)\n\n    Raises:\n        TypeError: If weight_func is not Callable/dict (depending on forecaster type),\n            or if series_weights is not a dict.\n\n    Warnings:\n        IgnoredArgumentWarning: If estimator doesn't support sample_weight.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_weights\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Simple weight function\n        &gt;&gt;&gt; def custom_weights(index):\n        ...     return np.ones(len(index))\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; estimator = Ridge()\n        &gt;&gt;&gt; wf, source, sw = initialize_weights(\n        ...     \"ForecasterRecursive\", estimator, custom_weights, None\n        ... )\n        &gt;&gt;&gt; wf is not None\n        True\n        &gt;&gt;&gt; isinstance(source, str)\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # No weight function\n        &gt;&gt;&gt; wf, source, sw = initialize_weights(\n        ...     \"ForecasterRecursive\", estimator, None, None\n        ... )\n        &gt;&gt;&gt; wf is None\n        True\n        &gt;&gt;&gt; source is None\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid type for non-MultiSeries forecaster\n        &gt;&gt;&gt; try:\n        ...     initialize_weights(\"ForecasterRecursive\", estimator, \"invalid\", None)\n        ... except TypeError as e:\n        ...     print(\"Error: weight_func must be Callable\")\n        Error: weight_func must be Callable\n    \"\"\"\n    import inspect\n    import warnings\n    from collections.abc import Callable\n\n    # Import IgnoredArgumentWarning if available, otherwise define locally\n    try:\n        from spotforecast2.exceptions import IgnoredArgumentWarning\n    except ImportError:\n\n        class IgnoredArgumentWarning(UserWarning):\n            \"\"\"Warning for ignored arguments.\"\"\"\n\n            pass\n\n    source_code_weight_func = None\n\n    if weight_func is not None:\n        if forecaster_name in [\"ForecasterRecursiveMultiSeries\"]:\n            if not isinstance(weight_func, (Callable, dict)):\n                raise TypeError(\n                    f\"Argument `weight_func` must be a Callable or a dict of \"\n                    f\"Callables. Got {type(weight_func)}.\"\n                )\n        elif not isinstance(weight_func, Callable):\n            raise TypeError(\n                f\"Argument `weight_func` must be a Callable. Got {type(weight_func)}.\"\n            )\n\n        if isinstance(weight_func, dict):\n            source_code_weight_func = {}\n            for key in weight_func:\n                try:\n                    source_code_weight_func[key] = inspect.getsource(weight_func[key])\n                except (OSError, TypeError):\n                    # OSError: source not available, TypeError: callable class instance\n                    source_code_weight_func[key] = (\n                        f\"&lt;source unavailable: {weight_func[key]!r}&gt;\"\n                    )\n        else:\n            try:\n                source_code_weight_func = inspect.getsource(weight_func)\n            except (OSError, TypeError):\n                # OSError: source not available (e.g., built-in, lambda in REPL)\n                # TypeError: callable class instance (e.g., WeightFunction)\n                # In these cases, we can't get source but the object can still be pickled\n                source_code_weight_func = f\"&lt;source unavailable: {weight_func!r}&gt;\"\n\n        if \"sample_weight\" not in inspect.signature(estimator.fit).parameters:\n            warnings.warn(\n                f\"Argument `weight_func` is ignored since estimator {estimator} \"\n                f\"does not accept `sample_weight` in its `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n            weight_func = None\n            source_code_weight_func = None\n\n    if series_weights is not None:\n        if not isinstance(series_weights, dict):\n            raise TypeError(\n                f\"Argument `series_weights` must be a dict of floats or ints.\"\n                f\"Got {type(series_weights)}.\"\n            )\n        if \"sample_weight\" not in inspect.signature(estimator.fit).parameters:\n            warnings.warn(\n                f\"Argument `series_weights` is ignored since estimator {estimator} \"\n                f\"does not accept `sample_weight` in its `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n            series_weights = None\n\n    return weight_func, source_code_weight_func, series_weights\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.utils.initialize_window_features","title":"<code>initialize_window_features(window_features)</code>","text":"<p>Check window_features argument input and generate the corresponding list.</p> <p>This function validates window feature objects and extracts their metadata, ensuring they have the required attributes (window_sizes, features_names) and methods (transform_batch, transform) for proper forecasting operations.</p> <p>Parameters:</p> Name Type Description Default <code>window_features</code> <code>Any</code> <p>Classes used to create window features. Can be a single object or a list of objects. Each object must have <code>window_sizes</code>, <code>features_names</code> attributes and <code>transform_batch</code>, <code>transform</code> methods.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[Optional[List[object]], Optional[List[str]], Optional[int]]</code> <p>A tuple containing: - window_features (list or None): List of classes used to create window features. - window_features_names (list or None): List with all the features names of the window features. - max_size_window_features (int or None): Maximum value of the <code>window_sizes</code> attribute of all classes.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>window_features</code> is an empty list.</p> <code>ValueError</code> <p>If a window feature is missing required attributes or methods.</p> <code>TypeError</code> <p>If <code>window_sizes</code> or <code>features_names</code> have incorrect types.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.forecaster.preprocessing import RollingFeatures\n&gt;&gt;&gt; wf = RollingFeatures(stats=['mean', 'std'], window_sizes=[7, 14])\n&gt;&gt;&gt; wf_list, names, max_size = initialize_window_features(wf)\n&gt;&gt;&gt; print(f\"Max window size: {max_size}\")\nMax window size: 14\n&gt;&gt;&gt; print(f\"Number of features: {len(names)}\")\nNumber of features: 4\n</code></pre> <p>Multiple window features:</p> <pre><code>&gt;&gt;&gt; wf1 = RollingFeatures(stats=['mean'], window_sizes=7)\n&gt;&gt;&gt; wf2 = RollingFeatures(stats=['max', 'min'], window_sizes=3)\n&gt;&gt;&gt; wf_list, names, max_size = initialize_window_features([wf1, wf2])\n&gt;&gt;&gt; print(f\"Max window size: {max_size}\")\nMax window size: 7\n</code></pre> Source code in <code>src/spotforecast2/forecaster/utils.py</code> <pre><code>def initialize_window_features(\n    window_features: Any,\n) -&gt; Tuple[Optional[List[object]], Optional[List[str]], Optional[int]]:\n    \"\"\"Check window_features argument input and generate the corresponding list.\n\n    This function validates window feature objects and extracts their metadata,\n    ensuring they have the required attributes (window_sizes, features_names) and\n    methods (transform_batch, transform) for proper forecasting operations.\n\n    Args:\n        window_features: Classes used to create window features. Can be a single\n            object or a list of objects. Each object must have `window_sizes`,\n            `features_names` attributes and `transform_batch`, `transform` methods.\n\n    Returns:\n        tuple: A tuple containing:\n            - window_features (list or None): List of classes used to create window features.\n            - window_features_names (list or None): List with all the features names of the window features.\n            - max_size_window_features (int or None): Maximum value of the `window_sizes` attribute of all classes.\n\n    Raises:\n        ValueError: If `window_features` is an empty list.\n        ValueError: If a window feature is missing required attributes or methods.\n        TypeError: If `window_sizes` or `features_names` have incorrect types.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.forecaster.preprocessing import RollingFeatures\n        &gt;&gt;&gt; wf = RollingFeatures(stats=['mean', 'std'], window_sizes=[7, 14])\n        &gt;&gt;&gt; wf_list, names, max_size = initialize_window_features(wf)\n        &gt;&gt;&gt; print(f\"Max window size: {max_size}\")\n        Max window size: 14\n        &gt;&gt;&gt; print(f\"Number of features: {len(names)}\")\n        Number of features: 4\n\n        Multiple window features:\n        &gt;&gt;&gt; wf1 = RollingFeatures(stats=['mean'], window_sizes=7)\n        &gt;&gt;&gt; wf2 = RollingFeatures(stats=['max', 'min'], window_sizes=3)\n        &gt;&gt;&gt; wf_list, names, max_size = initialize_window_features([wf1, wf2])\n        &gt;&gt;&gt; print(f\"Max window size: {max_size}\")\n        Max window size: 7\n    \"\"\"\n\n    needed_atts = [\"window_sizes\", \"features_names\"]\n    needed_methods = [\"transform_batch\", \"transform\"]\n\n    max_window_sizes = None\n    window_features_names = None\n    max_size_window_features = None\n    if window_features is not None:\n        if isinstance(window_features, list) and len(window_features) &lt; 1:\n            raise ValueError(\n                \"Argument `window_features` must contain at least one element.\"\n            )\n        if not isinstance(window_features, list):\n            window_features = [window_features]\n\n        link_to_docs = (\n            \"\\nVisit the documentation for more information about how to create \"\n            \"custom window features\\n\"\n        )\n\n        max_window_sizes = []\n        window_features_names = []\n        needed_atts_set = set(needed_atts)\n        needed_methods_set = set(needed_methods)\n        for wf in window_features:\n            wf_name = type(wf).__name__\n            atts_methods = set(dir(wf))\n            if not needed_atts_set.issubset(atts_methods):\n                raise ValueError(\n                    f\"{wf_name} must have the attributes: {needed_atts}.\" + link_to_docs\n                )\n            if not needed_methods_set.issubset(atts_methods):\n                raise ValueError(\n                    f\"{wf_name} must have the methods: {needed_methods}.\" + link_to_docs\n                )\n\n            window_sizes = wf.window_sizes\n            if not isinstance(window_sizes, (int, list)):\n                raise TypeError(\n                    f\"Attribute `window_sizes` of {wf_name} must be an int or a list \"\n                    f\"of ints. Got {type(window_sizes)}.\" + link_to_docs\n                )\n\n            if isinstance(window_sizes, int):\n                if window_sizes &lt; 1:\n                    raise ValueError(\n                        f\"If argument `window_sizes` is an integer, it must be equal to or \"\n                        f\"greater than 1. Got {window_sizes} from {wf_name}.\"\n                        + link_to_docs\n                    )\n                max_window_sizes.append(window_sizes)\n            else:\n                if not all(isinstance(ws, int) for ws in window_sizes) or not all(\n                    ws &gt;= 1 for ws in window_sizes\n                ):\n                    raise ValueError(\n                        f\"If argument `window_sizes` is a list, all elements must be integers \"\n                        f\"equal to or greater than 1. Got {window_sizes} from {wf_name}.\"\n                        + link_to_docs\n                    )\n                max_window_sizes.append(max(window_sizes))\n\n            features_names = wf.features_names\n            if not isinstance(features_names, (str, list)):\n                raise TypeError(\n                    f\"Attribute `features_names` of {wf_name} must be a str or \"\n                    f\"a list of strings. Got {type(features_names)}.\" + link_to_docs\n                )\n            if isinstance(features_names, str):\n                window_features_names.append(features_names)\n            else:\n                if not all(isinstance(fn, str) for fn in features_names):\n                    raise TypeError(\n                        f\"If argument `features_names` is a list, all elements \"\n                        f\"must be strings. Got {features_names} from {wf_name}.\"\n                        + link_to_docs\n                    )\n                window_features_names.extend(features_names)\n\n        max_size_window_features = max(max_window_sizes)\n        if len(set(window_features_names)) != len(window_features_names):\n            raise ValueError(\n                f\"All window features names must be unique. Got {window_features_names}.\"\n            )\n\n    return window_features, window_features_names, max_size_window_features\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.utils.input_to_frame","title":"<code>input_to_frame(data, input_name)</code>","text":"<p>Convert input data to a pandas DataFrame.</p> <p>This function ensures consistent DataFrame format for internal processing. If data is already a DataFrame, it's returned as-is. If it's a Series, it's converted to a single-column DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Series, DataFrame]</code> <p>Input data as pandas Series or DataFrame.</p> required <code>input_name</code> <code>str</code> <p>Name of the input data type. Accepted values are: - 'y': Target time series - 'last_window': Last window for prediction - 'exog': Exogenous variables</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame version of the input data. For Series input, uses the series</p> <code>DataFrame</code> <p>name if available, otherwise uses a default name based on input_name.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2.utils.data_transform import input_to_frame\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series with name\n&gt;&gt;&gt; y = pd.Series([1, 2, 3], name=\"sales\")\n&gt;&gt;&gt; df = input_to_frame(y, input_name=\"y\")\n&gt;&gt;&gt; df.columns.tolist()\n['sales']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series without name (uses default)\n&gt;&gt;&gt; y_no_name = pd.Series([1, 2, 3])\n&gt;&gt;&gt; df = input_to_frame(y_no_name, input_name=\"y\")\n&gt;&gt;&gt; df.columns.tolist()\n['y']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DataFrame (returned as-is)\n&gt;&gt;&gt; df_input = pd.DataFrame({\"temp\": [20, 21], \"humidity\": [50, 55]})\n&gt;&gt;&gt; df_output = input_to_frame(df_input, input_name=\"exog\")\n&gt;&gt;&gt; df_output.columns.tolist()\n['temp', 'humidity']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Exog series without name\n&gt;&gt;&gt; exog = pd.Series([10, 20, 30])\n&gt;&gt;&gt; df_exog = input_to_frame(exog, input_name=\"exog\")\n&gt;&gt;&gt; df_exog.columns.tolist()\n['exog']\n</code></pre> Source code in <code>src/spotforecast2/utils/data_transform.py</code> <pre><code>def input_to_frame(\n    data: Union[pd.Series, pd.DataFrame], input_name: str\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert input data to a pandas DataFrame.\n\n    This function ensures consistent DataFrame format for internal processing.\n    If data is already a DataFrame, it's returned as-is. If it's a Series,\n    it's converted to a single-column DataFrame.\n\n    Args:\n        data: Input data as pandas Series or DataFrame.\n        input_name: Name of the input data type. Accepted values are:\n            - 'y': Target time series\n            - 'last_window': Last window for prediction\n            - 'exog': Exogenous variables\n\n    Returns:\n        DataFrame version of the input data. For Series input, uses the series\n        name if available, otherwise uses a default name based on input_name.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2.utils.data_transform import input_to_frame\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series with name\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3], name=\"sales\")\n        &gt;&gt;&gt; df = input_to_frame(y, input_name=\"y\")\n        &gt;&gt;&gt; df.columns.tolist()\n        ['sales']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series without name (uses default)\n        &gt;&gt;&gt; y_no_name = pd.Series([1, 2, 3])\n        &gt;&gt;&gt; df = input_to_frame(y_no_name, input_name=\"y\")\n        &gt;&gt;&gt; df.columns.tolist()\n        ['y']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DataFrame (returned as-is)\n        &gt;&gt;&gt; df_input = pd.DataFrame({\"temp\": [20, 21], \"humidity\": [50, 55]})\n        &gt;&gt;&gt; df_output = input_to_frame(df_input, input_name=\"exog\")\n        &gt;&gt;&gt; df_output.columns.tolist()\n        ['temp', 'humidity']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Exog series without name\n        &gt;&gt;&gt; exog = pd.Series([10, 20, 30])\n        &gt;&gt;&gt; df_exog = input_to_frame(exog, input_name=\"exog\")\n        &gt;&gt;&gt; df_exog.columns.tolist()\n        ['exog']\n    \"\"\"\n    output_col_name = {\"y\": \"y\", \"last_window\": \"y\", \"exog\": \"exog\"}\n\n    if isinstance(data, pd.Series):\n        data = data.to_frame(\n            name=data.name if data.name is not None else output_col_name[input_name]\n        )\n\n    return data\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.utils.predict_multivariate","title":"<code>predict_multivariate(forecasters, steps_ahead, exog=None, show_progress=False)</code>","text":"<p>Generate multi-output predictions using multiple baseline forecasters.</p> <p>Parameters:</p> Name Type Description Default <code>forecasters</code> <code>dict</code> <p>Dictionary of fitted forecaster instances (one per target). Keys are target names, values are the fitted forecasters (e.g., ForecasterRecursive, ForecasterEquivalentDate).</p> required <code>steps_ahead</code> <code>int</code> <p>Number of steps to forecast.</p> required <code>exog</code> <code>DataFrame</code> <p>Exogenous variables for prediction. If provided, will be passed to each forecaster's predict method.</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>Show progress bar while predicting per target forecaster. Default: False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with predictions for all targets.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from spotforecast2.forecaster.utils import predict_multivariate\n&gt;&gt;&gt; y1 = pd.Series([1, 2, 3, 4, 5])\n&gt;&gt;&gt; y2 = pd.Series([2, 4, 6, 8, 10])\n&gt;&gt;&gt; f1 = ForecasterRecursive(estimator=LinearRegression(), lags=2)\n&gt;&gt;&gt; f2 = ForecasterRecursive(estimator=LinearRegression(), lags=2)\n&gt;&gt;&gt; f1.fit(y=y1)\n&gt;&gt;&gt; f2.fit(y=y2)\n&gt;&gt;&gt; forecasters = {'target1': f1, 'target2': f2}\n&gt;&gt;&gt; predictions = predict_multivariate(forecasters, steps_ahead=2)\n&gt;&gt;&gt; predictions\n   target1  target2\n5      6.0     12.0\n6      7.0     14.0\n</code></pre> Source code in <code>src/spotforecast2/forecaster/utils.py</code> <pre><code>def predict_multivariate(\n    forecasters: dict[str, Any],\n    steps_ahead: int,\n    exog: pd.DataFrame | None = None,\n    show_progress: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate multi-output predictions using multiple baseline forecasters.\n\n    Args:\n        forecasters (dict): Dictionary of fitted forecaster instances (one per target).\n            Keys are target names, values are the fitted forecasters (e.g.,\n            ForecasterRecursive, ForecasterEquivalentDate).\n        steps_ahead (int): Number of steps to forecast.\n        exog (pd.DataFrame, optional): Exogenous variables for prediction.\n            If provided, will be passed to each forecaster's predict method.\n        show_progress (bool, optional): Show progress bar while predicting\n            per target forecaster. Default: False.\n\n    Returns:\n        pd.DataFrame: DataFrame with predictions for all targets.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from spotforecast2.forecaster.utils import predict_multivariate\n        &gt;&gt;&gt; y1 = pd.Series([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; y2 = pd.Series([2, 4, 6, 8, 10])\n        &gt;&gt;&gt; f1 = ForecasterRecursive(estimator=LinearRegression(), lags=2)\n        &gt;&gt;&gt; f2 = ForecasterRecursive(estimator=LinearRegression(), lags=2)\n        &gt;&gt;&gt; f1.fit(y=y1)\n        &gt;&gt;&gt; f2.fit(y=y2)\n        &gt;&gt;&gt; forecasters = {'target1': f1, 'target2': f2}\n        &gt;&gt;&gt; predictions = predict_multivariate(forecasters, steps_ahead=2)\n        &gt;&gt;&gt; predictions\n           target1  target2\n        5      6.0     12.0\n        6      7.0     14.0\n    \"\"\"\n\n    if not forecasters:\n        return pd.DataFrame()\n\n    predictions = {}\n\n    target_iter = forecasters.items()\n    if show_progress and tqdm is not None:\n        target_iter = tqdm(\n            forecasters.items(),\n            desc=\"Predicting targets\",\n            unit=\"model\",\n        )\n\n    for target, forecaster in target_iter:\n        # Generate predictions for this target\n        if exog is not None:\n            pred = forecaster.predict(steps=steps_ahead, exog=exog)\n        else:\n            pred = forecaster.predict(steps=steps_ahead)\n        predictions[target] = pred\n\n    # Combine into a single DataFrame\n    return pd.concat(predictions, axis=1)\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.utils.prepare_steps_direct","title":"<code>prepare_steps_direct(max_step, steps=None)</code>","text":"<p>Prepare list of steps to be predicted in Direct Forecasters.</p> <p>Parameters:</p> Name Type Description Default <code>max_step</code> <code>int | list[int] | ndarray</code> <p>int, list, numpy ndarray Maximum number of future steps the forecaster will predict when using predict methods.</p> required <code>steps</code> <code>int | list[int] | None</code> <p>int, list, None, default None Predict n steps. The value of <code>steps</code> must be less than or equal to the value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list   are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at   initialization.</li> </ul> <code>None</code> <p>Returns:</p> Type Description <code>list[int]</code> <p>list[int]: Steps to be predicted.</p> Source code in <code>src/spotforecast2/forecaster/utils.py</code> <pre><code>def prepare_steps_direct(\n    max_step: int | list[int] | np.ndarray, steps: int | list[int] | None = None\n) -&gt; list[int]:\n    \"\"\"\n    Prepare list of steps to be predicted in Direct Forecasters.\n\n    Args:\n        max_step: int, list, numpy ndarray\n            Maximum number of future steps the forecaster will predict\n            when using predict methods.\n        steps: int, list, None, default None\n            Predict n steps. The value of `steps` must be less than or equal to the\n            value of steps defined when initializing the forecaster. Starts at 1.\n\n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list\n              are predicted.\n            - If `None`: As many steps are predicted as were defined at\n              initialization.\n\n    Returns:\n        list[int]:\n            Steps to be predicted.\n    \"\"\"\n\n    if isinstance(steps, int):\n        steps_direct = list(range(1, steps + 1))\n    elif steps is None:\n        if isinstance(max_step, int):\n            steps_direct = list(range(1, max_step + 1))\n        else:\n            steps_direct = [int(s) for s in max_step]\n    elif isinstance(steps, list):\n        steps_direct = []\n        for step in steps:\n            if not isinstance(step, (int, np.integer)):\n                raise TypeError(\n                    f\"`steps` argument must be an int, a list of ints or `None`. \"\n                    f\"Got {type(steps)}.\"\n                )\n            steps_direct.append(int(step))\n\n    return steps_direct\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.utils.select_n_jobs_fit_forecaster","title":"<code>select_n_jobs_fit_forecaster(forecaster_name, estimator)</code>","text":"<p>Select the number of jobs to run in parallel.</p> Source code in <code>src/spotforecast2/forecaster/utils.py</code> <pre><code>def select_n_jobs_fit_forecaster(forecaster_name, estimator):\n    \"\"\"\n    Select the number of jobs to run in parallel.\n    \"\"\"\n    import os\n\n    return os.cpu_count() or 1\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.utils.set_skforecast_warnings","title":"<code>set_skforecast_warnings(suppress_warnings, action='ignore')</code>","text":"<p>Suppress spotforecast warnings.</p> <p>Parameters:</p> Name Type Description Default <code>suppress_warnings</code> <code>bool</code> <p>bool If True, spotforecast warnings will be suppressed.</p> required <code>action</code> <code>str</code> <p>str, default 'ignore' Action to take regarding the warnings.</p> <code>'ignore'</code> Source code in <code>spotforecast2_safe/exceptions.py</code> <pre><code>def set_skforecast_warnings(suppress_warnings: bool, action: str = \"ignore\") -&gt; None:\n    \"\"\"\n    Suppress spotforecast warnings.\n\n    Args:\n        suppress_warnings: bool\n            If True, spotforecast warnings will be suppressed.\n        action: str, default 'ignore'\n            Action to take regarding the warnings.\n    \"\"\"\n    if suppress_warnings:\n        for category in warn_skforecast_categories:\n            warnings.simplefilter(action, category=category)\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.utils.transform_dataframe","title":"<code>transform_dataframe(df, transformer, fit=False, inverse_transform=False)</code>","text":"<p>Transform raw values of pandas DataFrame with a scikit-learn alike transformer, preprocessor or ColumnTransformer.</p> <p>The transformer used must have the following methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to be transformed.</p> required <code>transformer</code> <code>object</code> <p>Scikit-learn alike transformer, preprocessor, or ColumnTransformer. Must implement fit, transform, fit_transform and inverse_transform.</p> required <code>fit</code> <code>bool</code> <p>Train the transformer before applying it. Defaults to False.</p> <code>False</code> <code>inverse_transform</code> <code>bool</code> <p>Transform back the data to the original representation. This is not available when using transformers of class scikit-learn ColumnTransformers. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Transformed DataFrame.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If df is not a pandas DataFrame.</p> <code>ValueError</code> <p>If inverse_transform is requested for ColumnTransformer.</p> Source code in <code>spotforecast2_safe/utils/data_transform.py</code> <pre><code>def transform_dataframe(\n    df: pd.DataFrame,\n    transformer: object,\n    fit: bool = False,\n    inverse_transform: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform raw values of pandas DataFrame with a scikit-learn alike\n    transformer, preprocessor or ColumnTransformer.\n\n    The transformer used must have the following methods: fit, transform,\n    fit_transform and inverse_transform. ColumnTransformers are not allowed\n    since they do not have inverse_transform method.\n\n    Args:\n        df: DataFrame to be transformed.\n        transformer: Scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n            Must implement fit, transform, fit_transform and inverse_transform.\n        fit: Train the transformer before applying it. Defaults to False.\n        inverse_transform: Transform back the data to the original representation.\n            This is not available when using transformers of class\n            scikit-learn ColumnTransformers. Defaults to False.\n\n    Returns:\n        Transformed DataFrame.\n\n    Raises:\n        TypeError: If df is not a pandas DataFrame.\n        ValueError: If inverse_transform is requested for ColumnTransformer.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(f\"`df` argument must be a pandas DataFrame. Got {type(df)}\")\n\n    if transformer is None:\n        return df\n\n    # Check for ColumnTransformer by class name to avoid importing sklearn\n    is_column_transformer = type(\n        transformer\n    ).__name__ == \"ColumnTransformer\" or hasattr(transformer, \"transformers\")\n\n    if inverse_transform and is_column_transformer:\n        raise ValueError(\n            \"`inverse_transform` is not available when using ColumnTransformers.\"\n        )\n\n    if not inverse_transform:\n        if fit:\n            values_transformed = transformer.fit_transform(df)\n        else:\n            values_transformed = transformer.transform(df)\n    else:\n        values_transformed = transformer.inverse_transform(df)\n\n    if hasattr(values_transformed, \"toarray\"):\n        # If the returned values are in sparse matrix format, it is converted to dense\n        values_transformed = values_transformed.toarray()\n\n    if isinstance(values_transformed, pd.DataFrame):\n        df_transformed = values_transformed\n    else:\n        df_transformed = pd.DataFrame(\n            values_transformed, index=df.index, columns=df.columns\n        )\n\n    return df_transformed\n</code></pre>"},{"location":"api/forecaster/#spotforecast2.forecaster.utils.transform_numpy","title":"<code>transform_numpy(array, transformer, fit=False, inverse_transform=False)</code>","text":"<p>Transform raw values of a numpy ndarray with a scikit-learn alike transformer, preprocessor or ColumnTransformer. The transformer used must have the following methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>numpy ndarray Array to be transformed.</p> required <code>transformer</code> <code>object | None</code> <p>scikit-learn alike transformer, preprocessor, or ColumnTransformer. Scikit-learn alike transformer (preprocessor) with methods: fit, transform, fit_transform and inverse_transform.</p> required <p>fit: bool, default False     Train the transformer before applying it. inverse_transform: bool, default False     Transform back the data to the original representation. This is not available     when using transformers of class scikit-learn ColumnTransformers.</p>"},{"location":"api/forecaster/#spotforecast2.forecaster.utils.transform_numpy--returns","title":"Returns","text":"<p>array_transformed : numpy ndarray     Transformed array.</p> Source code in <code>src/spotforecast2/forecaster/utils.py</code> <pre><code>def transform_numpy(\n    array: np.ndarray,\n    transformer: object | None,\n    fit: bool = False,\n    inverse_transform: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"\n    Transform raw values of a numpy ndarray with a scikit-learn alike\n    transformer, preprocessor or ColumnTransformer. The transformer used must\n    have the following methods: fit, transform, fit_transform and\n    inverse_transform. ColumnTransformers are not allowed since they do not\n    have inverse_transform method.\n\n    Args:\n        array: numpy ndarray\n            Array to be transformed.\n        transformer: scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n            Scikit-learn alike transformer (preprocessor) with methods: fit, transform,\n            fit_transform and inverse_transform.\n    fit: bool, default False\n        Train the transformer before applying it.\n    inverse_transform: bool, default False\n        Transform back the data to the original representation. This is not available\n        when using transformers of class scikit-learn ColumnTransformers.\n\n    Returns\n    -------\n    array_transformed : numpy ndarray\n        Transformed array.\n\n    \"\"\"\n\n    if transformer is None:\n        return array\n\n    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"`array` argument must be a numpy ndarray. Got {type(array)}\")\n\n    original_ndim = array.ndim\n    original_shape = array.shape\n    reshaped_for_inverse = False\n\n    if original_ndim == 1:\n        array = array.reshape(-1, 1)\n\n    if inverse_transform and isinstance(transformer, ColumnTransformer):\n        raise ValueError(\n            \"`inverse_transform` is not available when using ColumnTransformers.\"\n        )\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\",\n            message=\"X does not have valid feature names\",\n            category=UserWarning,\n        )\n        if not inverse_transform:\n            if fit:\n                array_transformed = transformer.fit_transform(array)\n            else:\n                array_transformed = transformer.transform(array)\n        else:\n            # Vectorized inverse transformation for 2D arrays with multiple columns.\n            # Reshape to single column, transform, and reshape back.\n            # This is faster than applying the transformer column by column.\n            if array.shape[1] &gt; 1:\n                array = array.reshape(-1, 1)\n                reshaped_for_inverse = True\n            array_transformed = transformer.inverse_transform(array)\n\n    if hasattr(array_transformed, \"toarray\"):\n        # If the returned values are in sparse matrix format, it is converted to dense\n        array_transformed = array_transformed.toarray()\n\n    if isinstance(array_transformed, (pd.Series, pd.DataFrame)):\n        array_transformed = array_transformed.to_numpy()\n\n    # Reshape back to original shape only if we reshaped for inverse_transform\n    if reshaped_for_inverse:\n        array_transformed = array_transformed.reshape(original_shape)\n\n    if original_ndim == 1:\n        array_transformed = array_transformed.ravel()\n\n    return array_transformed\n</code></pre>"},{"location":"api/model_selection/","title":"Model Selection Module","text":""},{"location":"api/model_selection/#model-selection-for-safety-critical-forecasting","title":"Model Selection for Safety-Critical Forecasting","text":"<p>The model selection module provides robust validation and cross-validation tools designed for production environments where forecast reliability is paramount. These tools enable rigorous model evaluation through time series backtesting, ensuring models perform consistently across different temporal conditions.</p>"},{"location":"api/model_selection/#validation-backtesting-forecaster","title":"Validation: Backtesting Forecaster","text":""},{"location":"api/model_selection/#spotforecast2_safe.model_selection.backtesting_forecaster","title":"<code>spotforecast2_safe.model_selection.backtesting_forecaster(forecaster, y, cv, metric, exog=None, interval=None, interval_method='bootstrapping', n_boot=250, use_in_sample_residuals=True, use_binned_residuals=True, random_state=123, return_predictors=False, n_jobs='auto', verbose=False, show_progress=True, suppress_warnings=False)</code>","text":"<p>Backtesting of forecaster model following the folds generated by the TimeSeriesFold class and using the metric(s) provided.</p> <p>If <code>forecaster</code> is already trained and <code>initial_train_size</code> is set to <code>None</code> in the TimeSeriesFold class, no initial train will be done and all data will be used to evaluate the model. However, the first <code>len(forecaster.last_window)</code> observations are needed to create the initial predictors, so no predictions are calculated for them.</p> <p>A copy of the original forecaster is created so that it is not modified during the process.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>(ForecasterRecursive, ForecasterDirect, ForecasterEquivalentDate)</code> <p>Forecaster model.</p> required <code>y</code> <code>Series</code> <p>Training time series.</p> required <code>cv</code> <code>TimeSeriesFold</code> <p>TimeSeriesFold object with the information needed to split the data into folds.</p> required <code>metric</code> <code>str | Callable | list</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>str</code>: {'mean_squared_error', 'mean_absolute_error',   'mean_absolute_percentage_error', 'mean_squared_log_error',   'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}</li> <li>If <code>Callable</code>: Function with arguments <code>y_true</code>, <code>y_pred</code> and <code>y_train</code>   (Optional) that returns a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>exog</code> <code>Series | DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i]. Defaults to None.</p> <code>None</code> <code>interval</code> <code>float | list | tuple | str | object</code> <p>Specifies whether probabilistic predictions should be estimated and the method to use. The following options are supported:</p> <ul> <li>If <code>float</code>, represents the nominal (expected) coverage (between 0 and 1). For instance, <code>interval=0.95</code> corresponds to <code>[2.5, 97.5]</code> percentiles.</li> <li>If <code>list</code> or <code>tuple</code>: Sequence of percentiles to compute, each value must   be between 0 and 100 inclusive. For example, a 95% confidence interval can   be specified as <code>interval = [2.5, 97.5]</code> or multiple percentiles (e.g. 10,   50 and 90) as <code>interval = [10, 50, 90]</code>.</li> <li>If 'bootstrapping' (str): <code>n_boot</code> bootstrapping predictions will be   generated.</li> <li>If scipy.stats distribution object, the distribution parameters will   be estimated for each prediction.</li> <li>If None, no probabilistic predictions are estimated. Defaults to None.</li> </ul> <code>None</code> <code>interval_method</code> <code>str</code> <p>Technique used to estimate prediction intervals. Available options:</p> <ul> <li>'bootstrapping': Bootstrapping is used to generate prediction intervals.</li> <li>'conformal': Employs the conformal prediction split method for   interval estimation. Defaults to 'bootstrapping'.</li> </ul> <code>'bootstrapping'</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations to perform when estimating prediction intervals. Defaults to 250.</p> <code>250</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions. If <code>False</code>, out of sample residuals (calibration) are used. Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method. Defaults to True.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values (binned selection). If <code>False</code>, residuals are selected randomly. Defaults to True.</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generator to ensure reproducibility. Defaults to 123.</p> <code>123</code> <code>return_predictors</code> <code>bool</code> <p>If <code>True</code>, the predictors used to make the predictions are also returned. Defaults to False.</p> <code>False</code> <code>n_jobs</code> <code>int | str</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function <code>skforecast.utils.select_n_jobs_backtesting</code>. Defaults to 'auto'.</p> <code>'auto'</code> <code>verbose</code> <code>bool</code> <p>Print number of folds and index of training and validation sets used for backtesting. Defaults to False.</p> <code>False</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar. Defaults to True.</p> <code>True</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, spotforecast warnings will be suppressed during the backtesting process. See <code>spotforecast.exceptions.warn_skforecast_categories</code> for more information. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>(DataFrame, DataFrame)</code> <ul> <li>metric_values: Value(s) of the metric(s).</li> <li> <p>backtest_predictions: Value of predictions. The DataFrame includes   the following columns:</p> </li> <li> <p>fold: Indicates the fold number where the prediction was made.</p> </li> <li>pred: Predicted values for the corresponding series and time steps.</li> </ul> <p>If <code>interval</code> is not <code>None</code>, additional columns are included depending   on the method:</p> <ul> <li>For <code>float</code>: Columns <code>lower_bound</code> and <code>upper_bound</code>.</li> <li>For <code>list</code> or <code>tuple</code> of 2 elements: Columns <code>lower_bound</code> and     <code>upper_bound</code>.</li> <li>For <code>list</code> or <code>tuple</code> with multiple percentiles: One column per     percentile (e.g., <code>p_10</code>, <code>p_50</code>, <code>p_90</code>).</li> <li>For <code>'bootstrapping'</code>: One column per bootstrapping iteration     (e.g., <code>pred_boot_0</code>, <code>pred_boot_1</code>, ..., <code>pred_boot_n</code>).</li> <li>For <code>scipy.stats</code> distribution objects: One column for each     estimated parameter of the distribution (e.g., <code>loc</code>, <code>scale</code>).</li> </ul> <p>If <code>return_predictors</code> is <code>True</code>, one column per predictor is created.</p> <p>Depending on the relation between <code>steps</code> and <code>fold_stride</code>, the output   may include repeated indexes (if <code>fold_stride &lt; steps</code>) or gaps   (if <code>fold_stride &gt; steps</code>). See Notes below for more details.</p> Notes <p>Note on <code>fold_stride</code> vs. <code>steps</code>:</p> <ul> <li>If <code>fold_stride == steps</code>, test sets are placed back-to-back without overlap.   Each observation appears only once in the output DataFrame, so the   index is unique.</li> <li>If <code>fold_stride &lt; steps</code>, test sets overlap. Multiple forecasts are   generated for the same observations and, therefore, the output   DataFrame contains repeated indexes.</li> <li>If <code>fold_stride &gt; steps</code>, there are gaps between consecutive test sets.   Some observations in the series will not have associated predictions,   so the output DataFrame has non-contiguous indexes.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from spotforecast2_safe.model_selection import backtesting_forecaster, TimeSeriesFold\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; forecaster = ForecasterRecursive(\n...     estimator=RandomForestRegressor(random_state=123),\n...     lags=2\n... )\n&gt;&gt;&gt; cv = TimeSeriesFold(\n...     steps=2,\n...     initial_train_size=5,\n...     refit=False\n... )\n&gt;&gt;&gt; metric_values, backtest_predictions = backtesting_forecaster(\n...     forecaster=forecaster,\n...     y=y,\n...     cv=cv,\n...     metric='mean_squared_error'\n... )\n&gt;&gt;&gt; metric_values\n   mean_squared_error\n0            0.201334\n&gt;&gt;&gt; backtest_predictions\n   fold  pred\n5     0  5.18\n6     0  6.10\n7     1  7.36\n8     1  8.40\n9     2  9.31\n</code></pre> Source code in <code>spotforecast2_safe/model_selection/validation.py</code> <pre><code>def backtesting_forecaster(\n    forecaster: object,\n    y: pd.Series,\n    cv: TimeSeriesFold,\n    metric: str | Callable | list[str | Callable],\n    exog: pd.Series | pd.DataFrame | None = None,\n    interval: float | list[float] | tuple[float] | str | object | None = None,\n    interval_method: str = \"bootstrapping\",\n    n_boot: int = 250,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    random_state: int = 123,\n    return_predictors: bool = False,\n    n_jobs: int | str = \"auto\",\n    verbose: bool = False,\n    show_progress: bool = True,\n    suppress_warnings: bool = False,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Backtesting of forecaster model following the folds generated by the TimeSeriesFold\n    class and using the metric(s) provided.\n\n    If `forecaster` is already trained and `initial_train_size` is set to `None` in the\n    TimeSeriesFold class, no initial train will be done and all data will be used\n    to evaluate the model. However, the first `len(forecaster.last_window)` observations\n    are needed to create the initial predictors, so no predictions are calculated for\n    them.\n\n    A copy of the original forecaster is created so that it is not modified during\n    the process.\n\n    Args:\n        forecaster (ForecasterRecursive, ForecasterDirect, ForecasterEquivalentDate):\n            Forecaster model.\n        y (pd.Series): Training time series.\n        cv (TimeSeriesFold): TimeSeriesFold object with the information needed to\n            split the data into folds.\n        metric (str | Callable | list): Metric used to quantify the goodness of fit\n            of the model.\n\n            - If `str`: {'mean_squared_error', 'mean_absolute_error',\n              'mean_absolute_percentage_error', 'mean_squared_log_error',\n              'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n            - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n              (Optional) that returns a float.\n            - If `list`: List containing multiple strings and/or Callables.\n        exog (pd.Series | pd.DataFrame, optional): Exogenous variable/s included as\n            predictor/s. Must have the same number of observations as `y` and should\n            be aligned so that y[i] is regressed on exog[i]. Defaults to None.\n        interval (float | list | tuple | str | object, optional): Specifies whether\n            probabilistic predictions should be estimated and the method to use.\n            The following options are supported:\n\n            - If `float`, represents the nominal (expected) coverage (between 0 and 1).\n            For instance, `interval=0.95` corresponds to `[2.5, 97.5]` percentiles.\n            - If `list` or `tuple`: Sequence of percentiles to compute, each value must\n              be between 0 and 100 inclusive. For example, a 95% confidence interval can\n              be specified as `interval = [2.5, 97.5]` or multiple percentiles (e.g. 10,\n              50 and 90) as `interval = [10, 50, 90]`.\n            - If 'bootstrapping' (str): `n_boot` bootstrapping predictions will be\n              generated.\n            - If scipy.stats distribution object, the distribution parameters will\n              be estimated for each prediction.\n            - If None, no probabilistic predictions are estimated.\n            Defaults to None.\n        interval_method (str, optional): Technique used to estimate prediction\n            intervals. Available options:\n\n            - 'bootstrapping': Bootstrapping is used to generate prediction intervals.\n            - 'conformal': Employs the conformal prediction split method for\n              interval estimation.\n            Defaults to 'bootstrapping'.\n        n_boot (int, optional): Number of bootstrapping iterations to perform when\n            estimating prediction intervals. Defaults to 250.\n        use_in_sample_residuals (bool, optional): If `True`, residuals from the\n            training data are used as proxy of prediction error to create predictions.\n            If `False`, out of sample residuals (calibration) are used.\n            Out-of-sample residuals must be precomputed using Forecaster's\n            `set_out_sample_residuals()` method. Defaults to True.\n        use_binned_residuals (bool, optional): If `True`, residuals are selected\n            based on the predicted values (binned selection).\n            If `False`, residuals are selected randomly. Defaults to True.\n        random_state (int, optional): Seed for the random number generator to\n            ensure reproducibility. Defaults to 123.\n        return_predictors (bool, optional): If `True`, the predictors used to make\n            the predictions are also returned. Defaults to False.\n        n_jobs (int | str, optional): The number of jobs to run in parallel. If `-1`,\n            then the number of jobs is set to the number of cores. If 'auto', `n_jobs`\n            is set using the function `skforecast.utils.select_n_jobs_backtesting`.\n            Defaults to 'auto'.\n        verbose (bool, optional): Print number of folds and index of training and\n            validation sets used for backtesting. Defaults to False.\n        show_progress (bool, optional): Whether to show a progress bar.\n            Defaults to True.\n        suppress_warnings (bool, optional): If `True`, spotforecast warnings will be\n            suppressed during the backtesting process. See\n            `spotforecast.exceptions.warn_skforecast_categories` for more information.\n            Defaults to False.\n\n    Returns:\n        tuple (pd.DataFrame, pd.DataFrame):\n            - metric_values: Value(s) of the metric(s).\n            - backtest_predictions: Value of predictions. The DataFrame includes\n              the following columns:\n\n              - fold: Indicates the fold number where the prediction was made.\n              - pred: Predicted values for the corresponding series and time steps.\n\n              If `interval` is not `None`, additional columns are included depending\n              on the method:\n\n              - For `float`: Columns `lower_bound` and `upper_bound`.\n              - For `list` or `tuple` of 2 elements: Columns `lower_bound` and\n                `upper_bound`.\n              - For `list` or `tuple` with multiple percentiles: One column per\n                percentile (e.g., `p_10`, `p_50`, `p_90`).\n              - For `'bootstrapping'`: One column per bootstrapping iteration\n                (e.g., `pred_boot_0`, `pred_boot_1`, ..., `pred_boot_n`).\n              - For `scipy.stats` distribution objects: One column for each\n                estimated parameter of the distribution (e.g., `loc`, `scale`).\n\n              If `return_predictors` is `True`, one column per predictor is created.\n\n              Depending on the relation between `steps` and `fold_stride`, the output\n              may include repeated indexes (if `fold_stride &lt; steps`) or gaps\n              (if `fold_stride &gt; steps`). See Notes below for more details.\n\n    Notes:\n        Note on `fold_stride` vs. `steps`:\n\n        - If `fold_stride == steps`, test sets are placed back-to-back without overlap.\n          Each observation appears only once in the output DataFrame, so the\n          index is unique.\n        - If `fold_stride &lt; steps`, test sets overlap. Multiple forecasts are\n          generated for the same observations and, therefore, the output\n          DataFrame contains repeated indexes.\n        - If `fold_stride &gt; steps`, there are gaps between consecutive test sets.\n          Some observations in the series will not have associated predictions,\n          so the output DataFrame has non-contiguous indexes.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from spotforecast2_safe.model_selection import backtesting_forecaster, TimeSeriesFold\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(\n        ...     estimator=RandomForestRegressor(random_state=123),\n        ...     lags=2\n        ... )\n        &gt;&gt;&gt; cv = TimeSeriesFold(\n        ...     steps=2,\n        ...     initial_train_size=5,\n        ...     refit=False\n        ... )\n        &gt;&gt;&gt; metric_values, backtest_predictions = backtesting_forecaster(\n        ...     forecaster=forecaster,\n        ...     y=y,\n        ...     cv=cv,\n        ...     metric='mean_squared_error'\n        ... )\n        &gt;&gt;&gt; metric_values\n           mean_squared_error\n        0            0.201334\n        &gt;&gt;&gt; backtest_predictions\n           fold  pred\n        5     0  5.18\n        6     0  6.10\n        7     1  7.36\n        8     1  8.40\n        9     2  9.31\n    \"\"\"\n\n    if type(cv).__name__ == \"OneStepAheadFold\":\n        return backtesting_forecaster_one_step(\n            forecaster=forecaster,\n            y=y,\n            cv=cv,\n            metric=metric,\n            exog=exog,\n            interval=interval,\n            interval_method=interval_method,\n            n_boot=n_boot,\n            use_in_sample_residuals=use_in_sample_residuals,\n            use_binned_residuals=use_binned_residuals,\n            random_state=random_state,\n            return_predictors=return_predictors,\n            n_jobs=n_jobs,\n            verbose=verbose,\n            show_progress=show_progress,\n            suppress_warnings=suppress_warnings,\n        )\n\n    check_backtesting_input(\n        forecaster=forecaster,\n        cv=cv,\n        y=y,\n        metric=metric,\n        interval=interval,\n        interval_method=interval_method,\n        n_boot=n_boot,\n        use_in_sample_residuals=use_in_sample_residuals,\n        use_binned_residuals=use_binned_residuals,\n        random_state=random_state,\n        return_predictors=return_predictors,\n        n_jobs=n_jobs,\n        show_progress=show_progress,\n        suppress_warnings=suppress_warnings,\n    )\n\n    metric_values, backtest_predictions = _backtesting_forecaster(\n        forecaster=forecaster,\n        y=y,\n        cv=cv,\n        metric=metric,\n        exog=exog,\n        interval=interval,\n        interval_method=interval_method,\n        n_boot=n_boot,\n        use_in_sample_residuals=use_in_sample_residuals,\n        use_binned_residuals=use_binned_residuals,\n        random_state=random_state,\n        return_predictors=return_predictors,\n        n_jobs=n_jobs,\n        verbose=verbose,\n        show_progress=show_progress,\n        suppress_warnings=suppress_warnings,\n    )\n\n    return metric_values, backtest_predictions\n</code></pre>"},{"location":"api/model_selection/#design-philosophy-for-safety-critical-systems","title":"Design Philosophy for Safety-Critical Systems","text":"<p>In safety-critical environments (energy grid management, medical monitoring, industrial control systems), forecast failures can have severe consequences. The backtesting_forecaster function implements several defensive design patterns:</p> <ol> <li>Temporal Integrity: Strict enforcement of temporal ordering prevents data leakage that could mask model weaknesses</li> <li>Refit Strategy Control: Configurable refit intervals allow balancing between model freshness and computational cost</li> <li>Probabilistic Quantification: Prediction intervals provide uncertainty estimates essential for risk management</li> <li>Parallel Execution Safety: Careful handling of stateful operations during parallelization prevents race conditions</li> </ol>"},{"location":"api/model_selection/#fallback-mechanisms-in-production","title":"Fallback Mechanisms in Production","text":"<p>The backtesting framework serves as a critical fallback validation layer:</p> <ul> <li>Pre-deployment Validation: Comprehensive backtesting before model deployment catches issues that unit tests miss</li> <li>Continuous Monitoring: Regular backtesting on recent data detects model degradation</li> <li>A/B Testing Foundation: Provides fair comparison framework for evaluating model updates</li> <li>Rollback Decision Support: Quantitative metrics guide decisions to revert problematic model changes</li> </ul>"},{"location":"api/model_selection/#understanding-probabilistic-forecasting","title":"Understanding Probabilistic Forecasting","text":"<p>When forecasting in safety-critical contexts, point predictions alone are insufficient. Prediction intervals quantify uncertainty, enabling downstream systems to make risk-aware decisions. The backtesting_forecaster supports two interval estimation methods:</p> <ol> <li>Bootstrapping: Resamples residuals to generate empirical prediction distributions</li> <li>Conformal Prediction: Provides distribution-free coverage guarantees under mild assumptions</li> </ol> <p>Both methods require residuals (forecast errors) for interval construction:</p> <ul> <li>In-sample residuals: Computed from training data (default, always available)</li> <li>Out-of-sample residuals: Computed from held-out calibration data (more reliable, requires setup)</li> <li>Binned residuals: Stratified by prediction magnitude for heteroscedastic errors (recommended)</li> </ul>"},{"location":"api/model_selection/#probabilistic-forecasting-with-residuals","title":"Probabilistic Forecasting with Residuals","text":"<p>When using probabilistic forecasting methods (<code>interval</code> parameter), the forecaster requires residuals for generating prediction intervals:</p> <ul> <li> <p>In-sample residuals: Automatically stored when <code>use_in_sample_residuals=True</code> (default). The <code>backtesting_forecaster</code> function handles this by setting <code>store_in_sample_residuals=True</code> during training.</p> </li> <li> <p>Out-of-sample residuals: Used when <code>use_in_sample_residuals=False</code>. These must be precomputed using the forecaster's <code>set_out_sample_residuals(y_true, y_pred)</code> method before calling <code>backtesting_forecaster</code>.</p> </li> <li> <p>Binned residuals: When <code>use_binned_residuals=True</code> (default), residuals are selected based on predicted values for improved interval accuracy. This requires the forecaster to have a binner configured during initialization.</p> </li> </ul> <p>For conformal prediction (<code>interval_method='conformal'</code>), the method automatically uses the appropriate residuals based on the <code>use_in_sample_residuals</code> setting.</p>"},{"location":"api/model_selection/#complete-examples","title":"Complete Examples","text":""},{"location":"api/model_selection/#example-0-safety-critical-energy-grid-forecasting","title":"Example 0: Safety-Critical Energy Grid Forecasting","text":"<p>This comprehensive example demonstrates model validation for a production energy grid management system where forecast reliability is mission-critical. The example shows defensive programming practices, uncertainty quantification, and deployment decision support.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom spotforecast2_safe.forecaster.recursive import ForecasterRecursive\nfrom spotforecast2.model_selection import backtesting_forecaster, TimeSeriesFold\n\n# Simulate realistic energy load data with daily and weekly patterns\nrng = np.random.default_rng(42)\ndates = pd.date_range(\"2023-01-01\", periods=365 * 24, freq=\"h\")\n\n# Base load + daily pattern + weekly pattern + noise\nhour_of_day = dates.hour\nday_of_week = dates.dayofweek\nbase_load = 5000\ndaily_pattern = 2000 * np.sin(2 * np.pi * hour_of_day / 24)\nweekly_pattern = 500 * (day_of_week &lt; 5).astype(float)  # Weekday boost\nnoise = rng.normal(0, 200, len(dates))\ntrend = np.linspace(0, 500, len(dates))  # Gradual load increase\n\ny = pd.Series(\n    base_load + daily_pattern + weekly_pattern + trend + noise,\n    index=dates,\n    name=\"grid_load_mw\",\n)\n\n# Safety-critical configuration: Conservative forecaster with uncertainty quantification\nforecaster = ForecasterRecursive(\n    estimator=GradientBoostingRegressor(\n        n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42\n    ),\n    lags=24 * 7,  # One week of hourly data\n    binner_kwargs={\"n_bins\": 10},  # Enable binned residuals for better intervals\n)\n\n# Cross-validation strategy: Realistic evaluation with weekly retraining\ncv = TimeSeriesFold(\n    steps=24,  # Forecast 24 hours ahead\n    initial_train_size=24 * 30 * 6,  # 6 months initial training\n    refit=24 * 7,  # Retrain weekly (safety-critical: fresh models)\n    fixed_train_size=False,  # Expanding window (use all available data)\n    fold_stride=24 * 7,  # Evaluate weekly\n    gap=0,  # No gap (immediate forecasting)\n)\n\n# Perform backtesting with probabilistic forecasts\nmetric_values, predictions = backtesting_forecaster(\n    forecaster=forecaster,\n    y=y,\n    cv=cv,\n    metric=[\"mean_absolute_error\", \"mean_squared_error\"],\n    interval=0.95,  # 95% prediction interval for risk management\n    interval_method=\"conformal\",  # Distribution-free guarantees\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,  # Account for heteroscedasticity\n    n_jobs=1,  # Sequential for safety (avoid race conditions)\n    verbose=True,\n    show_progress=True,\n)\n\n# Safety metrics: Evaluate forecast reliability\nprint(\"Safety-Critical Performance Metrics:\")\nprint(f\"Mean Absolute Error: {metric_values['mean_absolute_error'].mean():.2f} MW\")\nprint(f\"RMSE: {np.sqrt(metric_values['mean_squared_error'].mean()):.2f} MW\")\n\n# Coverage analysis: Critical for safety applications\nactual_coverage = (\n    (y.loc[predictions.index] &gt;= predictions[\"lower_bound\"])\n    &amp; (y.loc[predictions.index] &lt;= predictions[\"upper_bound\"])\n).mean()\nprint(f\"Prediction Interval Coverage: {actual_coverage:.1%} (target: 95.0%)\")\n\n# Decision support: Model deployment recommendation\nif actual_coverage &gt;= 0.93:\n    print(\"\u2713 RECOMMENDATION: APPROVE for production deployment\")\nelse:\n    print(\"\u2717 RECOMMENDATION: REJECT - Coverage below safety threshold\")\n</code></pre> <p>Key safety-critical design elements:</p> <ol> <li>Expanding training window: Uses all historical data for maximum information</li> <li>Regular retraining: Weekly updates prevent model staleness</li> <li>Conformal intervals: Provides distribution-free coverage guarantees</li> <li>Binned residuals: Accounts for heteroscedastic errors (variance changes with load level)</li> <li>Sequential execution: Avoids parallelization race conditions in critical systems</li> <li>Coverage monitoring: Validates that uncertainty estimates are calibrated</li> <li>Deployment gates: Quantitative criteria for production approval</li> </ol>"},{"location":"api/model_selection/#example-1-bootstrapping-method","title":"Example 1: Bootstrapping Method","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Ridge\nfrom spotforecast2_safe.forecaster.recursive import ForecasterRecursive\nfrom spotforecast2.model_selection import backtesting_forecaster, TimeSeriesFold\n\n# Create sample time series data\nrng = np.random.default_rng(123)\ndates = pd.date_range(\"2020-01-01\", periods=200, freq=\"D\")\ny = pd.Series(\n    np.cumsum(rng.normal(loc=0.1, scale=1, size=200)) + 50,\n    index=dates,\n    name=\"value\"\n)\n\n# Initialize forecaster\nforecaster = ForecasterRecursive(\n    estimator=Ridge(random_state=123),\n    lags=14\n)\n\n# Configure cross-validation\ncv = TimeSeriesFold(\n    steps=10,\n    initial_train_size=150,\n    refit=True,\n    fold_stride=10\n)\n\n# Perform backtesting with bootstrapping method\nmetric_values, predictions = backtesting_forecaster(\n    forecaster=forecaster,\n    y=y,\n    cv=cv,\n    metric=\"mean_absolute_error\",\n    interval=\"bootstrapping\",  # Use bootstrapping for uncertainty quantification\n    interval_method=\"bootstrapping\",\n    n_boot=50,  # Generate 50 bootstrap samples\n    use_in_sample_residuals=True,\n    random_state=123\n)\n\n# Visualize results with bootstrap samples\nfig, ax = plt.subplots(figsize=(12, 6))\nax.plot(y.index, y.values, label=\"Actual\", color=\"black\", linewidth=2)\nax.plot(predictions.index, predictions[\"pred\"], label=\"Prediction\", color=\"blue\", linewidth=2)\n\n# Bootstrap samples are available when using interval=\"bootstrapping\"\n# These provide empirical prediction distributions\n\nax.set_xlabel(\"Date\")\nax.set_ylabel(\"Value\")\nax.set_title(\"Bootstrapping Method: Predictions with Uncertainty\")\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"MAE: {metric_values['mean_absolute_error'].values[0]:.3f}\")\n</code></pre>"},{"location":"api/model_selection/#example-2-conformal-prediction-with-binned-residuals","title":"Example 2: Conformal Prediction with Binned Residuals","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom spotforecast2_safe.forecaster.recursive import ForecasterRecursive\nfrom spotforecast2.model_selection import backtesting_forecaster, TimeSeriesFold\n\n# Create sample time series with trend\nrng = np.random.default_rng(456)\ndates = pd.date_range(\"2020-01-01\", periods=300, freq=\"h\")\ntrend = np.linspace(0, 50, 300)\nseasonal = 10 * np.sin(2 * np.pi * np.arange(300) / 24)\nnoise = rng.normal(0, 2, 300)\ny = pd.Series(trend + seasonal + noise, index=dates, name=\"power\")\n\n# Initialize forecaster with binner for improved interval accuracy\nforecaster = ForecasterRecursive(\n    estimator=GradientBoostingRegressor(random_state=456, n_estimators=50),\n    lags=24,\n    binner_kwargs={\"n_bins\": 10}\n)\n\n# Configure cross-validation\ncv = TimeSeriesFold(\n    steps=24,\n    initial_train_size=200,\n    refit=False,\n    fold_stride=24\n)\n\n# Perform backtesting with conformal prediction\nmetric_values, predictions = backtesting_forecaster(\n    forecaster=forecaster,\n    y=y,\n    cv=cv,\n    metric=[\"mean_absolute_error\", \"mean_squared_error\"],\n    interval=0.95,  # 95% nominal coverage\n    interval_method=\"conformal\",\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,\n    random_state=456\n)\n\n# Visualize results with prediction intervals\nfig, ax = plt.subplots(figsize=(14, 7))\nax.plot(y.index, y.values, label=\"Actual\", color=\"black\", linewidth=2, alpha=0.7)\nax.plot(predictions.index, predictions[\"pred\"], label=\"Prediction\", color=\"blue\", linewidth=2)\nax.fill_between(\n    predictions.index,\n    predictions[\"lower_bound\"],\n    predictions[\"upper_bound\"],\n    alpha=0.3,\n    color=\"blue\",\n    label=\"95% Prediction Interval\"\n)\n\nax.set_xlabel(\"Date\")\nax.set_ylabel(\"Power\")\nax.set_title(\"Conformal Prediction with Binned Residuals (95% Interval)\")\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# Verify prediction interval coverage\ncoverage = (\n    (predictions[\"pred\"] &gt;= predictions[\"lower_bound\"]) &amp;\n    (predictions[\"pred\"] &lt;= predictions[\"upper_bound\"])\n).mean()\n\nprint(f\"Interval coverage: {coverage:.2%}\")\nprint(metric_values)\n</code></pre>"},{"location":"api/model_selection/#example-3-forecasting-with-exogenous-variables","title":"Example 3: Forecasting with Exogenous Variables","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom spotforecast2_safe.forecaster.recursive import ForecasterRecursive\nfrom spotforecast2.model_selection import backtesting_forecaster, TimeSeriesFold\n\n# Create time series with exogenous variable\nrng = np.random.default_rng(789)\ndates = pd.date_range(\"2020-01-01\", periods=250, freq=\"D\")\nexog = pd.DataFrame({\n    \"temperature\": rng.normal(20, 5, 250),\n    \"day_of_week\": dates.dayofweek\n}, index=dates)\n\ny = pd.Series(\n    10 + 0.5 * exog[\"temperature\"] + 2 * (exog[\"day_of_week\"] &lt; 5) + rng.normal(0, 1, 250),\n    index=dates,\n    name=\"demand\"\n)\n\n# Initialize forecaster\nforecaster = ForecasterRecursive(\n    estimator=LinearRegression(),\n    lags=7\n)\n\n# Configure cross-validation\ncv = TimeSeriesFold(\n    steps=7,\n    initial_train_size=180,\n    refit=True,\n    fold_stride=7\n)\n\n# Perform backtesting with conformal prediction\nmetric_values, predictions = backtesting_forecaster(\n    forecaster=forecaster,\n    y=y,\n    exog=exog,\n    cv=cv,\n    metric=\"mean_absolute_percentage_error\",\n    interval=0.90,  # 90% prediction interval\n    interval_method=\"conformal\",\n    use_in_sample_residuals=True,\n    random_state=789\n)\n\n# Visualize results with prediction intervals and exogenous variable\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n\n# Top plot: Predictions with intervals\nax1.plot(y.index, y.values, label=\"Actual Demand\", color=\"black\", linewidth=2, alpha=0.7)\nax1.plot(predictions.index, predictions[\"pred\"], label=\"Prediction\", color=\"blue\", linewidth=2)\nax1.fill_between(\n    predictions.index,\n    predictions[\"lower_bound\"],\n    predictions[\"upper_bound\"],\n    alpha=0.3,\n    color=\"blue\",\n    label=\"90% Prediction Interval\"\n)\nax1.set_ylabel(\"Demand\")\nax1.set_title(\"Demand Forecasting with Exogenous Variables (90% Interval)\")\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Bottom plot: Exogenous variable (temperature)\nax2.plot(exog.index, exog[\"temperature\"], label=\"Temperature\", color=\"red\", linewidth=1.5)\nax2.set_xlabel(\"Date\")\nax2.set_ylabel(\"Temperature (\u00b0C)\")\nax2.set_title(\"Exogenous Variable: Temperature\")\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"MAPE: {metric_values['mean_absolute_percentage_error'].values[0]:.3f}\")\n</code></pre>"},{"location":"api/model_selection/#time-series-cross-validation-timeseriesfold","title":"Time Series Cross-Validation: TimeSeriesFold","text":"<p>The TimeSeriesFold class provides a robust framework for splitting time series data into training and validation folds while respecting temporal ordering. This is critical for safety-critical systems where data leakage from future observations could create dangerously optimistic performance estimates.</p>"},{"location":"api/model_selection/#spotforecast2.model_selection.split_ts_cv.TimeSeriesFold","title":"<code>spotforecast2.model_selection.split_ts_cv.TimeSeriesFold</code>","text":"<p>               Bases: <code>BaseFold</code></p> <p>Class to split time series data into train and test folds.</p> <p>When used within a backtesting or hyperparameter search, the arguments 'initial_train_size', 'window_size' and 'differentiation' are not required as they are automatically set by the backtesting or hyperparameter search functions.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of observations used to be predicted in each fold. This is also commonly referred to as the forecast horizon or test size.</p> required <code>initial_train_size</code> <code>int | str | Timestamp | None</code> <p>Number of observations used for initial training.</p> <ul> <li>If <code>None</code> or 0, the initial forecaster is not trained in the first fold.</li> <li>If an integer, the number of observations used for initial training.</li> <li>If a date string or pandas Timestamp, it is the last date included in   the initial training set.</li> </ul> <p>Defaults to None.</p> <code>None</code> <code>fold_stride</code> <code>int | None</code> <p>Number of observations that the start of the test set advances between consecutive folds.</p> <ul> <li>If <code>None</code>, it defaults to the same value as <code>steps</code>, meaning that folds   are placed back-to-back without overlap.</li> <li>If <code>fold_stride &lt; steps</code>, test sets overlap and multiple forecasts will   be generated for the same observations.</li> <li>If <code>fold_stride &gt; steps</code>, gaps are left between consecutive test sets.</li> </ul> <p>Defaults to None.</p> <code>None</code> <code>window_size</code> <code>int | None</code> <p>Number of observations needed to generate the autoregressive predictors. Defaults to None.</p> <code>None</code> <code>differentiation</code> <code>int | None</code> <p>Number of observations to use for differentiation. This is used to extend the <code>last_window</code> as many observations as the differentiation order. Defaults to None.</p> <code>None</code> <code>refit</code> <code>bool | int</code> <p>Whether to refit the forecaster in each fold.</p> <ul> <li>If <code>True</code>, the forecaster is refitted in each fold.</li> <li>If <code>False</code>, the forecaster is trained only in the first fold.</li> <li>If an integer, the forecaster is trained in the first fold and then refitted   every <code>refit</code> folds.</li> </ul> <p>Defaults to False.</p> <code>False</code> <code>fixed_train_size</code> <code>bool</code> <p>Whether the training size is fixed or increases in each fold. Defaults to True.</p> <code>True</code> <code>gap</code> <code>int</code> <p>Number of observations between the end of the training set and the start of the test set. Defaults to 0.</p> <code>0</code> <code>skip_folds</code> <code>int | list[int] | None</code> <p>Number of folds to skip.</p> <ul> <li>If an integer, every 'skip_folds'-th is returned.</li> <li>If a list, the indexes of the folds to skip.</li> </ul> <p>For example, if <code>skip_folds=3</code> and there are 10 folds, the returned folds are 0, 3, 6, and 9. If <code>skip_folds=[1, 2, 3]</code>, the returned folds are 0, 4, 5, 6, 7, 8, and 9. Defaults to None.</p> <code>None</code> <code>allow_incomplete_fold</code> <code>bool</code> <p>Whether to allow the last fold to include fewer observations than <code>steps</code>. If <code>False</code>, the last fold is excluded if it is incomplete. Defaults to True.</p> <code>True</code> <code>return_all_indexes</code> <code>bool</code> <p>Whether to return all indexes or only the start and end indexes of each fold. Defaults to False.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Whether to print information about generated folds. Defaults to True.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>steps</code> <p>Number of observations used to be predicted in each fold.</p> <code>initial_train_size</code> <p>Number of observations used for initial training. If <code>None</code> or 0, the initial forecaster is not trained in the first fold.</p> <code>fold_stride</code> <p>Number of observations that the start of the test set advances between consecutive folds.</p> <code>overlapping_folds</code> <p>Whether the folds overlap.</p> <code>window_size</code> <p>Number of observations needed to generate the autoregressive predictors.</p> <code>differentiation</code> <p>Number of observations to use for differentiation. This is used to extend the <code>last_window</code> as many observations as the differentiation order.</p> <code>refit</code> <p>Whether to refit the forecaster in each fold.</p> <code>fixed_train_size</code> <p>Whether the training size is fixed or increases in each fold.</p> <code>gap</code> <p>Number of observations between the end of the training set and the start of the test set.</p> <code>skip_folds</code> <p>Number of folds to skip.</p> <code>allow_incomplete_fold</code> <p>Whether to allow the last fold to include fewer observations than <code>steps</code>.</p> <code>return_all_indexes</code> <p>Whether to return all indexes or only the start and end indexes of each fold.</p> <code>verbose</code> <p>Whether to print information about generated folds.</p> <p>Examples:</p> <p>Basic usage with fixed train size:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.model_selection import TimeSeriesFold\n&gt;&gt;&gt; # Create sample time series data\n&gt;&gt;&gt; dates = pd.date_range('2020-01-01', periods=100, freq='D')\n&gt;&gt;&gt; y = pd.Series(np.arange(100), index=dates)\n&gt;&gt;&gt; # Create fold splitter\n&gt;&gt;&gt; cv = TimeSeriesFold(\n...     steps=10,\n...     initial_train_size=50,\n...     refit=True,\n...     fixed_train_size=True\n... )\n&gt;&gt;&gt; # Get folds\n&gt;&gt;&gt; folds = cv.split(y)\n&gt;&gt;&gt; print(f\"Number of folds: {len(folds)}\")\nNumber of folds: 4\n</code></pre> <p>Overlapping folds with custom stride:</p> <pre><code>&gt;&gt;&gt; cv = TimeSeriesFold(\n...     steps=30,\n...     initial_train_size=50,\n...     fold_stride=7,\n...     fixed_train_size=False\n... )\n&gt;&gt;&gt; folds = cv.split(y)\n&gt;&gt;&gt; # First test fold covers [50, 80), second [57, 87), etc.\n</code></pre> <p>Return as pandas DataFrame:</p> <pre><code>&gt;&gt;&gt; cv = TimeSeriesFold(steps=10, initial_train_size=50)\n&gt;&gt;&gt; folds_df = cv.split(y, as_pandas=True)\n&gt;&gt;&gt; print(folds_df.columns.tolist())\n['fold', 'train_start', 'train_end', 'last_window_start', 'last_window_end', 'test_start', 'test_end', 'test_start_with_gap', 'test_end_with_gap', 'fit_forecaster']\n</code></pre> <p>Skip folds for faster evaluation:</p> <pre><code>&gt;&gt;&gt; cv = TimeSeriesFold(\n...     steps=5,\n...     initial_train_size=50,\n...     skip_folds=2\n... )\n&gt;&gt;&gt; folds = cv.split(y)\n&gt;&gt;&gt; # Returns folds 0, 2, 4, 6, ...\n</code></pre> Note <p>Returned values are the positions of the observations and not the actual values of the index, so they can be used to slice the data directly using iloc. For example, if the input series is <code>X = [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]</code>, the <code>initial_train_size = 3</code>, <code>window_size = 2</code>, <code>steps = 4</code>, and <code>gap = 1</code>, the output of the first fold will: [0, [0, 3], [1, 3], [3, 8], [4, 8], True].</p> <p>The first element is the fold number, the first list <code>[0, 3]</code> indicates that the training set goes from the first to the third observation. The second list <code>[1, 3]</code> indicates that the last window seen by the forecaster during training goes from the second to the third observation. The third list <code>[3, 8]</code> indicates that the test set goes from the fourth to the eighth observation. The fourth list <code>[4, 8]</code> indicates that the test set including the gap goes from the fifth to the eighth observation. The boolean <code>False</code> indicates that the forecaster should not be trained in this fold.</p> <p>Following the python convention, the start index is inclusive and the end index is exclusive. This means that the last index is not included in the slice.</p> <p>As an example, with <code>initial_train_size=50</code>, <code>steps=30</code>, and <code>fold_stride=7</code>, the first test fold will cover observations [50, 80), the second fold [57, 87), and the third fold [64, 94). This configuration produces multiple forecasts for the same observations, which is often desirable in rolling-origin evaluation.</p> Source code in <code>src/spotforecast2/model_selection/split_ts_cv.py</code> <pre><code>class TimeSeriesFold(BaseFold):\n    \"\"\"Class to split time series data into train and test folds.\n\n    When used within a backtesting or hyperparameter search, the arguments\n    'initial_train_size', 'window_size' and 'differentiation' are not required\n    as they are automatically set by the backtesting or hyperparameter search\n    functions.\n\n    Args:\n        steps: Number of observations used to be predicted in each fold.\n            This is also commonly referred to as the forecast horizon or test size.\n        initial_train_size: Number of observations used for initial training.\n\n            - If `None` or 0, the initial forecaster is not trained in the first fold.\n            - If an integer, the number of observations used for initial training.\n            - If a date string or pandas Timestamp, it is the last date included in\n              the initial training set.\n\n            Defaults to None.\n        fold_stride: Number of observations that the start of the test set\n            advances between consecutive folds.\n\n            - If `None`, it defaults to the same value as `steps`, meaning that folds\n              are placed back-to-back without overlap.\n            - If `fold_stride &lt; steps`, test sets overlap and multiple forecasts will\n              be generated for the same observations.\n            - If `fold_stride &gt; steps`, gaps are left between consecutive test sets.\n\n            Defaults to None.\n        window_size: Number of observations needed to generate the\n            autoregressive predictors. Defaults to None.\n        differentiation: Number of observations to use for differentiation.\n            This is used to extend the `last_window` as many observations as the\n            differentiation order. Defaults to None.\n        refit: Whether to refit the forecaster in each fold.\n\n            - If `True`, the forecaster is refitted in each fold.\n            - If `False`, the forecaster is trained only in the first fold.\n            - If an integer, the forecaster is trained in the first fold and then refitted\n              every `refit` folds.\n\n            Defaults to False.\n        fixed_train_size: Whether the training size is fixed or increases\n            in each fold. Defaults to True.\n        gap: Number of observations between the end of the training set\n            and the start of the test set. Defaults to 0.\n        skip_folds: Number of folds to skip.\n\n            - If an integer, every 'skip_folds'-th is returned.\n            - If a list, the indexes of the folds to skip.\n\n            For example, if `skip_folds=3` and there are 10 folds, the returned folds are\n            0, 3, 6, and 9. If `skip_folds=[1, 2, 3]`, the returned folds are 0, 4, 5, 6, 7,\n            8, and 9. Defaults to None.\n        allow_incomplete_fold: Whether to allow the last fold to include\n            fewer observations than `steps`. If `False`, the last fold is excluded if it\n            is incomplete. Defaults to True.\n        return_all_indexes: Whether to return all indexes or only the\n            start and end indexes of each fold. Defaults to False.\n        verbose: Whether to print information about generated folds.\n            Defaults to True.\n\n    Attributes:\n        steps: Number of observations used to be predicted in each fold.\n        initial_train_size: Number of observations used for initial training.\n            If `None` or 0, the initial forecaster is not trained in the first fold.\n        fold_stride: Number of observations that the start of the test set\n            advances between consecutive folds.\n        overlapping_folds: Whether the folds overlap.\n        window_size: Number of observations needed to generate the\n            autoregressive predictors.\n        differentiation: Number of observations to use for differentiation.\n            This is used to extend the `last_window` as many observations as the\n            differentiation order.\n        refit: Whether to refit the forecaster in each fold.\n        fixed_train_size: Whether the training size is fixed or increases in each fold.\n        gap: Number of observations between the end of the training set and the\n            start of the test set.\n        skip_folds: Number of folds to skip.\n        allow_incomplete_fold: Whether to allow the last fold to include fewer\n            observations than `steps`.\n        return_all_indexes: Whether to return all indexes or only the start\n            and end indexes of each fold.\n        verbose: Whether to print information about generated folds.\n\n    Examples:\n        Basic usage with fixed train size:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.model_selection import TimeSeriesFold\n        &gt;&gt;&gt; # Create sample time series data\n        &gt;&gt;&gt; dates = pd.date_range('2020-01-01', periods=100, freq='D')\n        &gt;&gt;&gt; y = pd.Series(np.arange(100), index=dates)\n        &gt;&gt;&gt; # Create fold splitter\n        &gt;&gt;&gt; cv = TimeSeriesFold(\n        ...     steps=10,\n        ...     initial_train_size=50,\n        ...     refit=True,\n        ...     fixed_train_size=True\n        ... )\n        &gt;&gt;&gt; # Get folds\n        &gt;&gt;&gt; folds = cv.split(y)\n        &gt;&gt;&gt; print(f\"Number of folds: {len(folds)}\")\n        Number of folds: 4\n\n        Overlapping folds with custom stride:\n        &gt;&gt;&gt; cv = TimeSeriesFold(\n        ...     steps=30,\n        ...     initial_train_size=50,\n        ...     fold_stride=7,\n        ...     fixed_train_size=False\n        ... )\n        &gt;&gt;&gt; folds = cv.split(y)\n        &gt;&gt;&gt; # First test fold covers [50, 80), second [57, 87), etc.\n\n        Return as pandas DataFrame:\n        &gt;&gt;&gt; cv = TimeSeriesFold(steps=10, initial_train_size=50)\n        &gt;&gt;&gt; folds_df = cv.split(y, as_pandas=True)\n        &gt;&gt;&gt; print(folds_df.columns.tolist())\n        ['fold', 'train_start', 'train_end', 'last_window_start', 'last_window_end', 'test_start', 'test_end', 'test_start_with_gap', 'test_end_with_gap', 'fit_forecaster']\n\n        Skip folds for faster evaluation:\n        &gt;&gt;&gt; cv = TimeSeriesFold(\n        ...     steps=5,\n        ...     initial_train_size=50,\n        ...     skip_folds=2\n        ... )\n        &gt;&gt;&gt; folds = cv.split(y)\n        &gt;&gt;&gt; # Returns folds 0, 2, 4, 6, ...\n\n    Note:\n        Returned values are the positions of the observations and not the actual values of\n        the index, so they can be used to slice the data directly using iloc. For example,\n        if the input series is `X = [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]`, the\n        `initial_train_size = 3`, `window_size = 2`, `steps = 4`, and `gap = 1`,\n        the output of the first fold will: [0, [0, 3], [1, 3], [3, 8], [4, 8], True].\n\n        The first element is the fold number, the first list `[0, 3]` indicates that\n        the training set goes from the first to the third observation. The second\n        list `[1, 3]` indicates that the last window seen by the forecaster during\n        training goes from the second to the third observation. The third list `[3, 8]`\n        indicates that the test set goes from the fourth to the eighth observation.\n        The fourth list `[4, 8]` indicates that the test set including the gap goes\n        from the fifth to the eighth observation. The boolean `False` indicates that\n        the forecaster should not be trained in this fold.\n\n        Following the python convention, the start index is inclusive and the end index is\n        exclusive. This means that the last index is not included in the slice.\n\n        As an example, with `initial_train_size=50`, `steps=30`, and `fold_stride=7`,\n        the first test fold will cover observations [50, 80), the second fold [57, 87),\n        and the third fold [64, 94). This configuration produces multiple forecasts\n        for the same observations, which is often desirable in rolling-origin\n        evaluation.\n    \"\"\"\n\n    def __init__(\n        self,\n        steps: int,\n        initial_train_size: int | str | pd.Timestamp | None = None,\n        fold_stride: int | None = None,\n        window_size: int | None = None,\n        differentiation: int | None = None,\n        refit: bool | int = False,\n        fixed_train_size: bool = True,\n        gap: int = 0,\n        skip_folds: int | list[int] | None = None,\n        allow_incomplete_fold: bool = True,\n        return_all_indexes: bool = False,\n        verbose: bool = True,\n    ) -&gt; None:\n\n        super().__init__(\n            steps=steps,\n            initial_train_size=initial_train_size,\n            fold_stride=fold_stride,\n            window_size=window_size,\n            differentiation=differentiation,\n            refit=refit,\n            fixed_train_size=fixed_train_size,\n            gap=gap,\n            skip_folds=skip_folds,\n            allow_incomplete_fold=allow_incomplete_fold,\n            return_all_indexes=return_all_indexes,\n            verbose=verbose,\n        )\n\n        self.steps = steps\n        self.fold_stride = fold_stride if fold_stride is not None else steps\n        self.overlapping_folds = self.fold_stride &lt; self.steps\n        self.refit = refit\n        self.fixed_train_size = fixed_train_size\n        self.gap = gap\n        self.skip_folds = skip_folds\n        self.allow_incomplete_fold = allow_incomplete_fold\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Information displayed when printed.\n\n        Returns:\n            String representation of the TimeSeriesFold object.\n        \"\"\"\n\n        info = (\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"{type(self).__name__} \\n\"\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"Initial train size    = {self.initial_train_size},\\n\"\n            f\"Steps                 = {self.steps},\\n\"\n            f\"Fold stride           = {self.fold_stride},\\n\"\n            f\"Overlapping folds     = {self.overlapping_folds},\\n\"\n            f\"Window size           = {self.window_size},\\n\"\n            f\"Differentiation       = {self.differentiation},\\n\"\n            f\"Refit                 = {self.refit},\\n\"\n            f\"Fixed train size      = {self.fixed_train_size},\\n\"\n            f\"Gap                   = {self.gap},\\n\"\n            f\"Skip folds            = {self.skip_folds},\\n\"\n            f\"Allow incomplete fold = {self.allow_incomplete_fold},\\n\"\n            f\"Return all indexes    = {self.return_all_indexes},\\n\"\n            f\"Verbose               = {self.verbose}\\n\"\n        )\n\n        return info\n\n    def _repr_html_(self) -&gt; str:\n        \"\"\"HTML representation of the object.\n\n        The \"General Information\" section is expanded by default.\n\n        Returns:\n            HTML string representation for Jupyter notebooks.\n        \"\"\"\n\n        style, unique_id = get_style_repr_html()\n        content = f\"\"\"\n        &lt;div class=\"container-{unique_id}\"&gt;\n            &lt;p style=\"font-size: 1.5em; font-weight: bold; margin-block-start: 0.83em; margin-block-end: 0.83em;\"&gt;{type(self).__name__}&lt;/p&gt;\n            &lt;details open&gt;\n                &lt;summary&gt;General Information&lt;/summary&gt;\n                &lt;ul&gt;\n                    &lt;li&gt;&lt;strong&gt;Initial train size:&lt;/strong&gt; {self.initial_train_size}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Steps:&lt;/strong&gt; {self.steps}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Fold stride:&lt;/strong&gt; {self.fold_stride}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Overlapping folds:&lt;/strong&gt; {self.overlapping_folds}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Window size:&lt;/strong&gt; {self.window_size}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Differentiation:&lt;/strong&gt; {self.differentiation}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Refit:&lt;/strong&gt; {self.refit}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Fixed train size:&lt;/strong&gt; {self.fixed_train_size}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Gap:&lt;/strong&gt; {self.gap}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Skip folds:&lt;/strong&gt; {self.skip_folds}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Allow incomplete fold:&lt;/strong&gt; {self.allow_incomplete_fold}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Return all indexes:&lt;/strong&gt; {self.return_all_indexes}&lt;/li&gt;\n                &lt;/ul&gt;\n            &lt;/details&gt;\n        &lt;/div&gt;\n        \"\"\"\n\n        return style + content\n\n    def split(\n        self,\n        X: pd.Series | pd.DataFrame | pd.Index | dict[str, pd.Series | pd.DataFrame],\n        as_pandas: bool = False,\n    ) -&gt; list | pd.DataFrame:\n        \"\"\"Split the time series data into train and test folds.\n\n        Args:\n            X: Time series data or index to split. Can be a pandas Series, DataFrame,\n                Index, or a dictionary of Series/DataFrames.\n            as_pandas: If True, the folds are returned as a DataFrame. This is useful\n                to visualize the folds in a more interpretable way. Defaults to False.\n\n        Returns:\n            A list of lists containing the indices (position) for each fold, or a\n            DataFrame if `as_pandas=True`. Each list contains 4 lists and a boolean\n            with the following information:\n\n            - **fold**: fold number.\n            - **[train_start, train_end]**: list with the start and end positions of\n                    the training set.\n            - **[last_window_start, last_window_end]**: list with the start and end\n                    positions of the last window seen by the forecaster during training.\n                    The last window is used to generate the lags use as predictors. If\n                    `differentiation` is included, the interval is extended as many\n                    observations as the differentiation order. If the argument `window_size`\n                    is `None`, this list is empty.\n            - **[test_start, test_end]**: list with the start and end positions of\n                    the test set. These are the observations used to evaluate the forecaster.\n            - **[test_start_with_gap, test_end_with_gap]**: list with the start and\n                    end positions of the test set including the gap. The gap is the number\n                    of observations between the end of the training set and the start of\n                    the test set.\n            - **fit_forecaster**: boolean indicating whether the forecaster should be\n                    fitted in this fold.\n\n        Note:\n            The returned values are the positions of the observations and not the\n            actual values of the index, so they can be used to slice the data directly\n            using iloc.\n\n            If `as_pandas` is `True`, the folds are returned as a DataFrame with the\n            following columns: 'fold', 'train_start', 'train_end', 'last_window_start',\n            'last_window_end', 'test_start', 'test_end', 'test_start_with_gap',\n            'test_end_with_gap', 'fit_forecaster'.\n\n            Following the python convention, the start index is inclusive and the end\n            index is exclusive. This means that the last index is not included in the\n            slice.\n        \"\"\"\n\n        if not isinstance(X, (pd.Series, pd.DataFrame, pd.Index, dict)):\n            raise TypeError(\n                f\"X must be a pandas Series, DataFrame, Index or a dictionary. \"\n                f\"Got {type(X)}.\"\n            )\n\n        window_size_as_date_offset = isinstance(\n            self.window_size, pd.tseries.offsets.DateOffset\n        )\n        if window_size_as_date_offset:\n            # Calculate the window_size in steps. This is not a exact calculation\n            # because the offset follows the calendar rules and the distance between\n            # two dates may not be constant.\n            first_valid_index = X.index[-1] - self.window_size\n            try:\n                window_size_idx_start = X.index.get_loc(first_valid_index)\n                window_size_idx_end = X.index.get_loc(X.index[-1])\n                self.window_size = window_size_idx_end - window_size_idx_start\n            except KeyError:\n                raise ValueError(\n                    f\"The length of `y` ({len(X)}), must be greater than or equal \"\n                    f\"to the window size ({self.window_size}). This is because  \"\n                    f\"the offset (forecaster.offset) is larger than the available \"\n                    f\"data. Try to decrease the size of the offset (forecaster.offset), \"\n                    f\"the number of `n_offsets` (forecaster.n_offsets) or increase the \"\n                    f\"size of `y`.\"\n                )\n\n        if self.initial_train_size is None:\n            if self.window_size is None:\n                raise ValueError(\n                    \"To use split method when `initial_train_size` is None, \"\n                    \"`window_size` must be an integer greater than 0. \"\n                    \"Although no initial training is done and all data is used to \"\n                    \"evaluate the model, the first `window_size` observations are \"\n                    \"needed to create the initial predictors. Got `window_size` = None.\"\n                )\n            if self.refit:\n                raise ValueError(\n                    \"`refit` is only allowed when `initial_train_size` is not `None`. \"\n                    \"Set `refit` to `False` if you want to use `initial_train_size = None`.\"\n                )\n            externally_fitted = True\n            self.initial_train_size = self.window_size  # Reset to None later\n        else:\n            if self.window_size is None:\n                warnings.warn(\n                    \"Last window cannot be calculated because `window_size` is None.\",\n                    IgnoredArgumentWarning,\n                )\n            externally_fitted = False\n\n        index = self._extract_index(X)\n        idx = range(len(index))\n        folds = []\n        i = 0\n\n        self.initial_train_size = date_to_index_position(\n            index=index,\n            date_input=self.initial_train_size,\n            method=\"validation\",\n            date_literal=\"initial_train_size\",\n        )\n\n        if window_size_as_date_offset:\n            if self.initial_train_size is not None:\n                if self.initial_train_size &lt; self.window_size:\n                    raise ValueError(\n                        f\"If `initial_train_size` is an integer, it must be greater than \"\n                        f\"the `window_size` of the forecaster ({self.window_size}) \"\n                        f\"and smaller than the length of the series ({len(X)}). If \"\n                        f\"it is a date, it must be within this range of the index.\"\n                    )\n\n        if self.allow_incomplete_fold:\n            # At least one observation after the gap to allow incomplete fold\n            if len(index) &lt;= self.initial_train_size + self.gap:\n                raise ValueError(\n                    f\"The time series must have more than `initial_train_size + gap` \"\n                    f\"observations to create at least one fold.\\n\"\n                    f\"    Time series length: {len(index)}\\n\"\n                    f\"    Required &gt; {self.initial_train_size + self.gap}\\n\"\n                    f\"    initial_train_size: {self.initial_train_size}\\n\"\n                    f\"    gap: {self.gap}\\n\"\n                )\n        else:\n            # At least one complete fold\n            if len(index) &lt; self.initial_train_size + self.gap + self.steps:\n                raise ValueError(\n                    f\"The time series must have at least `initial_train_size + gap + steps` \"\n                    f\"observations to create a minimum of one complete fold \"\n                    f\"(allow_incomplete_fold=False).\\n\"\n                    f\"    Time series length: {len(index)}\\n\"\n                    f\"    Required &gt;= {self.initial_train_size + self.gap + self.steps}\\n\"\n                    f\"    initial_train_size: {self.initial_train_size}\\n\"\n                    f\"    gap: {self.gap}\\n\"\n                    f\"    steps: {self.steps}\\n\"\n                )\n\n        while self.initial_train_size + (i * self.fold_stride) + self.gap &lt; len(index):\n\n            if self.refit:\n                # NOTE: If `fixed_train_size` the train size doesn't increase but\n                # moves by `fold_stride` positions in each iteration. If `False`,\n                # the train size increases by `fold_stride` in each iteration.\n                train_iloc_start = (\n                    i * (self.fold_stride) if self.fixed_train_size else 0\n                )\n                train_iloc_end = self.initial_train_size + i * (self.fold_stride)\n                test_iloc_start = train_iloc_end\n            else:\n                # NOTE: The train size doesn't increase and doesn't move.\n                train_iloc_start = 0\n                train_iloc_end = self.initial_train_size\n                test_iloc_start = self.initial_train_size + i * (self.fold_stride)\n\n            if self.window_size is not None:\n                last_window_iloc_start = test_iloc_start - self.window_size\n\n            test_iloc_end = test_iloc_start + self.gap + self.steps\n\n            partitions = [\n                idx[train_iloc_start:train_iloc_end],\n                (\n                    idx[last_window_iloc_start:test_iloc_start]\n                    if self.window_size is not None\n                    else []\n                ),\n                idx[test_iloc_start:test_iloc_end],\n                idx[test_iloc_start + self.gap : test_iloc_end],\n            ]\n            folds.append(partitions)\n            i += 1\n\n        # NOTE: Delete all incomplete folds at the end if not allowed\n        n_removed_folds = 0\n        if not self.allow_incomplete_fold:\n            # NOTE: While folds and the last \"test_index_with_gap\" is incomplete,\n            # calculating len of range objects\n            while folds and len(folds[-1][3]) &lt; self.steps:\n                folds.pop()\n                n_removed_folds += 1\n\n        # Replace partitions inside folds with length 0 with `None`\n        folds = [\n            [partition if len(partition) &gt; 0 else None for partition in fold]\n            for fold in folds\n        ]\n\n        # Create a flag to know whether to train the forecaster\n        if self.refit == 0:\n            self.refit = False\n\n        if isinstance(self.refit, bool):\n            fit_forecaster = [self.refit] * len(folds)\n            fit_forecaster[0] = True\n        else:\n            fit_forecaster = [False] * len(folds)\n            for i in range(0, len(fit_forecaster), self.refit):\n                fit_forecaster[i] = True\n\n        for i in range(len(folds)):\n            folds[i].insert(0, i)\n            folds[i].append(fit_forecaster[i])\n            if fit_forecaster[i] is False:\n                folds[i][1] = folds[i - 1][1]\n\n        index_to_skip = []\n        if self.skip_folds is not None:\n            if isinstance(self.skip_folds, (int, np.integer)) and self.skip_folds &gt; 0:\n                index_to_keep = np.arange(0, len(folds), self.skip_folds)\n                index_to_skip = np.setdiff1d(\n                    np.arange(0, len(folds)), index_to_keep, assume_unique=True\n                )\n                index_to_skip = [\n                    int(x) for x in index_to_skip\n                ]  # Required since numpy 2.0\n            if isinstance(self.skip_folds, list):\n                index_to_skip = [i for i in self.skip_folds if i &lt; len(folds)]\n\n        if self.verbose:\n            self._print_info(\n                index=index,\n                folds=folds,\n                externally_fitted=externally_fitted,\n                n_removed_folds=n_removed_folds,\n                index_to_skip=index_to_skip,\n            )\n\n        folds = [fold for i, fold in enumerate(folds) if i not in index_to_skip]\n        if not self.return_all_indexes:\n            # NOTE: +1 to prevent iloc pandas from deleting the last observation\n            folds = [\n                [\n                    fold[0],\n                    [fold[1][0], fold[1][-1] + 1],\n                    (\n                        [fold[2][0], fold[2][-1] + 1]\n                        if self.window_size is not None\n                        else []\n                    ),\n                    [fold[3][0], fold[3][-1] + 1],\n                    [fold[4][0], fold[4][-1] + 1],\n                    fold[5],\n                ]\n                for fold in folds\n            ]\n\n        if externally_fitted:\n            self.initial_train_size = None\n            folds[0][5] = False\n\n        if as_pandas:\n            if self.window_size is None:\n                for fold in folds:\n                    fold[2] = [None, None]\n\n            if not self.return_all_indexes:\n                folds = pd.DataFrame(\n                    data=[\n                        [fold[0]] + list(itertools.chain(*fold[1:-1])) + [fold[-1]]\n                        for fold in folds\n                    ],\n                    columns=[\n                        \"fold\",\n                        \"train_start\",\n                        \"train_end\",\n                        \"last_window_start\",\n                        \"last_window_end\",\n                        \"test_start\",\n                        \"test_end\",\n                        \"test_start_with_gap\",\n                        \"test_end_with_gap\",\n                        \"fit_forecaster\",\n                    ],\n                )\n            else:\n                folds = pd.DataFrame(\n                    data=folds,\n                    columns=[\n                        \"fold\",\n                        \"train_index\",\n                        \"last_window_index\",\n                        \"test_index\",\n                        \"test_index_with_gap\",\n                        \"fit_forecaster\",\n                    ],\n                )\n\n        return folds\n\n    def _print_info(\n        self,\n        index: pd.Index,\n        folds: list[list[int]],\n        externally_fitted: bool,\n        n_removed_folds: int,\n        index_to_skip: list[int],\n    ) -&gt; None:\n        \"\"\"Print information about folds.\n\n        Args:\n            index: Index of the time series data.\n            folds: A list of lists containing the indices (position) for each fold.\n            externally_fitted: Whether an already trained forecaster is to be used.\n            n_removed_folds: Number of folds removed.\n            index_to_skip: Number of folds skipped.\n        \"\"\"\n\n        print(\"Information of folds\")\n        print(\"--------------------\")\n        if externally_fitted:\n            print(\n                f\"An already trained forecaster is to be used. Window size: \"\n                f\"{self.window_size}\"\n            )\n        else:\n            if self.differentiation is None:\n                print(\n                    f\"Number of observations used for initial training: \"\n                    f\"{self.initial_train_size}\"\n                )\n            else:\n                print(\n                    f\"Number of observations used for initial training: \"\n                    f\"{self.initial_train_size - self.differentiation}\"\n                )\n                print(\n                    f\"    First {self.differentiation} observation/s in training sets \"\n                    f\"are used for differentiation\"\n                )\n        print(\n            f\"Number of observations used for backtesting: \"\n            f\"{len(index) - self.initial_train_size}\"\n        )\n        print(f\"    Number of folds: {len(folds)}\")\n        print(\n            f\"    Number skipped folds: \"\n            f\"{len(index_to_skip)} {index_to_skip if index_to_skip else ''}\"\n        )\n        print(f\"    Number of steps per fold: {self.steps}\")\n        if self.steps != self.fold_stride:\n            print(\n                f\"    Number of steps to the next fold (fold stride): {self.fold_stride}\"\n            )\n        print(\n            f\"    Number of steps to exclude between last observed data \"\n            f\"(last window) and predictions (gap): {self.gap}\"\n        )\n        if n_removed_folds &gt; 0:\n            print(\n                f\"    The last {n_removed_folds} fold(s) have been excluded \"\n                f\"because they were incomplete.\"\n            )\n\n        if len(folds[-1][4]) &lt; self.steps:\n            print(f\"    Last fold only includes {len(folds[-1][4])} observations.\")\n\n        print(\"\")\n\n        if self.differentiation is None:\n            differentiation = 0\n        else:\n            differentiation = self.differentiation\n\n        for i, fold in enumerate(folds):\n            is_fold_skipped = i in index_to_skip\n            has_training = fold[-1] if i != 0 else True\n            training_start = (\n                index[fold[1][0] + differentiation] if fold[1] is not None else None\n            )\n            training_end = index[fold[1][-1]] if fold[1] is not None else None\n            training_length = (\n                len(fold[1]) - differentiation if fold[1] is not None else 0\n            )\n            validation_start = index[fold[4][0]]\n            validation_end = index[fold[4][-1]]\n            validation_length = len(fold[4])\n\n            print(f\"Fold: {i}\")\n            if is_fold_skipped:\n                print(\"    Fold skipped\")\n            elif not externally_fitted and has_training:\n                print(\n                    f\"    Training:   {training_start} -- {training_end}  \"\n                    f\"(n={training_length})\"\n                )\n                print(\n                    f\"    Validation: {validation_start} -- {validation_end}  \"\n                    f\"(n={validation_length})\"\n                )\n            else:\n                print(\"    Training:   No training in this fold\")\n                print(\n                    f\"    Validation: {validation_start} -- {validation_end}  \"\n                    f\"(n={validation_length})\"\n                )\n\n        print(\"\")\n</code></pre>"},{"location":"api/model_selection/#spotforecast2.model_selection.split_ts_cv.TimeSeriesFold.__repr__","title":"<code>__repr__()</code>","text":"<p>Information displayed when printed.</p> <p>Returns:</p> Type Description <code>str</code> <p>String representation of the TimeSeriesFold object.</p> Source code in <code>src/spotforecast2/model_selection/split_ts_cv.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Information displayed when printed.\n\n    Returns:\n        String representation of the TimeSeriesFold object.\n    \"\"\"\n\n    info = (\n        f\"{'=' * len(type(self).__name__)} \\n\"\n        f\"{type(self).__name__} \\n\"\n        f\"{'=' * len(type(self).__name__)} \\n\"\n        f\"Initial train size    = {self.initial_train_size},\\n\"\n        f\"Steps                 = {self.steps},\\n\"\n        f\"Fold stride           = {self.fold_stride},\\n\"\n        f\"Overlapping folds     = {self.overlapping_folds},\\n\"\n        f\"Window size           = {self.window_size},\\n\"\n        f\"Differentiation       = {self.differentiation},\\n\"\n        f\"Refit                 = {self.refit},\\n\"\n        f\"Fixed train size      = {self.fixed_train_size},\\n\"\n        f\"Gap                   = {self.gap},\\n\"\n        f\"Skip folds            = {self.skip_folds},\\n\"\n        f\"Allow incomplete fold = {self.allow_incomplete_fold},\\n\"\n        f\"Return all indexes    = {self.return_all_indexes},\\n\"\n        f\"Verbose               = {self.verbose}\\n\"\n    )\n\n    return info\n</code></pre>"},{"location":"api/model_selection/#spotforecast2.model_selection.split_ts_cv.TimeSeriesFold.split","title":"<code>split(X, as_pandas=False)</code>","text":"<p>Split the time series data into train and test folds.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Series | DataFrame | Index | dict[str, Series | DataFrame]</code> <p>Time series data or index to split. Can be a pandas Series, DataFrame, Index, or a dictionary of Series/DataFrames.</p> required <code>as_pandas</code> <code>bool</code> <p>If True, the folds are returned as a DataFrame. This is useful to visualize the folds in a more interpretable way. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list | DataFrame</code> <p>A list of lists containing the indices (position) for each fold, or a</p> <code>list | DataFrame</code> <p>DataFrame if <code>as_pandas=True</code>. Each list contains 4 lists and a boolean</p> <code>list | DataFrame</code> <p>with the following information:</p> <code>list | DataFrame</code> <ul> <li>fold: fold number.</li> </ul> <code>list | DataFrame</code> <ul> <li>[train_start, train_end]: list with the start and end positions of     the training set.</li> </ul> <code>list | DataFrame</code> <ul> <li>[last_window_start, last_window_end]: list with the start and end     positions of the last window seen by the forecaster during training.     The last window is used to generate the lags use as predictors. If     <code>differentiation</code> is included, the interval is extended as many     observations as the differentiation order. If the argument <code>window_size</code>     is <code>None</code>, this list is empty.</li> </ul> <code>list | DataFrame</code> <ul> <li>[test_start, test_end]: list with the start and end positions of     the test set. These are the observations used to evaluate the forecaster.</li> </ul> <code>list | DataFrame</code> <ul> <li>[test_start_with_gap, test_end_with_gap]: list with the start and     end positions of the test set including the gap. The gap is the number     of observations between the end of the training set and the start of     the test set.</li> </ul> <code>list | DataFrame</code> <ul> <li>fit_forecaster: boolean indicating whether the forecaster should be     fitted in this fold.</li> </ul> Note <p>The returned values are the positions of the observations and not the actual values of the index, so they can be used to slice the data directly using iloc.</p> <p>If <code>as_pandas</code> is <code>True</code>, the folds are returned as a DataFrame with the following columns: 'fold', 'train_start', 'train_end', 'last_window_start', 'last_window_end', 'test_start', 'test_end', 'test_start_with_gap', 'test_end_with_gap', 'fit_forecaster'.</p> <p>Following the python convention, the start index is inclusive and the end index is exclusive. This means that the last index is not included in the slice.</p> Source code in <code>src/spotforecast2/model_selection/split_ts_cv.py</code> <pre><code>def split(\n    self,\n    X: pd.Series | pd.DataFrame | pd.Index | dict[str, pd.Series | pd.DataFrame],\n    as_pandas: bool = False,\n) -&gt; list | pd.DataFrame:\n    \"\"\"Split the time series data into train and test folds.\n\n    Args:\n        X: Time series data or index to split. Can be a pandas Series, DataFrame,\n            Index, or a dictionary of Series/DataFrames.\n        as_pandas: If True, the folds are returned as a DataFrame. This is useful\n            to visualize the folds in a more interpretable way. Defaults to False.\n\n    Returns:\n        A list of lists containing the indices (position) for each fold, or a\n        DataFrame if `as_pandas=True`. Each list contains 4 lists and a boolean\n        with the following information:\n\n        - **fold**: fold number.\n        - **[train_start, train_end]**: list with the start and end positions of\n                the training set.\n        - **[last_window_start, last_window_end]**: list with the start and end\n                positions of the last window seen by the forecaster during training.\n                The last window is used to generate the lags use as predictors. If\n                `differentiation` is included, the interval is extended as many\n                observations as the differentiation order. If the argument `window_size`\n                is `None`, this list is empty.\n        - **[test_start, test_end]**: list with the start and end positions of\n                the test set. These are the observations used to evaluate the forecaster.\n        - **[test_start_with_gap, test_end_with_gap]**: list with the start and\n                end positions of the test set including the gap. The gap is the number\n                of observations between the end of the training set and the start of\n                the test set.\n        - **fit_forecaster**: boolean indicating whether the forecaster should be\n                fitted in this fold.\n\n    Note:\n        The returned values are the positions of the observations and not the\n        actual values of the index, so they can be used to slice the data directly\n        using iloc.\n\n        If `as_pandas` is `True`, the folds are returned as a DataFrame with the\n        following columns: 'fold', 'train_start', 'train_end', 'last_window_start',\n        'last_window_end', 'test_start', 'test_end', 'test_start_with_gap',\n        'test_end_with_gap', 'fit_forecaster'.\n\n        Following the python convention, the start index is inclusive and the end\n        index is exclusive. This means that the last index is not included in the\n        slice.\n    \"\"\"\n\n    if not isinstance(X, (pd.Series, pd.DataFrame, pd.Index, dict)):\n        raise TypeError(\n            f\"X must be a pandas Series, DataFrame, Index or a dictionary. \"\n            f\"Got {type(X)}.\"\n        )\n\n    window_size_as_date_offset = isinstance(\n        self.window_size, pd.tseries.offsets.DateOffset\n    )\n    if window_size_as_date_offset:\n        # Calculate the window_size in steps. This is not a exact calculation\n        # because the offset follows the calendar rules and the distance between\n        # two dates may not be constant.\n        first_valid_index = X.index[-1] - self.window_size\n        try:\n            window_size_idx_start = X.index.get_loc(first_valid_index)\n            window_size_idx_end = X.index.get_loc(X.index[-1])\n            self.window_size = window_size_idx_end - window_size_idx_start\n        except KeyError:\n            raise ValueError(\n                f\"The length of `y` ({len(X)}), must be greater than or equal \"\n                f\"to the window size ({self.window_size}). This is because  \"\n                f\"the offset (forecaster.offset) is larger than the available \"\n                f\"data. Try to decrease the size of the offset (forecaster.offset), \"\n                f\"the number of `n_offsets` (forecaster.n_offsets) or increase the \"\n                f\"size of `y`.\"\n            )\n\n    if self.initial_train_size is None:\n        if self.window_size is None:\n            raise ValueError(\n                \"To use split method when `initial_train_size` is None, \"\n                \"`window_size` must be an integer greater than 0. \"\n                \"Although no initial training is done and all data is used to \"\n                \"evaluate the model, the first `window_size` observations are \"\n                \"needed to create the initial predictors. Got `window_size` = None.\"\n            )\n        if self.refit:\n            raise ValueError(\n                \"`refit` is only allowed when `initial_train_size` is not `None`. \"\n                \"Set `refit` to `False` if you want to use `initial_train_size = None`.\"\n            )\n        externally_fitted = True\n        self.initial_train_size = self.window_size  # Reset to None later\n    else:\n        if self.window_size is None:\n            warnings.warn(\n                \"Last window cannot be calculated because `window_size` is None.\",\n                IgnoredArgumentWarning,\n            )\n        externally_fitted = False\n\n    index = self._extract_index(X)\n    idx = range(len(index))\n    folds = []\n    i = 0\n\n    self.initial_train_size = date_to_index_position(\n        index=index,\n        date_input=self.initial_train_size,\n        method=\"validation\",\n        date_literal=\"initial_train_size\",\n    )\n\n    if window_size_as_date_offset:\n        if self.initial_train_size is not None:\n            if self.initial_train_size &lt; self.window_size:\n                raise ValueError(\n                    f\"If `initial_train_size` is an integer, it must be greater than \"\n                    f\"the `window_size` of the forecaster ({self.window_size}) \"\n                    f\"and smaller than the length of the series ({len(X)}). If \"\n                    f\"it is a date, it must be within this range of the index.\"\n                )\n\n    if self.allow_incomplete_fold:\n        # At least one observation after the gap to allow incomplete fold\n        if len(index) &lt;= self.initial_train_size + self.gap:\n            raise ValueError(\n                f\"The time series must have more than `initial_train_size + gap` \"\n                f\"observations to create at least one fold.\\n\"\n                f\"    Time series length: {len(index)}\\n\"\n                f\"    Required &gt; {self.initial_train_size + self.gap}\\n\"\n                f\"    initial_train_size: {self.initial_train_size}\\n\"\n                f\"    gap: {self.gap}\\n\"\n            )\n    else:\n        # At least one complete fold\n        if len(index) &lt; self.initial_train_size + self.gap + self.steps:\n            raise ValueError(\n                f\"The time series must have at least `initial_train_size + gap + steps` \"\n                f\"observations to create a minimum of one complete fold \"\n                f\"(allow_incomplete_fold=False).\\n\"\n                f\"    Time series length: {len(index)}\\n\"\n                f\"    Required &gt;= {self.initial_train_size + self.gap + self.steps}\\n\"\n                f\"    initial_train_size: {self.initial_train_size}\\n\"\n                f\"    gap: {self.gap}\\n\"\n                f\"    steps: {self.steps}\\n\"\n            )\n\n    while self.initial_train_size + (i * self.fold_stride) + self.gap &lt; len(index):\n\n        if self.refit:\n            # NOTE: If `fixed_train_size` the train size doesn't increase but\n            # moves by `fold_stride` positions in each iteration. If `False`,\n            # the train size increases by `fold_stride` in each iteration.\n            train_iloc_start = (\n                i * (self.fold_stride) if self.fixed_train_size else 0\n            )\n            train_iloc_end = self.initial_train_size + i * (self.fold_stride)\n            test_iloc_start = train_iloc_end\n        else:\n            # NOTE: The train size doesn't increase and doesn't move.\n            train_iloc_start = 0\n            train_iloc_end = self.initial_train_size\n            test_iloc_start = self.initial_train_size + i * (self.fold_stride)\n\n        if self.window_size is not None:\n            last_window_iloc_start = test_iloc_start - self.window_size\n\n        test_iloc_end = test_iloc_start + self.gap + self.steps\n\n        partitions = [\n            idx[train_iloc_start:train_iloc_end],\n            (\n                idx[last_window_iloc_start:test_iloc_start]\n                if self.window_size is not None\n                else []\n            ),\n            idx[test_iloc_start:test_iloc_end],\n            idx[test_iloc_start + self.gap : test_iloc_end],\n        ]\n        folds.append(partitions)\n        i += 1\n\n    # NOTE: Delete all incomplete folds at the end if not allowed\n    n_removed_folds = 0\n    if not self.allow_incomplete_fold:\n        # NOTE: While folds and the last \"test_index_with_gap\" is incomplete,\n        # calculating len of range objects\n        while folds and len(folds[-1][3]) &lt; self.steps:\n            folds.pop()\n            n_removed_folds += 1\n\n    # Replace partitions inside folds with length 0 with `None`\n    folds = [\n        [partition if len(partition) &gt; 0 else None for partition in fold]\n        for fold in folds\n    ]\n\n    # Create a flag to know whether to train the forecaster\n    if self.refit == 0:\n        self.refit = False\n\n    if isinstance(self.refit, bool):\n        fit_forecaster = [self.refit] * len(folds)\n        fit_forecaster[0] = True\n    else:\n        fit_forecaster = [False] * len(folds)\n        for i in range(0, len(fit_forecaster), self.refit):\n            fit_forecaster[i] = True\n\n    for i in range(len(folds)):\n        folds[i].insert(0, i)\n        folds[i].append(fit_forecaster[i])\n        if fit_forecaster[i] is False:\n            folds[i][1] = folds[i - 1][1]\n\n    index_to_skip = []\n    if self.skip_folds is not None:\n        if isinstance(self.skip_folds, (int, np.integer)) and self.skip_folds &gt; 0:\n            index_to_keep = np.arange(0, len(folds), self.skip_folds)\n            index_to_skip = np.setdiff1d(\n                np.arange(0, len(folds)), index_to_keep, assume_unique=True\n            )\n            index_to_skip = [\n                int(x) for x in index_to_skip\n            ]  # Required since numpy 2.0\n        if isinstance(self.skip_folds, list):\n            index_to_skip = [i for i in self.skip_folds if i &lt; len(folds)]\n\n    if self.verbose:\n        self._print_info(\n            index=index,\n            folds=folds,\n            externally_fitted=externally_fitted,\n            n_removed_folds=n_removed_folds,\n            index_to_skip=index_to_skip,\n        )\n\n    folds = [fold for i, fold in enumerate(folds) if i not in index_to_skip]\n    if not self.return_all_indexes:\n        # NOTE: +1 to prevent iloc pandas from deleting the last observation\n        folds = [\n            [\n                fold[0],\n                [fold[1][0], fold[1][-1] + 1],\n                (\n                    [fold[2][0], fold[2][-1] + 1]\n                    if self.window_size is not None\n                    else []\n                ),\n                [fold[3][0], fold[3][-1] + 1],\n                [fold[4][0], fold[4][-1] + 1],\n                fold[5],\n            ]\n            for fold in folds\n        ]\n\n    if externally_fitted:\n        self.initial_train_size = None\n        folds[0][5] = False\n\n    if as_pandas:\n        if self.window_size is None:\n            for fold in folds:\n                fold[2] = [None, None]\n\n        if not self.return_all_indexes:\n            folds = pd.DataFrame(\n                data=[\n                    [fold[0]] + list(itertools.chain(*fold[1:-1])) + [fold[-1]]\n                    for fold in folds\n                ],\n                columns=[\n                    \"fold\",\n                    \"train_start\",\n                    \"train_end\",\n                    \"last_window_start\",\n                    \"last_window_end\",\n                    \"test_start\",\n                    \"test_end\",\n                    \"test_start_with_gap\",\n                    \"test_end_with_gap\",\n                    \"fit_forecaster\",\n                ],\n            )\n        else:\n            folds = pd.DataFrame(\n                data=folds,\n                columns=[\n                    \"fold\",\n                    \"train_index\",\n                    \"last_window_index\",\n                    \"test_index\",\n                    \"test_index_with_gap\",\n                    \"fit_forecaster\",\n                ],\n            )\n\n    return folds\n</code></pre>"},{"location":"api/model_selection/#design-philosophy-for-safety-critical-validation","title":"Design Philosophy for Safety-Critical Validation","text":"<p>In safety-critical forecasting (medical devices, autonomous systems, financial trading), improper validation can lead to catastrophic failures in production. TimeSeriesFold implements several defensive patterns:</p> <ol> <li>Temporal Integrity Enforcement: Strict chronological ordering prevents look-ahead bias that would invalidate safety assessments</li> <li>Realistic Retraining Simulation: Configurable refit strategies mirror actual production deployment patterns</li> <li>Gap Handling: Models the delay between data availability and prediction requirements</li> <li>Incomplete Fold Management: Handles edge cases at data boundaries that could cause production failures</li> </ol>"},{"location":"api/model_selection/#timeseriesfold-as-a-fallback-mechanism","title":"TimeSeriesFold as a Fallback Mechanism","text":"<p>The TimeSeriesFold class serves multiple roles in a defense-in-depth validation strategy:</p> <ul> <li>Primary Validation Layer: Provides the fundamental train/test split infrastructure for all model evaluation</li> <li>Degradation Detection: Regular backtesting with fixed fold configurations detects model performance decay over time</li> <li>A/B Test Infrastructure: Consistent fold generation ensures fair comparison between model versions</li> <li>Rollback Validation: Enables testing whether reverting to an older model would improve performance</li> <li>Stress Testing Framework: Configurable parameters allow testing models under various temporal conditions</li> </ul>"},{"location":"api/model_selection/#understanding-fold-configuration-parameters","title":"Understanding Fold Configuration Parameters","text":"<p>The fold configuration directly impacts validation realism and computational cost. Key tradeoffs:</p> <ol> <li>initial_train_size: Larger values provide more stable models but reduce validation data</li> <li>refit frequency: More frequent refitting increases realism but multiplies computational cost</li> <li>fixed_train_size vs expanding window: Fixed mimics resource-constrained systems, expanding uses all available information</li> <li>fold_stride: Smaller strides provide more evaluation points but increase overlap and computation</li> <li>gap: Models real-world delays between data collection and prediction deployment</li> </ol>"},{"location":"api/model_selection/#safety-critical-configuration-patterns","title":"Safety-Critical Configuration Patterns","text":"<p>Different safety-critical applications require different validation strategies:</p> <ol> <li>High-Frequency Trading: Small gaps (seconds), frequent refit, fixed window to match production constraints</li> <li>Medical Monitoring: Expanding window (use all patient history), moderate refit, strict temporal gaps</li> <li>Energy Grid Management: Daily/weekly refit cycles, expanding window, minimal gaps for immediate response</li> <li>Autonomous Vehicles: Very frequent refit (continuous learning), small fixed windows for real-time constraints</li> </ol>"},{"location":"api/model_selection/#complete-examples_1","title":"Complete Examples","text":""},{"location":"api/model_selection/#example-1-medical-device-validation-expanding-window-strategy","title":"Example 1: Medical Device Validation - Expanding Window Strategy","text":"<p>This example demonstrates validation for a medical monitoring system where patient safety depends on reliable predictions. The expanding window strategy uses all available patient history.</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom spotforecast2_safe.forecaster.recursive import ForecasterRecursive\nfrom spotforecast2.model_selection import backtesting_forecaster, TimeSeriesFold\n\n# Simulate patient vital signs monitoring (heart rate)\nrng = np.random.default_rng(123)\ndates = pd.date_range(\"2024-01-01\", periods=30 * 24 * 60, freq=\"min\")  # 30 days, 1-minute intervals\n\n# Baseline heart rate with circadian rhythm and random variations\nhour_of_day = dates.hour + dates.minute / 60\ncircadian_pattern = 10 * np.sin(2 * np.pi * (hour_of_day - 6) / 24)  # Peak afternoon\nbaseline_hr = 70\nnoise = rng.normal(0, 3, len(dates))\n# Simulate gradual patient deterioration\ndeterioration = np.linspace(0, 15, len(dates))\n\ny = pd.Series(\n    baseline_hr + circadian_pattern + deterioration + noise,\n    index=dates,\n    name=\"heart_rate_bpm\",\n)\n\n# Medical device configuration: Conservative, expanding window\nforecaster = ForecasterRecursive(\n    estimator=RandomForestRegressor(n_estimators=50, random_state=123),\n    lags=60,  # Last 60 minutes of data\n)\n\n# Validation strategy: Expanding window to use all patient history\ncv = TimeSeriesFold(\n    steps=30,  # Predict 30 minutes ahead\n    initial_train_size=7 * 24 * 60,  # 7 days initial training\n    refit=24 * 60,  # Retrain daily (balance freshness vs computation)\n    fixed_train_size=False,  # Expanding: use all patient history\n    fold_stride=24 * 60,  # Evaluate daily\n    gap=5,  # 5-minute processing delay (realistic constraint)\n    allow_incomplete_fold=True,  # Don't discard recent data\n    verbose=True,\n)\n\n# Inspect fold structure before running expensive backtesting\nfolds_df = cv.split(y, as_pandas=True)\nprint(\"Fold Structure for Medical Device Validation:\")\nprint(folds_df[[\"fold\", \"train_start\", \"train_end\", \"test_start\", \"test_end\"]])\nprint(f\"\\nTotal folds: {len(folds_df)}\")\nprint(f\"Training data grows from {folds_df.iloc[0]['train_end'] - folds_df.iloc[0]['train_start']} \"\n      f\"to {folds_df.iloc[-1]['train_end'] - folds_df.iloc[-1]['train_start']} observations\")\n\n# Run backtesting with the configured folds\nmetric_values, predictions = backtesting_forecaster(\n    forecaster=forecaster,\n    y=y,\n    cv=cv,\n    metric=\"mean_absolute_error\",\n    verbose=False,\n    show_progress=True,\n)\n\nprint(f\"\\nMedical Device Validation Results:\")\nprint(f\"Mean Absolute Error: {metric_values['mean_absolute_error'].mean():.2f} BPM\")\nprint(f\"Max Error: {(y.loc[predictions.index] - predictions['pred']).abs().max():.2f} BPM\")\n\n# Safety check: Verify no catastrophic errors\nmax_acceptable_error = 10.0  # BPM\ncatastrophic_errors = (y.loc[predictions.index] - predictions['pred']).abs() &gt; max_acceptable_error\nif catastrophic_errors.any():\n    print(f\"\u26a0 WARNING: {catastrophic_errors.sum()} predictions exceed {max_acceptable_error} BPM threshold\")\nelse:\n    print(f\"\u2713 All predictions within {max_acceptable_error} BPM safety threshold\")\n</code></pre> <p>Key medical device validation elements:</p> <ol> <li>Expanding window: Uses complete patient history for maximum information</li> <li>Daily retraining: Balances model freshness with computational constraints</li> <li>Gap parameter: Models realistic processing delays in medical devices</li> <li>Incomplete folds allowed: Ensures most recent data is evaluated</li> <li>Safety thresholds: Explicit error bounds for clinical acceptability</li> </ol>"},{"location":"api/model_selection/#example-2-high-frequency-trading-fixed-window-strategy","title":"Example 2: High-Frequency Trading - Fixed Window Strategy","text":"<p>This example demonstrates validation for a high-frequency trading system where computational resources are constrained and only recent data is relevant.</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import Ridge\nfrom spotforecast2_safe.forecaster.recursive import ForecasterRecursive\nfrom spotforecast2.model_selection import backtesting_forecaster, TimeSeriesFold\n\n# Simulate high-frequency price data\nrng = np.random.default_rng(456)\ndates = pd.date_range(\"2024-01-01 09:30\", periods=6.5 * 60 * 60, freq=\"s\")  # Trading day, 1-second bars\n\n# Price with mean reversion and volatility clustering\nprice = 100.0\nprices = [price]\nfor i in range(len(dates) - 1):\n    # Mean reversion + random walk\n    drift = -0.0001 * (price - 100.0)\n    volatility = 0.01 * (1 + 0.5 * abs(rng.normal()))\n    price += drift + rng.normal(0, volatility)\n    prices.append(price)\n\ny = pd.Series(prices, index=dates, name=\"price_usd\")\n\n# HFT configuration: Fast, fixed window\nforecaster = ForecasterRecursive(\n    estimator=Ridge(alpha=0.1),  # Fast linear model\n    lags=60,  # Last 60 seconds\n)\n\n# Validation strategy: Fixed window mimicking production constraints\ncv = TimeSeriesFold(\n    steps=10,  # Predict 10 seconds ahead\n    initial_train_size=3600,  # 1 hour initial training\n    refit=300,  # Retrain every 5 minutes (production-realistic)\n    fixed_train_size=True,  # Fixed: only use recent data (memory constraint)\n    fold_stride=300,  # Evaluate every 5 minutes\n    gap=1,  # 1-second execution delay\n    allow_incomplete_fold=False,  # Strict: only complete folds\n    verbose=True,\n)\n\n# Inspect fold structure\nfolds_df = cv.split(y, as_pandas=True)\nprint(\"Fold Structure for HFT Validation:\")\nprint(folds_df[[\"fold\", \"train_start\", \"train_end\", \"test_start\", \"test_end\"]].head(10))\nprint(f\"\\nTotal folds: {len(folds_df)}\")\nprint(f\"Training window size: {folds_df.iloc[0]['train_end'] - folds_df.iloc[0]['train_start']} observations (constant)\")\n\n# Run backtesting\nmetric_values, predictions = backtesting_forecaster(\n    forecaster=forecaster,\n    y=y,\n    cv=cv,\n    metric=\"mean_absolute_error\",\n    verbose=False,\n    show_progress=True,\n)\n\nprint(f\"\\nHFT Validation Results:\")\nprint(f\"Mean Absolute Error: {metric_values['mean_absolute_error'].mean():.4f} USD\")\n\n# Profitability check: Can we beat transaction costs?\ntransaction_cost = 0.001  # $0.001 per trade\nprediction_accuracy = metric_values['mean_absolute_error'].mean()\nif prediction_accuracy &lt; transaction_cost:\n    print(f\"\u2713 Prediction accuracy ({prediction_accuracy:.4f}) beats transaction costs ({transaction_cost:.4f})\")\nelse:\n    print(f\"\u2717 Prediction accuracy ({prediction_accuracy:.4f}) exceeds transaction costs ({transaction_cost:.4f})\")\n    print(\"  Strategy not viable for production deployment\")\n</code></pre> <p>Key HFT validation elements:</p> <ol> <li>Fixed window: Mimics production memory constraints and recency bias</li> <li>Frequent retraining: Matches realistic production update cycles</li> <li>Small gap: Models minimal execution delay</li> <li>No incomplete folds: Ensures all evaluations use complete test sets</li> <li>Transaction cost analysis: Economic viability check before deployment</li> </ol>"},{"location":"api/model_selection/#example-3-overlapping-folds-for-robust-evaluation","title":"Example 3: Overlapping Folds for Robust Evaluation","text":"<p>This example demonstrates using overlapping folds to increase evaluation density without changing the forecast horizon.</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom spotforecast2_safe.forecaster.recursive import ForecasterRecursive\nfrom spotforecast2.model_selection import backtesting_forecaster, TimeSeriesFold\n\n# Simulate industrial sensor data (temperature)\nrng = np.random.default_rng(789)\ndates = pd.date_range(\"2024-01-01\", periods=365, freq=\"D\")\n\n# Seasonal pattern + trend + noise\nday_of_year = np.arange(len(dates))\nseasonal = 10 * np.sin(2 * np.pi * day_of_year / 365)\ntrend = 0.01 * day_of_year\nnoise = rng.normal(0, 2, len(dates))\n\ny = pd.Series(20 + seasonal + trend + noise, index=dates, name=\"temperature_c\")\n\n# Industrial monitoring configuration\nforecaster = ForecasterRecursive(\n    estimator=GradientBoostingRegressor(n_estimators=50, random_state=789),\n    lags=30,  # Last 30 days\n)\n\n# Overlapping folds: More evaluation points for robust assessment\ncv = TimeSeriesFold(\n    steps=7,  # Predict 7 days ahead\n    initial_train_size=180,  # 6 months initial training\n    refit=7,  # Retrain weekly\n    fixed_train_size=False,  # Expanding window\n    fold_stride=1,  # Advance 1 day at a time (creates overlap!)\n    gap=0,\n    allow_incomplete_fold=True,\n    verbose=True,\n)\n\n# Inspect overlapping structure\nfolds_df = cv.split(y, as_pandas=True)\nprint(\"Overlapping Fold Structure:\")\nprint(folds_df[[\"fold\", \"test_start\", \"test_end\"]].head(15))\nprint(f\"\\nTotal folds: {len(folds_df)}\")\nprint(f\"Overlap: Each observation appears in up to {cv.steps} different test sets\")\n\n# Run backtesting\nmetric_values, predictions = backtesting_forecaster(\n    forecaster=forecaster,\n    y=y,\n    cv=cv,\n    metric=\"mean_absolute_error\",\n    verbose=False,\n    show_progress=True,\n)\n\nprint(f\"\\nIndustrial Monitoring Validation Results:\")\nprint(f\"Mean Absolute Error: {metric_values['mean_absolute_error'].mean():.2f} \u00b0C\")\nprint(f\"Number of predictions: {len(predictions)}\")\nprint(f\"Number of unique timestamps: {len(predictions.index.unique())}\")\n\n# Note: With overlapping folds, some timestamps have multiple predictions\n# This provides multiple independent forecasts for the same period\n</code></pre> <p>Key overlapping fold elements:</p> <ol> <li>fold_stride &lt; steps: Creates overlapping test sets</li> <li>Multiple forecasts per timestamp: Provides ensemble-like robustness assessment</li> <li>Increased evaluation density: More data points for statistical significance</li> <li>Computational cost: Proportional to number of folds (trade-off consideration)</li> </ol>"},{"location":"api/model_selection/#one-step-ahead-fold-simplified-validation-for-specific-use-cases","title":"One Step Ahead Fold: Simplified Validation for Specific Use Cases","text":"<p>The OneStepAheadFold class provides a streamlined validation approach for one-step-ahead forecasting scenarios. Unlike TimeSeriesFold which creates multiple temporal folds, OneStepAheadFold creates a single train/test split optimized for evaluating models on all remaining data after initial training.</p>"},{"location":"api/model_selection/#spotforecast2_safe.model_selection.OneStepAheadFold","title":"<code>spotforecast2_safe.model_selection.OneStepAheadFold</code>","text":"<p>               Bases: <code>BaseFold</code></p> <p>Class to split time series data into train and test folds for one-step-ahead forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>initial_train_size</code> <code>int | str | Timestamp</code> <p>Number of observations used for initial training.</p> <ul> <li>If an integer, the number of observations used for initial training.</li> <li>If a date string or pandas Timestamp, it is the last date included in   the initial training set.</li> </ul> required <code>window_size</code> <code>int</code> <p>Number of observations needed to generate the autoregressive predictors. Defaults to None.</p> <code>None</code> <code>differentiation</code> <code>int</code> <p>Number of observations to use for differentiation. This is used to extend the <code>last_window</code> as many observations as the differentiation order. Defaults to None.</p> <code>None</code> <code>return_all_indexes</code> <code>bool</code> <p>Whether to return all indexes or only the start and end indexes of each fold. Defaults to False.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Whether to print information about generated folds. Defaults to True.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>initial_train_size</code> <code>int</code> <p>Number of observations used for initial training.</p> <code>window_size</code> <code>int</code> <p>Number of observations needed to generate the autoregressive predictors.</p> <code>differentiation</code> <code>int</code> <p>Number of observations to use for differentiation. This is used to extend the <code>last_window</code> as many observations as the differentiation order.</p> <code>return_all_indexes</code> <code>bool</code> <p>Whether to return all indexes or only the start and end indexes of each fold.</p> <code>verbose</code> <code>bool</code> <p>Whether to print information about generated folds.</p> Source code in <code>spotforecast2_safe/model_selection/split_one_step.py</code> <pre><code>class OneStepAheadFold(BaseFold):\n    \"\"\"\n    Class to split time series data into train and test folds for one-step-ahead\n    forecasting.\n\n    Args:\n        initial_train_size (int | str | pd.Timestamp): Number of observations used\n            for initial training.\n\n            - If an integer, the number of observations used for initial training.\n            - If a date string or pandas Timestamp, it is the last date included in\n              the initial training set.\n        window_size (int, optional): Number of observations needed to generate the\n            autoregressive predictors. Defaults to None.\n        differentiation (int, optional): Number of observations to use for differentiation.\n            This is used to extend the `last_window` as many observations as the\n            differentiation order. Defaults to None.\n        return_all_indexes (bool, optional): Whether to return all indexes or only the\n            start and end indexes of each fold. Defaults to False.\n        verbose (bool, optional): Whether to print information about generated folds.\n            Defaults to True.\n\n    Attributes:\n        initial_train_size (int): Number of observations used for initial training.\n        window_size (int): Number of observations needed to generate the\n            autoregressive predictors.\n        differentiation (int): Number of observations to use for differentiation.\n            This is used to extend the `last_window` as many observations as the\n            differentiation order.\n        return_all_indexes (bool): Whether to return all indexes or only the start\n            and end indexes of each fold.\n        verbose (bool): Whether to print information about generated folds.\n    \"\"\"\n\n    def __init__(\n        self,\n        initial_train_size: int | str | pd.Timestamp,\n        window_size: int | None = None,\n        differentiation: int | None = None,\n        return_all_indexes: bool = False,\n        verbose: bool = True,\n    ) -&gt; None:\n\n        super().__init__(\n            initial_train_size=initial_train_size,\n            window_size=window_size,\n            differentiation=differentiation,\n            return_all_indexes=return_all_indexes,\n            verbose=verbose,\n        )\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Information displayed when printed.\n        \"\"\"\n\n        info = (\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"{type(self).__name__} \\n\"\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"Initial train size = {self.initial_train_size},\\n\"\n            f\"Window size        = {self.window_size},\\n\"\n            f\"Differentiation    = {self.differentiation},\\n\"\n            f\"Return all indexes = {self.return_all_indexes},\\n\"\n            f\"Verbose            = {self.verbose}\\n\"\n        )\n\n        return info\n\n    def _repr_html_(self) -&gt; str:\n        \"\"\"\n        HTML representation of the object.\n        The \"General Information\" section is expanded by default.\n        \"\"\"\n\n        style, unique_id = get_style_repr_html()\n        content = f\"\"\"\n        &lt;div class=\"container-{unique_id}\"&gt;\n            &lt;p style=\"font-size: 1.5em; font-weight: bold; margin-block-start: 0.83em; margin-block-end: 0.83em;\"&gt;{type(self).__name__}&lt;/p&gt;\n            &lt;details open&gt;\n                &lt;summary&gt;General Information&lt;/summary&gt;\n                &lt;ul&gt;\n                    &lt;li&gt;&lt;strong&gt;Initial train size:&lt;/strong&gt; {self.initial_train_size}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Window size:&lt;/strong&gt; {self.window_size}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Differentiation:&lt;/strong&gt; {self.differentiation}&lt;/li&gt;\n                    &lt;li&gt;&lt;strong&gt;Return all indexes:&lt;/strong&gt; {self.return_all_indexes}&lt;/li&gt;\n                &lt;/ul&gt;\n            &lt;/details&gt;\n        &lt;/div&gt;\n        \"\"\"\n\n        return style + content\n\n    def split(\n        self,\n        X: pd.Series | pd.DataFrame | pd.Index | dict[str, pd.Series | pd.DataFrame],\n        as_pandas: bool = False,\n        externally_fitted: Any = None,\n    ) -&gt; list | pd.DataFrame:\n        \"\"\"\n        Split the time series data into train and test folds.\n\n        Args:\n            X (pd.Series | pd.DataFrame | pd.Index | dict): Time series data or index to split.\n            as_pandas (bool, optional): If True, the folds are returned as a DataFrame.\n                This is useful to visualize the folds in a more interpretable way.\n                Defaults to False.\n            externally_fitted (Any, optional): This argument is not used in this class.\n                It is included for API consistency. Defaults to None.\n\n        Returns:\n            list | pd.DataFrame: A list of lists containing the indices (position) of\n            the fold. The list contains 2 lists with the following information:\n\n            - fold: fold number.\n            - [train_start, train_end]: list with the start and end positions of the\n                training set.\n            - [test_start, test_end]: list with the start and end positions of the test\n                set. These are the observations used to evaluate the forecaster.\n            - fit_forecaster: boolean indicating whether the forecaster should be fitted\n                in this fold.\n\n            It is important to note that the returned values are the positions of the\n            observations and not the actual values of the index, so they can be used to\n            slice the data directly using iloc.\n\n            If `as_pandas` is `True`, the folds are returned as a DataFrame with the\n            following columns: 'fold', 'train_start', 'train_end', 'test_start',\n            'test_end', 'fit_forecaster'.\n\n            Following the python convention, the start index is inclusive and the end\n            index is exclusive. This means that the last index is not included in the\n            slice.\n        \"\"\"\n\n        if not isinstance(X, (pd.Series, pd.DataFrame, pd.Index, dict)):\n            raise TypeError(\n                f\"X must be a pandas Series, DataFrame, Index or a dictionary. \"\n                f\"Got {type(X)}.\"\n            )\n\n        index = self._extract_index(X)\n\n        self.initial_train_size = date_to_index_position(\n            index=index,\n            date_input=self.initial_train_size,\n            method=\"validation\",\n            date_literal=\"initial_train_size\",\n        )\n\n        fold = [\n            0,\n            [0, self.initial_train_size - 1],\n            [self.initial_train_size, len(X)],\n            True,\n        ]\n\n        if self.verbose:\n            self._print_info(index=index, fold=fold)\n\n        # NOTE: +1 to prevent iloc pandas from deleting the last observation\n        if self.return_all_indexes:\n            fold = [\n                fold[0],\n                [range(fold[1][0], fold[1][1] + 1)],\n                [range(fold[2][0], fold[2][1])],\n                fold[3],\n            ]\n        else:\n            fold = [\n                fold[0],\n                [fold[1][0], fold[1][1] + 1],\n                [fold[2][0], fold[2][1]],\n                fold[3],\n            ]\n\n        if as_pandas:\n            if not self.return_all_indexes:\n                fold = pd.DataFrame(\n                    data=[[fold[0]] + list(itertools.chain(*fold[1:-1])) + [fold[-1]]],\n                    columns=[\n                        \"fold\",\n                        \"train_start\",\n                        \"train_end\",\n                        \"test_start\",\n                        \"test_end\",\n                        \"fit_forecaster\",\n                    ],\n                )\n            else:\n                fold = pd.DataFrame(\n                    data=[fold],\n                    columns=[\"fold\", \"train_index\", \"test_index\", \"fit_forecaster\"],\n                )\n\n        return fold\n\n    def _print_info(self, index: pd.Index, fold: list[list[int]]) -&gt; None:\n        \"\"\"\n        Print information about folds.\n\n        Args:\n            index (pd.Index): Index of the time series data.\n            fold (list): A list of lists containing the indices (position) of the fold.\n        \"\"\"\n\n        if self.differentiation is None:\n            differentiation = 0\n        else:\n            differentiation = self.differentiation\n\n        initial_train_size = self.initial_train_size - differentiation\n        test_length = len(index) - (initial_train_size + differentiation)\n\n        print(\"Information of folds\")\n        print(\"--------------------\")\n        print(f\"Number of observations in train: {initial_train_size}\")\n        if self.differentiation is not None:\n            print(\n                f\"    First {differentiation} observation/s in training set \"\n                f\"are used for differentiation\"\n            )\n        print(f\"Number of observations in test: {test_length}\")\n\n        training_start = index[fold[1][0] + differentiation]\n        training_end = index[fold[1][-1]]\n        test_start = index[fold[2][0]]\n        test_end = index[fold[2][-1] - 1]\n\n        print(f\"Training : {training_start} -- {training_end} (n={initial_train_size})\")\n        print(f\"Test     : {test_start} -- {test_end} (n={test_length})\")\n        print(\"\")\n</code></pre>"},{"location":"api/model_selection/#spotforecast2_safe.model_selection.OneStepAheadFold.__repr__","title":"<code>__repr__()</code>","text":"<p>Information displayed when printed.</p> Source code in <code>spotforecast2_safe/model_selection/split_one_step.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Information displayed when printed.\n    \"\"\"\n\n    info = (\n        f\"{'=' * len(type(self).__name__)} \\n\"\n        f\"{type(self).__name__} \\n\"\n        f\"{'=' * len(type(self).__name__)} \\n\"\n        f\"Initial train size = {self.initial_train_size},\\n\"\n        f\"Window size        = {self.window_size},\\n\"\n        f\"Differentiation    = {self.differentiation},\\n\"\n        f\"Return all indexes = {self.return_all_indexes},\\n\"\n        f\"Verbose            = {self.verbose}\\n\"\n    )\n\n    return info\n</code></pre>"},{"location":"api/model_selection/#spotforecast2_safe.model_selection.OneStepAheadFold.split","title":"<code>split(X, as_pandas=False, externally_fitted=None)</code>","text":"<p>Split the time series data into train and test folds.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Series | DataFrame | Index | dict</code> <p>Time series data or index to split.</p> required <code>as_pandas</code> <code>bool</code> <p>If True, the folds are returned as a DataFrame. This is useful to visualize the folds in a more interpretable way. Defaults to False.</p> <code>False</code> <code>externally_fitted</code> <code>Any</code> <p>This argument is not used in this class. It is included for API consistency. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list | DataFrame</code> <p>list | pd.DataFrame: A list of lists containing the indices (position) of</p> <code>list | DataFrame</code> <p>the fold. The list contains 2 lists with the following information:</p> <code>list | DataFrame</code> <ul> <li>fold: fold number.</li> </ul> <code>list | DataFrame</code> <ul> <li>[train_start, train_end]: list with the start and end positions of the training set.</li> </ul> <code>list | DataFrame</code> <ul> <li>[test_start, test_end]: list with the start and end positions of the test set. These are the observations used to evaluate the forecaster.</li> </ul> <code>list | DataFrame</code> <ul> <li>fit_forecaster: boolean indicating whether the forecaster should be fitted in this fold.</li> </ul> <code>list | DataFrame</code> <p>It is important to note that the returned values are the positions of the</p> <code>list | DataFrame</code> <p>observations and not the actual values of the index, so they can be used to</p> <code>list | DataFrame</code> <p>slice the data directly using iloc.</p> <code>list | DataFrame</code> <p>If <code>as_pandas</code> is <code>True</code>, the folds are returned as a DataFrame with the</p> <code>list | DataFrame</code> <p>following columns: 'fold', 'train_start', 'train_end', 'test_start',</p> <code>list | DataFrame</code> <p>'test_end', 'fit_forecaster'.</p> <code>list | DataFrame</code> <p>Following the python convention, the start index is inclusive and the end</p> <code>list | DataFrame</code> <p>index is exclusive. This means that the last index is not included in the</p> <code>list | DataFrame</code> <p>slice.</p> Source code in <code>spotforecast2_safe/model_selection/split_one_step.py</code> <pre><code>def split(\n    self,\n    X: pd.Series | pd.DataFrame | pd.Index | dict[str, pd.Series | pd.DataFrame],\n    as_pandas: bool = False,\n    externally_fitted: Any = None,\n) -&gt; list | pd.DataFrame:\n    \"\"\"\n    Split the time series data into train and test folds.\n\n    Args:\n        X (pd.Series | pd.DataFrame | pd.Index | dict): Time series data or index to split.\n        as_pandas (bool, optional): If True, the folds are returned as a DataFrame.\n            This is useful to visualize the folds in a more interpretable way.\n            Defaults to False.\n        externally_fitted (Any, optional): This argument is not used in this class.\n            It is included for API consistency. Defaults to None.\n\n    Returns:\n        list | pd.DataFrame: A list of lists containing the indices (position) of\n        the fold. The list contains 2 lists with the following information:\n\n        - fold: fold number.\n        - [train_start, train_end]: list with the start and end positions of the\n            training set.\n        - [test_start, test_end]: list with the start and end positions of the test\n            set. These are the observations used to evaluate the forecaster.\n        - fit_forecaster: boolean indicating whether the forecaster should be fitted\n            in this fold.\n\n        It is important to note that the returned values are the positions of the\n        observations and not the actual values of the index, so they can be used to\n        slice the data directly using iloc.\n\n        If `as_pandas` is `True`, the folds are returned as a DataFrame with the\n        following columns: 'fold', 'train_start', 'train_end', 'test_start',\n        'test_end', 'fit_forecaster'.\n\n        Following the python convention, the start index is inclusive and the end\n        index is exclusive. This means that the last index is not included in the\n        slice.\n    \"\"\"\n\n    if not isinstance(X, (pd.Series, pd.DataFrame, pd.Index, dict)):\n        raise TypeError(\n            f\"X must be a pandas Series, DataFrame, Index or a dictionary. \"\n            f\"Got {type(X)}.\"\n        )\n\n    index = self._extract_index(X)\n\n    self.initial_train_size = date_to_index_position(\n        index=index,\n        date_input=self.initial_train_size,\n        method=\"validation\",\n        date_literal=\"initial_train_size\",\n    )\n\n    fold = [\n        0,\n        [0, self.initial_train_size - 1],\n        [self.initial_train_size, len(X)],\n        True,\n    ]\n\n    if self.verbose:\n        self._print_info(index=index, fold=fold)\n\n    # NOTE: +1 to prevent iloc pandas from deleting the last observation\n    if self.return_all_indexes:\n        fold = [\n            fold[0],\n            [range(fold[1][0], fold[1][1] + 1)],\n            [range(fold[2][0], fold[2][1])],\n            fold[3],\n        ]\n    else:\n        fold = [\n            fold[0],\n            [fold[1][0], fold[1][1] + 1],\n            [fold[2][0], fold[2][1]],\n            fold[3],\n        ]\n\n    if as_pandas:\n        if not self.return_all_indexes:\n            fold = pd.DataFrame(\n                data=[[fold[0]] + list(itertools.chain(*fold[1:-1])) + [fold[-1]]],\n                columns=[\n                    \"fold\",\n                    \"train_start\",\n                    \"train_end\",\n                    \"test_start\",\n                    \"test_end\",\n                    \"fit_forecaster\",\n                ],\n            )\n        else:\n            fold = pd.DataFrame(\n                data=[fold],\n                columns=[\"fold\", \"train_index\", \"test_index\", \"fit_forecaster\"],\n            )\n\n    return fold\n</code></pre>"},{"location":"api/model_selection/#design-philosophy-for-safety-critical-validation_1","title":"Design Philosophy for Safety-Critical Validation","text":"<p>OneStepAheadFold serves as a complementary validation strategy to TimeSeriesFold, particularly valuable in safety-critical contexts where:</p> <ol> <li>Rapid Model Assessment: Quick validation without multiple retraining cycles reduces time-to-deployment</li> <li>Maximum Test Coverage: Uses all post-training data for evaluation, maximizing statistical power</li> <li>Computational Efficiency: Single training run minimizes resource consumption in constrained environments</li> <li>Baseline Establishment: Provides fast baseline performance metrics before more expensive cross-validation</li> </ol>"},{"location":"api/model_selection/#onestepaheadfold-as-a-fallback-mechanism","title":"OneStepAheadFold as a Fallback Mechanism","text":"<p>The OneStepAheadFold class serves critical roles in a layered validation strategy:</p> <ul> <li>Fast Sanity Check: Quick validation before committing to expensive multi-fold cross-validation</li> <li>Computational Fallback: When TimeSeriesFold is too expensive, OneStepAheadFold provides rapid assessment</li> <li>Maximum Data Utilization: Evaluates on all available post-training data without gaps</li> <li>Model Comparison Baseline: Establishes performance floor before testing retraining strategies</li> <li>Emergency Validation: When production issues require immediate model assessment with minimal computation</li> </ul>"},{"location":"api/model_selection/#when-to-use-onestepaheadfold-vs-timeseriesfold","title":"When to Use OneStepAheadFold vs TimeSeriesFold","text":"<p>Choose OneStepAheadFold when:</p> <ol> <li>Computational resources are severely constrained</li> <li>You need rapid initial model assessment</li> <li>The model will not be retrained in production (static deployment)</li> <li>You want to maximize test set size for statistical significance</li> <li>Establishing a performance baseline before more complex validation</li> </ol> <p>Choose TimeSeriesFold when:</p> <ol> <li>The model will be retrained in production (requires realistic refit simulation)</li> <li>You need to assess model degradation over time</li> <li>Computational resources allow multiple training runs</li> <li>You want to test different retraining strategies</li> <li>Production deployment involves rolling forecasts with periodic updates</li> </ol>"},{"location":"api/model_selection/#understanding-the-single-split-strategy","title":"Understanding the Single Split Strategy","text":"<p>OneStepAheadFold creates exactly one fold:</p> <ul> <li>Training Set: First <code>initial_train_size</code> observations</li> <li>Test Set: All remaining observations after training set</li> <li>No Retraining: Model trained once, evaluated on all subsequent data</li> <li>No Gaps: Immediate evaluation after training period ends</li> </ul> <p>This differs fundamentally from TimeSeriesFold's multiple overlapping or sequential folds with configurable retraining.</p>"},{"location":"api/model_selection/#complete-examples_2","title":"Complete Examples","text":""},{"location":"api/model_selection/#example-1-rapid-model-screening-for-safety-critical-deployment","title":"Example 1: Rapid Model Screening for Safety-Critical Deployment","text":"<p>This example demonstrates using OneStepAheadFold for fast initial screening of multiple model candidates before committing to expensive cross-validation.</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import Ridge, Lasso\nfrom spotforecast2_safe.forecaster.recursive import ForecasterRecursive\nfrom spotforecast2.model_selection import backtesting_forecaster, OneStepAheadFold\n\n# Simulate critical infrastructure monitoring (water pressure)\nrng = np.random.default_rng(321)\ndates = pd.date_range(\"2024-01-01\", periods=365 * 24, freq=\"h\")\n\n# Pressure with daily cycle and gradual degradation\nhour_of_day = dates.hour\ndaily_cycle = 5 * np.sin(2 * np.pi * hour_of_day / 24)\nbaseline_pressure = 50  # PSI\ndegradation = -0.01 * np.arange(len(dates))  # Gradual pressure loss\nnoise = rng.normal(0, 1, len(dates))\n\ny = pd.Series(\n    baseline_pressure + daily_cycle + degradation + noise,\n    index=dates,\n    name=\"pressure_psi\",\n)\n\n# Define candidate models for rapid screening\nmodel_candidates = {\n    \"Ridge\": Ridge(alpha=1.0),\n    \"Lasso\": Lasso(alpha=0.1),\n    \"RandomForest\": RandomForestRegressor(n_estimators=50, random_state=321),\n    \"GradientBoosting\": GradientBoostingRegressor(n_estimators=50, random_state=321),\n}\n\n# OneStepAheadFold: Fast screening with single train/test split\ncv = OneStepAheadFold(\n    initial_train_size=180 * 24,  # 6 months training\n    verbose=True,\n)\n\n# Inspect the single fold structure\nfolds_df = cv.split(y, as_pandas=True)\nprint(\"OneStepAheadFold Structure:\")\nprint(folds_df)\nprint(f\"\\nTraining observations: {folds_df['train_end'].iloc[0] - folds_df['train_start'].iloc[0]}\")\nprint(f\"Test observations: {folds_df['test_end'].iloc[0] - folds_df['test_start'].iloc[0]}\")\n\n# Rapid screening of all candidates\nresults = {}\nprint(\"\\nRapid Model Screening Results:\")\nprint(\"=\" * 60)\n\nfor name, estimator in model_candidates.items():\n    forecaster = ForecasterRecursive(\n        estimator=estimator,\n        lags=24 * 7,  # One week of hourly data\n    )\n\n    metric_values, predictions = backtesting_forecaster(\n        forecaster=forecaster,\n        y=y,\n        cv=cv,\n        metric=\"mean_absolute_error\",\n        verbose=False,\n        show_progress=False,\n    )\n\n    mae = metric_values[\"mean_absolute_error\"].iloc[0]\n    results[name] = mae\n    print(f\"{name:20s}: MAE = {mae:.3f} PSI\")\n\n# Select best model for further validation\nbest_model = min(results, key=results.get)\nprint(f\"\\n\u2713 Best model for detailed validation: {best_model}\")\nprint(f\"  MAE: {results[best_model]:.3f} PSI\")\nprint(f\"  Next step: Run TimeSeriesFold cross-validation on {best_model}\")\n\n# Safety check: Verify best model meets minimum requirements\nmax_acceptable_mae = 2.0  # PSI\nif results[best_model] &lt;= max_acceptable_mae:\n    print(f\"\u2713 Best model meets safety threshold ({max_acceptable_mae} PSI)\")\nelse:\n    print(f\"\u2717 WARNING: Best model exceeds safety threshold\")\n    print(f\"  Consider additional feature engineering or model development\")\n</code></pre> <p>Key rapid screening elements:</p> <ol> <li>Single split: Minimal computation for fast iteration</li> <li>Multiple candidates: Screen many models quickly</li> <li>Maximum test data: Uses all post-training data for robust assessment</li> <li>Safety gates: Immediate feedback on viability</li> <li>Workflow integration: Identifies candidates for deeper validation</li> </ol>"},{"location":"api/model_selection/#example-2-static-model-deployment-validation","title":"Example 2: Static Model Deployment Validation","text":"<p>This example demonstrates validating a model that will be deployed without retraining, making OneStepAheadFold the appropriate validation strategy.</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom spotforecast2_safe.forecaster.recursive import ForecasterRecursive\nfrom spotforecast2.model_selection import backtesting_forecaster, OneStepAheadFold\n\n# Simulate embedded sensor system (temperature) with limited compute for retraining\nrng = np.random.default_rng(654)\ndates = pd.date_range(\"2024-01-01\", periods=730, freq=\"D\")  # 2 years\n\n# Seasonal temperature pattern\nday_of_year = np.arange(len(dates)) % 365\nseasonal = 15 * np.sin(2 * np.pi * day_of_year / 365)\nbaseline_temp = 20\nnoise = rng.normal(0, 2, len(dates))\n\ny = pd.Series(baseline_temp + seasonal + noise, index=dates, name=\"temperature_c\")\n\n# Embedded system: Model will be deployed statically (no retraining capability)\nforecaster = ForecasterRecursive(\n    estimator=GradientBoostingRegressor(n_estimators=100, random_state=654),\n    lags=30,  # Last 30 days\n)\n\n# OneStepAheadFold: Matches static deployment (train once, predict forever)\ncv = OneStepAheadFold(\n    initial_train_size=365,  # Train on first year\n    verbose=True,\n)\n\nprint(\"Static Deployment Validation:\")\nprint(\"=\" * 60)\n\n# Validate on entire second year (simulates production behavior)\nmetric_values, predictions = backtesting_forecaster(\n    forecaster=forecaster,\n    y=y,\n    cv=cv,\n    metric=[\"mean_absolute_error\", \"mean_squared_error\"],\n    interval=0.90,  # 90% prediction interval\n    interval_method=\"conformal\",\n    use_in_sample_residuals=True,\n    verbose=False,\n    show_progress=True,\n)\n\nprint(f\"\\nStatic Model Performance (Year 2):\")\nprint(f\"Mean Absolute Error: {metric_values['mean_absolute_error'].iloc[0]:.2f} \u00b0C\")\nprint(f\"RMSE: {np.sqrt(metric_values['mean_squared_error'].iloc[0]):.2f} \u00b0C\")\n\n# Temporal degradation analysis: Check if performance degrades over time\n# Split test period into quarters\ntest_predictions = predictions.copy()\nn_test = len(test_predictions)\nquarter_size = n_test // 4\n\nquarterly_mae = []\nfor i in range(4):\n    start_idx = i * quarter_size\n    end_idx = (i + 1) * quarter_size if i &lt; 3 else n_test\n    quarter_preds = test_predictions.iloc[start_idx:end_idx]\n    quarter_actual = y.loc[quarter_preds.index]\n    quarter_mae = (quarter_actual - quarter_preds[\"pred\"]).abs().mean()\n    quarterly_mae.append(quarter_mae)\n    print(f\"Quarter {i+1} MAE: {quarter_mae:.2f} \u00b0C\")\n\n# Degradation check: Is performance stable over time?\nmae_trend = np.polyfit(range(4), quarterly_mae, 1)[0]\nif mae_trend &gt; 0.1:\n    print(f\"\\n\u26a0 WARNING: Performance degrading over time (trend: +{mae_trend:.3f} \u00b0C/quarter)\")\n    print(\"  Consider implementing periodic retraining capability\")\nelse:\n    print(f\"\\n\u2713 Performance stable over time (trend: {mae_trend:+.3f} \u00b0C/quarter)\")\n    print(\"  Static deployment validated for production\")\n\n# Coverage stability check\ncoverage = (\n    (y.loc[predictions.index] &gt;= predictions[\"lower_bound\"])\n    &amp; (y.loc[predictions.index] &lt;= predictions[\"upper_bound\"])\n).mean()\nprint(f\"\\nPrediction Interval Coverage: {coverage:.1%} (target: 90%)\")\n</code></pre> <p>Key static deployment elements:</p> <ol> <li>Single training: Matches production constraint (no retraining)</li> <li>Long test period: Validates sustained performance</li> <li>Temporal degradation analysis: Detects performance decay</li> <li>Coverage stability: Ensures uncertainty estimates remain calibrated</li> <li>Deployment decision: Clear criteria for static vs dynamic deployment</li> </ol>"},{"location":"api/model_selection/#example-3-emergency-production-validation","title":"Example 3: Emergency Production Validation","text":"<p>This example demonstrates using OneStepAheadFold for rapid validation when production issues require immediate model assessment.</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom spotforecast2_safe.forecaster.recursive import ForecasterRecursive\nfrom spotforecast2.model_selection import backtesting_forecaster, OneStepAheadFold\nimport time\n\n# Simulate production scenario: Model performance suddenly degraded\n# Need to quickly validate if rolling back to previous model version helps\nrng = np.random.default_rng(987)\ndates = pd.date_range(\"2024-01-01\", periods=90, freq=\"D\")\n\n# Simulated production data with recent distribution shift\nbaseline = 100\ntrend = 0.1 * np.arange(len(dates))\n# Distribution shift in last 30 days\nshift = np.where(np.arange(len(dates)) &gt; 60, 10, 0)\nnoise = rng.normal(0, 5, len(dates))\n\ny = pd.Series(baseline + trend + shift + noise, index=dates, name=\"metric\")\n\n# Emergency scenario: Current model failing, test rollback candidate quickly\nrollback_model = ForecasterRecursive(\n    estimator=RandomForestRegressor(n_estimators=30, random_state=987),  # Faster\n    lags=7,\n)\n\n# OneStepAheadFold: Fastest possible validation\ncv = OneStepAheadFold(\n    initial_train_size=60,  # Train on pre-shift data\n    verbose=False,  # Suppress output for speed\n)\n\nprint(\"Emergency Production Validation:\")\nprint(\"=\" * 60)\nprint(\"Scenario: Production model failing, testing rollback candidate...\")\n\n# Time the validation\nstart_time = time.time()\n\nmetric_values, predictions = backtesting_forecaster(\n    forecaster=rollback_model,\n    y=y,\n    cv=cv,\n    metric=\"mean_absolute_error\",\n    verbose=False,\n    show_progress=False,\n)\n\nvalidation_time = time.time() - start_time\n\nprint(f\"\\nValidation completed in {validation_time:.2f} seconds\")\nprint(f\"Rollback Model MAE: {metric_values['mean_absolute_error'].iloc[0]:.2f}\")\n\n# Compare to current production model performance (simulated)\ncurrent_production_mae = 15.0  # Degraded performance\nrollback_mae = metric_values[\"mean_absolute_error\"].iloc[0]\n\nprint(f\"\\nProduction Comparison:\")\nprint(f\"Current Model MAE:  {current_production_mae:.2f}\")\nprint(f\"Rollback Model MAE: {rollback_mae:.2f}\")\nprint(f\"Improvement:        {current_production_mae - rollback_mae:.2f} ({(1 - rollback_mae/current_production_mae)*100:.1f}%)\")\n\n# Emergency decision criteria\nif rollback_mae &lt; current_production_mae * 0.8:  # 20% improvement threshold\n    print(f\"\\n\u2713 RECOMMENDATION: APPROVE rollback\")\n    print(f\"  Significant improvement detected\")\n    print(f\"  Proceed with rollback to restore service\")\nelse:\n    print(f\"\\n\u2717 RECOMMENDATION: REJECT rollback\")\n    print(f\"  Insufficient improvement\")\n    print(f\"  Investigate root cause instead of rollback\")\n\n# Quick diagnostic: Where is the rollback model failing?\nerrors = (y.loc[predictions.index] - predictions[\"pred\"]).abs()\nworst_period_start = errors.idxmax()\nprint(f\"\\nWorst prediction period: {worst_period_start}\")\nprint(f\"Error magnitude: {errors.max():.2f}\")\n</code></pre> <p>Key emergency validation elements:</p> <ol> <li>Speed priority: Minimal computation for rapid decision</li> <li>Rollback testing: Quick assessment of previous model version</li> <li>Clear decision criteria: Quantitative thresholds for action</li> <li>Diagnostic information: Identifies failure modes</li> <li>Production context: Balances speed vs thoroughness appropriately</li> </ol>"},{"location":"api/model_selection/#computational-efficiency-comparison","title":"Computational Efficiency Comparison","text":"<p>OneStepAheadFold vs TimeSeriesFold computational cost:</p> <ul> <li>OneStepAheadFold: 1 model training + 1 prediction pass</li> <li>TimeSeriesFold (10 folds, refit=True): 10 model trainings + 10 prediction passes</li> <li>TimeSeriesFold (10 folds, refit=False): 1 model training + 10 prediction passes</li> </ul> <p>For expensive models or large datasets, OneStepAheadFold can be 10-100x faster than TimeSeriesFold with refit=True.</p>"},{"location":"api/model_selection/#grid-search","title":"Grid Search","text":""},{"location":"api/model_selection/#spotforecast2.model_selection.grid_search","title":"<code>spotforecast2.model_selection.grid_search</code>","text":""},{"location":"api/model_selection/#spotforecast2.model_selection.grid_search.grid_search_forecaster","title":"<code>grid_search_forecaster(forecaster, y, cv, param_grid, metric, exog=None, lags_grid=None, return_best=True, n_jobs='auto', verbose=False, show_progress=True, suppress_warnings=False, output_file=None)</code>","text":"<p>Exhaustive grid search over parameter values for a Forecaster.</p> Source code in <code>src/spotforecast2/model_selection/grid_search.py</code> <pre><code>def grid_search_forecaster(\n    forecaster: object,\n    y: pd.Series,\n    cv: TimeSeriesFold | OneStepAheadFold,\n    param_grid: dict,\n    metric: str | Callable | list[str | Callable],\n    exog: pd.Series | pd.DataFrame | None = None,\n    lags_grid: (\n        list[int | list[int] | np.ndarray[int] | range[int]]\n        | dict[str, list[int | list[int] | np.ndarray[int] | range[int]]]\n        | None\n    ) = None,\n    return_best: bool = True,\n    n_jobs: int | str = \"auto\",\n    verbose: bool = False,\n    show_progress: bool = True,\n    suppress_warnings: bool = False,\n    output_file: str | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Exhaustive grid search over parameter values for a Forecaster.\n    \"\"\"\n\n    param_grid = list(ParameterGrid(param_grid))\n\n    results = _evaluate_grid_hyperparameters(\n        forecaster=forecaster,\n        y=y,\n        cv=cv,\n        param_grid=param_grid,\n        metric=metric,\n        exog=exog,\n        lags_grid=lags_grid,\n        return_best=return_best,\n        n_jobs=n_jobs,\n        verbose=verbose,\n        show_progress=show_progress,\n        suppress_warnings=suppress_warnings,\n        output_file=output_file,\n    )\n\n    return results\n</code></pre>"},{"location":"api/model_selection/#spotoptim-search","title":"spotoptim Search","text":""},{"location":"api/model_selection/#spotforecast2.model_selection.spotoptim_search","title":"<code>spotforecast2.model_selection.spotoptim_search</code>","text":"<p>Hyperparameter search functions for forecasters using SpotOptim.</p>"},{"location":"api/model_selection/#spotforecast2.model_selection.spotoptim_search.spotoptim_search_forecaster","title":"<code>spotoptim_search_forecaster(forecaster, y, cv, search_space, metric, exog=None, n_trials=10, n_initial=5, random_state=123, return_best=True, n_jobs='auto', verbose=False, show_progress=True, suppress_warnings=False, output_file=None, kwargs_spotoptim={})</code>","text":"<p>Hyperparameter optimization for a Forecaster using SpotOptim.</p> <p>Performs hyperparameter search using the SpotOptim library for a Forecaster object. Validation is done using time series backtesting with the provided cross-validation strategy.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>object</code> <p>Forecaster model. Can be ForecasterRecursive, ForecasterDirect, or any compatible forecaster class.</p> required <code>y</code> <code>Series</code> <p>Training time series values. Must be a pandas Series with a datetime or numeric index.</p> required <code>cv</code> <code>TimeSeriesFold | OneStepAheadFold</code> <p>Cross-validation strategy with information needed to split the data into folds. Must be an instance of TimeSeriesFold or OneStepAheadFold.</p> required <code>search_space</code> <code>ParameterSet | Dict[str, Any]</code> <p>Hyperparameter search space. Can be either: - ParameterSet: A ParameterSet object from spotoptim.hyperparameters   defining parameters with their types, bounds, and transformations. - Dict: A dictionary with keys 'bounds', 'var_type', 'var_name', 'var_trans'   (SpotOptim format), or a mapping of parameter names to (low, high) tuples.</p> required <code>metric</code> <code>str | Callable | list[str | Callable]</code> <p>Metric(s) to quantify model goodness of fit.</p> required <code>exog</code> <code>Series | DataFrame | None</code> <p>Exogenous variable(s) included as predictors. Default is None.</p> <code>None</code> <code>n_trials</code> <code>int</code> <p>Total number of evaluations (initial + sequential iterations). Default is 10.</p> <code>10</code> <code>n_initial</code> <code>int</code> <p>Number of initial random points to sample before starting sequential optimization. Default is 5.</p> <code>5</code> <code>random_state</code> <code>int</code> <p>Seed for sampling reproducibility. Default is 123.</p> <code>123</code> <code>return_best</code> <code>bool</code> <p>If True, refit the forecaster using the best parameters found on the whole dataset at the end. Default is True.</p> <code>True</code> <code>n_jobs</code> <code>int | str</code> <p>Number of parallel jobs. If -1, uses all cores. If 'auto', automatically determines the number of jobs. Default is 'auto'.</p> <code>'auto'</code> <code>verbose</code> <code>bool</code> <p>If True, print optimization progress. Default is False.</p> <code>False</code> <code>show_progress</code> <code>bool</code> <p>Whether to show progress (currently handled by verbose). Default is True.</p> <code>True</code> <code>suppress_warnings</code> <code>bool</code> <p>If True, suppress spotforecast warnings. Default is False.</p> <code>False</code> <code>output_file</code> <code>str | None</code> <p>Filename or full path to save results as TSV. Default is None.</p> <code>None</code> <code>kwargs_spotoptim</code> <code>dict</code> <p>Additional keyword arguments passed to SpotOptim(). Default is {}.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, object]</code> <p>tuple[pd.DataFrame, object]: A tuple containing: - results: DataFrame with columns 'lags', 'params', metric values,   and individual parameter columns. Sorted by the first metric. - optimizer: SpotOptim object containing the optimization results.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If exog length doesn't match y length when return_best=True.</p> <code>TypeError</code> <p>If cv is not an instance of TimeSeriesFold or OneStepAheadFold.</p> Source code in <code>src/spotforecast2/model_selection/spotoptim_search.py</code> <pre><code>def spotoptim_search_forecaster(\n    forecaster: object,\n    y: pd.Series,\n    cv: TimeSeriesFold | OneStepAheadFold,\n    search_space: ParameterSet | Dict[str, Any],\n    metric: str | Callable | list[str | Callable],\n    exog: pd.Series | pd.DataFrame | None = None,\n    n_trials: int = 10,\n    n_initial: int = 5,\n    random_state: int = 123,\n    return_best: bool = True,\n    n_jobs: int | str = \"auto\",\n    verbose: bool = False,\n    show_progress: bool = True,\n    suppress_warnings: bool = False,\n    output_file: str | None = None,\n    kwargs_spotoptim: dict = {},\n) -&gt; tuple[pd.DataFrame, object]:\n    \"\"\"\n    Hyperparameter optimization for a Forecaster using SpotOptim.\n\n    Performs hyperparameter search using the SpotOptim library for a\n    Forecaster object. Validation is done using time series backtesting with\n    the provided cross-validation strategy.\n\n    Args:\n        forecaster: Forecaster model. Can be ForecasterRecursive, ForecasterDirect,\n            or any compatible forecaster class.\n        y: Training time series values. Must be a pandas Series with a\n            datetime or numeric index.\n        cv: Cross-validation strategy with information needed to split the data\n            into folds. Must be an instance of TimeSeriesFold or OneStepAheadFold.\n        search_space: Hyperparameter search space. Can be either:\n            - ParameterSet: A ParameterSet object from spotoptim.hyperparameters\n              defining parameters with their types, bounds, and transformations.\n            - Dict: A dictionary with keys 'bounds', 'var_type', 'var_name', 'var_trans'\n              (SpotOptim format), or a mapping of parameter names to (low, high) tuples.\n        metric: Metric(s) to quantify model goodness of fit.\n        exog: Exogenous variable(s) included as predictors. Default is None.\n        n_trials: Total number of evaluations (initial + sequential iterations).\n            Default is 10.\n        n_initial: Number of initial random points to sample before starting\n            sequential optimization. Default is 5.\n        random_state: Seed for sampling reproducibility. Default is 123.\n        return_best: If True, refit the forecaster using the best parameters\n            found on the whole dataset at the end. Default is True.\n        n_jobs: Number of parallel jobs. If -1, uses all cores. If 'auto',\n            automatically determines the number of jobs. Default is 'auto'.\n        verbose: If True, print optimization progress. Default is False.\n        show_progress: Whether to show progress (currently handled by verbose).\n            Default is True.\n        suppress_warnings: If True, suppress spotforecast warnings. Default is False.\n        output_file: Filename or full path to save results as TSV. Default is None.\n        kwargs_spotoptim: Additional keyword arguments passed to SpotOptim().\n            Default is {}.\n\n    Returns:\n        tuple[pd.DataFrame, object]: A tuple containing:\n            - results: DataFrame with columns 'lags', 'params', metric values,\n              and individual parameter columns. Sorted by the first metric.\n            - optimizer: SpotOptim object containing the optimization results.\n\n    Raises:\n        ValueError: If exog length doesn't match y length when return_best=True.\n        TypeError: If cv is not an instance of TimeSeriesFold or OneStepAheadFold.\n    \"\"\"\n\n    if return_best and exog is not None and (len(exog) != len(y)):\n        raise ValueError(\n            f\"`exog` must have same number of samples as `y`. \"\n            f\"length `exog`: ({len(exog)}), length `y`: ({len(y)})\"\n        )\n\n    results, optimizer = _spotoptim_search(\n        forecaster=forecaster,\n        y=y,\n        cv=cv,\n        exog=exog,\n        search_space=search_space,\n        metric=metric,\n        n_trials=n_trials,\n        n_initial=n_initial,\n        random_state=random_state,\n        return_best=return_best,\n        n_jobs=n_jobs,\n        verbose=verbose,\n        suppress_warnings=suppress_warnings,\n        output_file=output_file,\n        kwargs_spotoptim=kwargs_spotoptim,\n    )\n\n    return results, optimizer\n</code></pre>"},{"location":"api/model_selection/#bayesian-search","title":"Bayesian Search","text":""},{"location":"api/model_selection/#spotforecast2.model_selection.bayesian_search","title":"<code>spotforecast2.model_selection.bayesian_search</code>","text":"<p>Bayesian hyperparameter search functions for forecasters using Optuna.</p>"},{"location":"api/model_selection/#spotforecast2.model_selection.bayesian_search.bayesian_search_forecaster","title":"<code>bayesian_search_forecaster(forecaster, y, cv, search_space, metric, exog=None, n_trials=10, random_state=123, return_best=True, n_jobs='auto', verbose=False, show_progress=True, suppress_warnings=False, output_file=None, kwargs_create_study={}, kwargs_study_optimize={})</code>","text":"<p>Bayesian hyperparameter optimization for a Forecaster using Optuna.</p> <p>Performs Bayesian hyperparameter search using the Optuna library for a Forecaster object. Validation is done using time series backtesting with the provided cross-validation strategy.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>object</code> <p>Forecaster model. Can be ForecasterRecursive, ForecasterDirect, or any compatible forecaster class.</p> required <code>y</code> <code>Series</code> <p>Training time series values. Must be a pandas Series with a datetime or numeric index.</p> required <code>cv</code> <code>TimeSeriesFold | OneStepAheadFold</code> <p>Cross-validation strategy with information needed to split the data into folds. Must be an instance of TimeSeriesFold or OneStepAheadFold.</p> required <code>search_space</code> <code>Callable</code> <p>Callable function with argument <code>trial</code> that returns a dictionary with parameter names (str) as keys and Trial objects from optuna (trial.suggest_float, trial.suggest_int, trial.suggest_categorical) as values. Can optionally include 'lags' key to search over different lag configurations.</p> required <code>metric</code> <code>str | Callable | list[str | Callable]</code> <p>Metric(s) to quantify model goodness of fit. Can be: - str: One of 'mean_squared_error', 'mean_absolute_error',   'mean_absolute_percentage_error', 'mean_squared_log_error',   'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'. - Callable: Function with arguments (y_true, y_pred) or   (y_true, y_pred, y_train) that returns a float. - list: List containing multiple strings and/or Callables.</p> required <code>exog</code> <code>Series | DataFrame | None</code> <p>Exogenous variable(s) included as predictors. Must have the same number of observations as <code>y</code> and aligned so that y[i] is regressed on exog[i]. Default is None.</p> <code>None</code> <code>n_trials</code> <code>int</code> <p>Number of parameter settings sampled during optimization. Default is 10.</p> <code>10</code> <code>random_state</code> <code>int</code> <p>Seed for sampling reproducibility. When passing a custom sampler in kwargs_create_study, set the seed within the sampler (e.g., {'sampler': TPESampler(seed=145)}). Default is 123.</p> <code>123</code> <code>return_best</code> <code>bool</code> <p>If True, refit the forecaster using the best parameters found on the whole dataset at the end. Default is True.</p> <code>True</code> <code>n_jobs</code> <code>int | str</code> <p>Number of parallel jobs. If -1, uses all cores. If 'auto', uses spotforecast.skforecast.utils.select_n_jobs_backtesting to automatically determine the number of jobs. Default is 'auto'.</p> <code>'auto'</code> <code>verbose</code> <code>bool</code> <p>If True, print number of folds used for cross-validation. Default is False.</p> <code>False</code> <code>show_progress</code> <code>bool</code> <p>Whether to show an Optuna progress bar during optimization. Default is True.</p> <code>True</code> <code>suppress_warnings</code> <code>bool</code> <p>If True, suppress spotforecast warnings during hyperparameter search. Default is False.</p> <code>False</code> <code>output_file</code> <code>str | None</code> <p>Filename or full path to save results as TSV. If None, results are not saved to file. Default is None.</p> <code>None</code> <code>kwargs_create_study</code> <code>dict</code> <p>Additional keyword arguments passed to optuna.create_study(). If not specified, direction is set to 'minimize' and TPESampler(seed=123) is used. Default is {}.</p> <code>{}</code> <code>kwargs_study_optimize</code> <code>dict</code> <p>Additional keyword arguments passed to study.optimize(). Default is {}.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, object]</code> <p>tuple[pd.DataFrame, object]: A tuple containing: - results: DataFrame with columns 'lags', 'params', metric values,   and individual parameter columns. Sorted by the first metric. - best_trial: Best optimization result as an optuna.FrozenTrial   object containing the best parameters and metric value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If exog length doesn't match y length when return_best=True.</p> <code>TypeError</code> <p>If cv is not an instance of TimeSeriesFold or OneStepAheadFold.</p> <code>ValueError</code> <p>If metric list contains duplicate metric names.</p> Source code in <code>src/spotforecast2/model_selection/bayesian_search.py</code> <pre><code>def bayesian_search_forecaster(\n    forecaster: object,\n    y: pd.Series,\n    cv: TimeSeriesFold | OneStepAheadFold,\n    search_space: Callable,\n    metric: str | Callable | list[str | Callable],\n    exog: pd.Series | pd.DataFrame | None = None,\n    n_trials: int = 10,\n    random_state: int = 123,\n    return_best: bool = True,\n    n_jobs: int | str = \"auto\",\n    verbose: bool = False,\n    show_progress: bool = True,\n    suppress_warnings: bool = False,\n    output_file: str | None = None,\n    kwargs_create_study: dict = {},\n    kwargs_study_optimize: dict = {},\n) -&gt; tuple[pd.DataFrame, object]:\n    \"\"\"\n    Bayesian hyperparameter optimization for a Forecaster using Optuna.\n\n    Performs Bayesian hyperparameter search using the Optuna library for a\n    Forecaster object. Validation is done using time series backtesting with\n    the provided cross-validation strategy.\n\n    Args:\n        forecaster: Forecaster model. Can be ForecasterRecursive, ForecasterDirect,\n            or any compatible forecaster class.\n        y: Training time series values. Must be a pandas Series with a\n            datetime or numeric index.\n        cv: Cross-validation strategy with information needed to split the data\n            into folds. Must be an instance of TimeSeriesFold or OneStepAheadFold.\n        search_space: Callable function with argument `trial` that returns\n            a dictionary with parameter names (str) as keys and Trial objects\n            from optuna (trial.suggest_float, trial.suggest_int,\n            trial.suggest_categorical) as values. Can optionally include 'lags'\n            key to search over different lag configurations.\n        metric: Metric(s) to quantify model goodness of fit. Can be:\n            - str: One of 'mean_squared_error', 'mean_absolute_error',\n              'mean_absolute_percentage_error', 'mean_squared_log_error',\n              'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'.\n            - Callable: Function with arguments (y_true, y_pred) or\n              (y_true, y_pred, y_train) that returns a float.\n            - list: List containing multiple strings and/or Callables.\n        exog: Exogenous variable(s) included as predictors. Must have the\n            same number of observations as `y` and aligned so that y[i] is\n            regressed on exog[i]. Default is None.\n        n_trials: Number of parameter settings sampled during optimization.\n            Default is 10.\n        random_state: Seed for sampling reproducibility. When passing a custom\n            sampler in kwargs_create_study, set the seed within the sampler\n            (e.g., {'sampler': TPESampler(seed=145)}). Default is 123.\n        return_best: If True, refit the forecaster using the best parameters\n            found on the whole dataset at the end. Default is True.\n        n_jobs: Number of parallel jobs. If -1, uses all cores. If 'auto',\n            uses spotforecast.skforecast.utils.select_n_jobs_backtesting to\n            automatically determine the number of jobs. Default is 'auto'.\n        verbose: If True, print number of folds used for cross-validation.\n            Default is False.\n        show_progress: Whether to show an Optuna progress bar during\n            optimization. Default is True.\n        suppress_warnings: If True, suppress spotforecast warnings during\n            hyperparameter search. Default is False.\n        output_file: Filename or full path to save results as TSV. If None,\n            results are not saved to file. Default is None.\n        kwargs_create_study: Additional keyword arguments passed to\n            optuna.create_study(). If not specified, direction is set to\n            'minimize' and TPESampler(seed=123) is used. Default is {}.\n        kwargs_study_optimize: Additional keyword arguments passed to\n            study.optimize(). Default is {}.\n\n    Returns:\n        tuple[pd.DataFrame, object]: A tuple containing:\n            - results: DataFrame with columns 'lags', 'params', metric values,\n              and individual parameter columns. Sorted by the first metric.\n            - best_trial: Best optimization result as an optuna.FrozenTrial\n              object containing the best parameters and metric value.\n\n    Raises:\n        ValueError: If exog length doesn't match y length when return_best=True.\n        TypeError: If cv is not an instance of TimeSeriesFold or OneStepAheadFold.\n        ValueError: If metric list contains duplicate metric names.\n\n    \"\"\"\n\n    if return_best and exog is not None and (len(exog) != len(y)):\n        raise ValueError(\n            f\"`exog` must have same number of samples as `y`. \"\n            f\"length `exog`: ({len(exog)}), length `y`: ({len(y)})\"\n        )\n\n    results, best_trial = _bayesian_search_optuna(\n        forecaster=forecaster,\n        y=y,\n        cv=cv,\n        exog=exog,\n        search_space=search_space,\n        metric=metric,\n        n_trials=n_trials,\n        random_state=random_state,\n        return_best=return_best,\n        n_jobs=n_jobs,\n        verbose=verbose,\n        show_progress=show_progress,\n        suppress_warnings=suppress_warnings,\n        output_file=output_file,\n        kwargs_create_study=kwargs_create_study,\n        kwargs_study_optimize=kwargs_study_optimize,\n    )\n\n    return results, best_trial\n</code></pre>"},{"location":"api/model_selection/#random-search","title":"Random Search","text":""},{"location":"api/model_selection/#spotforecast2.model_selection.random_search","title":"<code>spotforecast2.model_selection.random_search</code>","text":"<p>Random search hyperparameter optimization for forecasters.</p>"},{"location":"api/model_selection/#spotforecast2.model_selection.random_search.random_search_forecaster","title":"<code>random_search_forecaster(forecaster, y, cv, param_distributions, metric, exog=None, lags_grid=None, n_iter=10, random_state=123, return_best=True, n_jobs='auto', verbose=False, show_progress=True, suppress_warnings=False, output_file=None)</code>","text":"<p>Random search over parameter distributions for a Forecaster.</p> <p>Performs random sampling of parameter settings from distributions for a Forecaster object. Validation is done using time series backtesting with the provided cross-validation strategy. This is more efficient than grid search when exploring large parameter spaces.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>object</code> <p>Forecaster model (ForecasterRecursive or ForecasterDirect).</p> required <code>y</code> <code>Series</code> <p>Training time series.</p> required <code>cv</code> <code>TimeSeriesFold | OneStepAheadFold</code> <p>Cross-validation strategy (TimeSeriesFold or OneStepAheadFold) with information needed to split the data into folds.</p> required <code>param_distributions</code> <code>dict</code> <p>Dictionary with parameter names (str) as keys and distributions or lists of parameters to try as values. Use scipy.stats distributions for continuous parameters.</p> required <code>metric</code> <code>str | Callable | list[str | Callable]</code> <p>Metric(s) to quantify model goodness of fit. If str: 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error', 'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'. If Callable: Function with arguments (y_true, y_pred, y_train) that returns a float. If list: Multiple strings and/or Callables.</p> required <code>exog</code> <code>Series | DataFrame | None</code> <p>Exogenous variable(s) included as predictors. Must have the same number of observations as y and aligned so that y[i] is regressed on exog[i]. Default is None.</p> <code>None</code> <code>lags_grid</code> <code>list[int | list[int] | ndarray[int] | range[int]] | dict[str, list[int | list[int] | ndarray[int] | range[int]]] | None</code> <p>Lists of lags to try. Can be int, lists, numpy ndarray, or range objects. If dict, keys are used as labels in results DataFrame. Default is None.</p> <code>None</code> <code>n_iter</code> <code>int</code> <p>Number of parameter settings sampled per lags configuration. Trades off runtime vs solution quality. Default is 10.</p> <code>10</code> <code>random_state</code> <code>int</code> <p>Seed for random sampling for reproducible output. Default is 123.</p> <code>123</code> <code>return_best</code> <code>bool</code> <p>If True, refit the forecaster using best parameters on the whole dataset. Default is True.</p> <code>True</code> <code>n_jobs</code> <code>int | str</code> <p>Number of jobs to run in parallel. If -1, uses all cores. If 'auto', uses select_n_jobs_backtesting. Default is 'auto'.</p> <code>'auto'</code> <code>verbose</code> <code>bool</code> <p>If True, print number of folds used for cv. Default is False.</p> <code>False</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar. Default is True.</p> <code>True</code> <code>suppress_warnings</code> <code>bool</code> <p>If True, suppress spotforecast warnings during hyperparameter search. Default is False.</p> <code>False</code> <code>output_file</code> <code>str | None</code> <p>Filename or full path to save results as TSV. If None, results are not saved to file. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Results for each parameter combination with columns: lags (lags</p> <code>DataFrame</code> <p>configuration), lags_label (descriptive label), params (parameters</p> <code>DataFrame</code> <p>configuration), metric (metric value), and additional columns with</p> <code>DataFrame</code> <p>param=value pairs.</p> <p>Examples:</p> <p>Basic random search with continuous parameter distributions:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; from scipy.stats import uniform\n&gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n&gt;&gt;&gt; from spotforecast2.model_selection import TimeSeriesFold\n&gt;&gt;&gt; from spotforecast2.model_selection.random_search import random_search_forecaster\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; np.random.seed(123)\n&gt;&gt;&gt; y = pd.Series(np.random.randn(50), name='y')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Set up forecaster and cross-validation\n&gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n&gt;&gt;&gt; cv = TimeSeriesFold(steps=3, initial_train_size=20, refit=False)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Define parameter distributions with scipy.stats\n&gt;&gt;&gt; param_distributions = {\n...     'estimator__alpha': uniform(0.1, 10.0)  # Uniform between 0.1 and 10.1\n... }\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Run random search\n&gt;&gt;&gt; results = random_search_forecaster(\n...     forecaster=forecaster,\n...     y=y,\n...     cv=cv,\n...     param_distributions=param_distributions,\n...     metric='mean_squared_error',\n...     n_iter=5,\n...     random_state=42,\n...     return_best=False,\n...     verbose=False,\n...     show_progress=False\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check results\n&gt;&gt;&gt; print(results.shape[0])\n5\n&gt;&gt;&gt; print('estimator__alpha' in results.columns)\nTrue\n&gt;&gt;&gt; print('mean_squared_error' in results.columns)\nTrue\n</code></pre> Source code in <code>src/spotforecast2/model_selection/random_search.py</code> <pre><code>def random_search_forecaster(\n    forecaster: object,\n    y: pd.Series,\n    cv: TimeSeriesFold | OneStepAheadFold,\n    param_distributions: dict,\n    metric: str | Callable | list[str | Callable],\n    exog: pd.Series | pd.DataFrame | None = None,\n    lags_grid: (\n        list[int | list[int] | np.ndarray[int] | range[int]]\n        | dict[str, list[int | list[int] | np.ndarray[int] | range[int]]]\n        | None\n    ) = None,\n    n_iter: int = 10,\n    random_state: int = 123,\n    return_best: bool = True,\n    n_jobs: int | str = \"auto\",\n    verbose: bool = False,\n    show_progress: bool = True,\n    suppress_warnings: bool = False,\n    output_file: str | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Random search over parameter distributions for a Forecaster.\n\n    Performs random sampling of parameter settings from distributions for a\n    Forecaster object. Validation is done using time series backtesting with\n    the provided cross-validation strategy. This is more efficient than grid\n    search when exploring large parameter spaces.\n\n    Args:\n        forecaster: Forecaster model (ForecasterRecursive or ForecasterDirect).\n        y: Training time series.\n        cv: Cross-validation strategy (TimeSeriesFold or OneStepAheadFold)\n            with information needed to split the data into folds.\n        param_distributions: Dictionary with parameter names (str) as keys\n            and distributions or lists of parameters to try as values.\n            Use scipy.stats distributions for continuous parameters.\n        metric: Metric(s) to quantify model goodness of fit. If str:\n            'mean_squared_error', 'mean_absolute_error',\n            'mean_absolute_percentage_error', 'mean_squared_log_error',\n            'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'.\n            If Callable: Function with arguments (y_true, y_pred, y_train)\n            that returns a float. If list: Multiple strings and/or Callables.\n        exog: Exogenous variable(s) included as predictors. Must have the\n            same number of observations as y and aligned so that y[i] is\n            regressed on exog[i]. Default is None.\n        lags_grid: Lists of lags to try. Can be int, lists, numpy ndarray,\n            or range objects. If dict, keys are used as labels in results\n            DataFrame. Default is None.\n        n_iter: Number of parameter settings sampled per lags configuration.\n            Trades off runtime vs solution quality. Default is 10.\n        random_state: Seed for random sampling for reproducible output.\n            Default is 123.\n        return_best: If True, refit the forecaster using best parameters\n            on the whole dataset. Default is True.\n        n_jobs: Number of jobs to run in parallel. If -1, uses all cores.\n            If 'auto', uses select_n_jobs_backtesting. Default is 'auto'.\n        verbose: If True, print number of folds used for cv. Default is False.\n        show_progress: Whether to show a progress bar. Default is True.\n        suppress_warnings: If True, suppress spotforecast warnings during\n            hyperparameter search. Default is False.\n        output_file: Filename or full path to save results as TSV. If None,\n            results are not saved to file. Default is None.\n\n    Returns:\n        Results for each parameter combination with columns: lags (lags\n        configuration), lags_label (descriptive label), params (parameters\n        configuration), metric (metric value), and additional columns with\n        param=value pairs.\n\n    Examples:\n        Basic random search with continuous parameter distributions:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; from scipy.stats import uniform\n        &gt;&gt;&gt; from spotforecast2_safe.forecaster.recursive import ForecasterRecursive\n        &gt;&gt;&gt; from spotforecast2.model_selection import TimeSeriesFold\n        &gt;&gt;&gt; from spotforecast2.model_selection.random_search import random_search_forecaster\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample data\n        &gt;&gt;&gt; np.random.seed(123)\n        &gt;&gt;&gt; y = pd.Series(np.random.randn(50), name='y')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Set up forecaster and cross-validation\n        &gt;&gt;&gt; forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)\n        &gt;&gt;&gt; cv = TimeSeriesFold(steps=3, initial_train_size=20, refit=False)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Define parameter distributions with scipy.stats\n        &gt;&gt;&gt; param_distributions = {\n        ...     'estimator__alpha': uniform(0.1, 10.0)  # Uniform between 0.1 and 10.1\n        ... }\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Run random search\n        &gt;&gt;&gt; results = random_search_forecaster(\n        ...     forecaster=forecaster,\n        ...     y=y,\n        ...     cv=cv,\n        ...     param_distributions=param_distributions,\n        ...     metric='mean_squared_error',\n        ...     n_iter=5,\n        ...     random_state=42,\n        ...     return_best=False,\n        ...     verbose=False,\n        ...     show_progress=False\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check results\n        &gt;&gt;&gt; print(results.shape[0])\n        5\n        &gt;&gt;&gt; print('estimator__alpha' in results.columns)\n        True\n        &gt;&gt;&gt; print('mean_squared_error' in results.columns)\n        True\n    \"\"\"\n\n    param_grid = list(\n        ParameterSampler(param_distributions, n_iter=n_iter, random_state=random_state)\n    )\n\n    results = _evaluate_grid_hyperparameters(\n        forecaster=forecaster,\n        y=y,\n        cv=cv,\n        param_grid=param_grid,\n        metric=metric,\n        exog=exog,\n        lags_grid=lags_grid,\n        return_best=return_best,\n        n_jobs=n_jobs,\n        verbose=verbose,\n        show_progress=show_progress,\n        suppress_warnings=suppress_warnings,\n        output_file=output_file,\n    )\n\n    return results\n</code></pre>"},{"location":"api/preprocessing/","title":"Preprocessing Module","text":""},{"location":"api/preprocessing/#curate-data","title":"Curate Data","text":""},{"location":"api/preprocessing/#spotforecast2.preprocessing.curate_data.get_start_end","title":"<code>spotforecast2.preprocessing.curate_data.get_start_end(data, forecast_horizon, verbose=True)</code>","text":"<p>Get start and end date strings for data and covariate ranges. Covariate range is extended by the forecast horizon.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The dataset with a datetime index.</p> required <code>forecast_horizon</code> <code>int</code> <p>The forecast horizon in hours.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print the determined date ranges.</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[str, str, str, str]</code> <p>tuple[str, str, str, str]: (data_start, data_end, covariate_start, covariate_end) Date strings in the format \"YYYY-MM-DDTHH:MM\" for data and covariate ranges.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; date_rng = pd.date_range(start='2023-01-01', end='2023-01-10', freq='h')\n&gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=['date'])\n&gt;&gt;&gt; data.set_index('date', inplace=True)\n&gt;&gt;&gt; start, end, cov_start, cov_end = get_start_end(data, forecast_horizon=24, verbose=False)\n&gt;&gt;&gt; print(start, end, cov_start, cov_end)\n2023-01-01T00:00 2023-01-10T00:00 2023-01-01T00:00 2023-01-11T00:00\n</code></pre> Source code in <code>src/spotforecast2/preprocessing/curate_data.py</code> <pre><code>def get_start_end(\n    data: pd.DataFrame,\n    forecast_horizon: int,\n    verbose: bool = True,\n) -&gt; tuple[str, str, str, str]:\n    \"\"\"Get start and end date strings for data and covariate ranges.\n    Covariate range is extended by the forecast horizon.\n\n    Args:\n        data (pd.DataFrame):\n            The dataset with a datetime index.\n        forecast_horizon (int):\n            The forecast horizon in hours.\n        verbose (bool):\n            Whether to print the determined date ranges.\n\n    Returns:\n        tuple[str, str, str, str]: (data_start, data_end, covariate_start, covariate_end)\n            Date strings in the format \"YYYY-MM-DDTHH:MM\" for data and covariate ranges.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; date_rng = pd.date_range(start='2023-01-01', end='2023-01-10', freq='h')\n        &gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=['date'])\n        &gt;&gt;&gt; data.set_index('date', inplace=True)\n        &gt;&gt;&gt; start, end, cov_start, cov_end = get_start_end(data, forecast_horizon=24, verbose=False)\n        &gt;&gt;&gt; print(start, end, cov_start, cov_end)\n        2023-01-01T00:00 2023-01-10T00:00 2023-01-01T00:00 2023-01-11T00:00\n    \"\"\"\n    FORECAST_HORIZON = forecast_horizon\n\n    START = data.index.min().strftime(\"%Y-%m-%dT%H:%M\")\n    END = data.index.max().strftime(\"%Y-%m-%dT%H:%M\")\n    if verbose:\n        print(f\"Data range: {START} to {END}\")\n    # Define covariate range relative to data range\n    COV_START = START\n    # Extend end date by forecast horizon to include future covariates\n    COV_END = (pd.to_datetime(END) + pd.Timedelta(hours=FORECAST_HORIZON)).strftime(\n        \"%Y-%m-%dT%H:%M\"\n    )\n    if verbose:\n        print(f\"Covariate data range: {COV_START} to {COV_END}\")\n    return START, END, COV_START, COV_END\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2.preprocessing.curate_data.curate_holidays","title":"<code>spotforecast2.preprocessing.curate_data.curate_holidays(holiday_df, data, forecast_horizon)</code>","text":"<p>Checks if the holiday dataframe has the correct shape. Args:     holiday_df (pd.DataFrame):         DataFrame containing holiday information.     data (pd.DataFrame):         The main dataset.     forecast_horizon (int):         The forecast horizon in hours.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data, fetch_holiday_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end, curate_holidays\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n...     data=data,\n...     forecast_horizon=24,\n...     verbose=False\n... )\n&gt;&gt;&gt; holiday_df = fetch_holiday_data(\n...     start='2023-01-01T00:00',\n...     end='2023-01-10T00:00',\n...     tz='UTC',\n...     freq='h',\n...     country_code='DE',\n...     state='NW'\n... )\n&gt;&gt;&gt; FORECAST_HORIZON = 24\n&gt;&gt;&gt; curate_holidays(holiday_df, data, forecast_horizon=FORECAST_HORIZON)\n</code></pre> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the holiday dataframe does not have the correct number of rows.</p> Source code in <code>src/spotforecast2/preprocessing/curate_data.py</code> <pre><code>def curate_holidays(\n    holiday_df: pd.DataFrame, data: pd.DataFrame, forecast_horizon: int\n):\n    \"\"\"Checks if the holiday dataframe has the correct shape.\n    Args:\n        holiday_df (pd.DataFrame):\n            DataFrame containing holiday information.\n        data (pd.DataFrame):\n            The main dataset.\n        forecast_horizon (int):\n            The forecast horizon in hours.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data, fetch_holiday_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end, curate_holidays\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n        ...     data=data,\n        ...     forecast_horizon=24,\n        ...     verbose=False\n        ... )\n        &gt;&gt;&gt; holiday_df = fetch_holiday_data(\n        ...     start='2023-01-01T00:00',\n        ...     end='2023-01-10T00:00',\n        ...     tz='UTC',\n        ...     freq='h',\n        ...     country_code='DE',\n        ...     state='NW'\n        ... )\n        &gt;&gt;&gt; FORECAST_HORIZON = 24\n        &gt;&gt;&gt; curate_holidays(holiday_df, data, forecast_horizon=FORECAST_HORIZON)\n\n    Raises:\n        AssertionError:\n            If the holiday dataframe does not have the correct number of rows.\n    \"\"\"\n    try:\n        assert holiday_df.shape[0] == data.shape[0] + forecast_horizon\n        print(\"Holiday dataframe has correct shape.\")\n    except AssertionError:\n        print(\"Holiday dataframe has wrong shape.\")\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2.preprocessing.curate_data.curate_weather","title":"<code>spotforecast2.preprocessing.curate_data.curate_weather(weather_df, data, forecast_horizon)</code>","text":"<p>Checks if the weather dataframe has the correct shape.</p> <p>Parameters:</p> Name Type Description Default <code>weather_df</code> <code>DataFrame</code> <p>DataFrame containing weather information.</p> required <code>data</code> <code>DataFrame</code> <p>The main dataset.</p> required <code>forecast_horizon</code> <code>int</code> <p>The forecast horizon in hours.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data, fetch_weather_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end, curate_weather\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n...     data=data,\n...     forecast_horizon=24,\n...     verbose=False\n... )\n&gt;&gt;&gt; weather_df = fetch_weather_data(\n...     cov_start=COV_START,\n...     cov_end=COV_END,\n...     tz='UTC',\n...     freq='h',\n...     latitude=51.5136,\n...     longitude=7.4653\n... )\n&gt;&gt;&gt; FORECAST_HORIZON = 24\n&gt;&gt;&gt; curate_weather(weather_df, data, forecast_horizon=FORECAST_HORIZON)\n</code></pre> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the weather dataframe does not have the correct number of rows.</p> Source code in <code>src/spotforecast2/preprocessing/curate_data.py</code> <pre><code>def curate_weather(weather_df: pd.DataFrame, data: pd.DataFrame, forecast_horizon: int):\n    \"\"\"Checks if the weather dataframe has the correct shape.\n\n    Args:\n        weather_df (pd.DataFrame):\n            DataFrame containing weather information.\n        data (pd.DataFrame):\n            The main dataset.\n        forecast_horizon (int):\n            The forecast horizon in hours.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data, fetch_weather_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import get_start_end, curate_weather\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; START, END, COV_START, COV_END = get_start_end(\n        ...     data=data,\n        ...     forecast_horizon=24,\n        ...     verbose=False\n        ... )\n        &gt;&gt;&gt; weather_df = fetch_weather_data(\n        ...     cov_start=COV_START,\n        ...     cov_end=COV_END,\n        ...     tz='UTC',\n        ...     freq='h',\n        ...     latitude=51.5136,\n        ...     longitude=7.4653\n        ... )\n        &gt;&gt;&gt; FORECAST_HORIZON = 24\n        &gt;&gt;&gt; curate_weather(weather_df, data, forecast_horizon=FORECAST_HORIZON)\n\n    Raises:\n        AssertionError:\n            If the weather dataframe does not have the correct number of rows.\n    \"\"\"\n    try:\n        assert weather_df.shape[0] == data.shape[0] + forecast_horizon\n        print(\"Weather dataframe has correct shape.\")\n    except AssertionError:\n        print(\"Weather dataframe has wrong shape.\")\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2.preprocessing.curate_data.basic_ts_checks","title":"<code>spotforecast2.preprocessing.curate_data.basic_ts_checks(data, verbose=False)</code>","text":"<p>Checks if the time series data has a datetime index and is sorted.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The main dataset.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import basic_ts_checks\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; basic_ts_checks(data)\n</code></pre> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the index is not a datetime index.</p> <code>ValueError</code> <p>If the datetime index is not sorted in increasing order or is incomplete.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the datetime index is valid, sorted, and complete.</p> Source code in <code>src/spotforecast2/preprocessing/curate_data.py</code> <pre><code>def basic_ts_checks(data: pd.DataFrame, verbose: bool = False) -&gt; bool:\n    \"\"\"Checks if the time series data has a datetime index and is sorted.\n\n    Args:\n        data (pd.DataFrame):\n            The main dataset.\n        verbose (bool):\n            Whether to print additional information.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import basic_ts_checks\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; basic_ts_checks(data)\n\n    Raises:\n        TypeError:\n            If the index is not a datetime index.\n        ValueError:\n            If the datetime index is not sorted in increasing order or is incomplete.\n\n    Returns:\n        bool: True if the datetime index is valid, sorted, and complete.\n    \"\"\"\n    # Check if the time series data has a datetime index\n    if not pd.api.types.is_datetime64_any_dtype(data.index):\n        raise TypeError(\"The index is not a datetime index.\")\n\n    # Check if the datetime index is sorted\n    if not data.index.is_monotonic_increasing:\n        raise ValueError(\"The datetime index is not sorted in increasing order.\")\n\n    # Check if the index is complete (no missing timestamps)\n    start_date = data.index.min()\n    end_date = data.index.max()\n    complete_date_range = pd.date_range(\n        start=start_date, end=end_date, freq=data.index.freq\n    )\n    is_index_complete = (data.index == complete_date_range).all()\n\n    if not is_index_complete:\n        raise ValueError(\n            \"The datetime index has missing timestamps and is not complete.\"\n        )\n    if verbose:\n        print(\n            \"The time series data has a valid datetime index that is sorted and complete.\"\n        )\n    return True\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2.preprocessing.curate_data.agg_and_resample_data","title":"<code>spotforecast2.preprocessing.curate_data.agg_and_resample_data(data, rule='h', closed='left', label='left', by='mean', verbose=False)</code>","text":"<p>Aggregates and resamples the data to (e.g.,hourly) frequency by computing the specified aggregation (e.g. for each hour).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The dataset with a datetime index.</p> required <code>rule</code> <code>str</code> <p>The resample rule (e.g., 'h' for hourly, 'D' for daily). Default is 'h' which creates an hourly grid.</p> <code>'h'</code> <code>closed</code> <code>str</code> <p>Which side of bin interval is closed. Default is 'left'. Using <code>closed=\"left\", label=\"left\"</code> specifies that a time interval (e.g., 10:00 to 11:00) is labeled with the start timestamp (10:00). For consumption data, a different representation is usually more common: <code>closed=\"left\", label=\"right\"</code>, so the interval is labeled with the end timestamp (11:00), since consumption is typically reported after one hour.</p> <code>'left'</code> <code>label</code> <code>str</code> <p>Which bin edge label to use. Default is 'left'. See 'closed' parameter for details on labeling behavior.</p> <code>'left'</code> <code>by</code> <code>str or callable</code> <p>Aggregation method to apply (e.g., 'mean', 'sum', 'median'). Default is 'mean'. The aggregation serves robustness: if the data were more finely resolved (e.g., quarter-hourly), asfreq would only pick one value (sampling), while .agg(\"mean\") forms the correct average over the hour. If the data is already hourly, .agg doesn't change anything but ensures that no duplicates exist.</p> <code>'mean'</code> <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Resampled and aggregated dataframe.</p> Notes <ul> <li>resample(rule=\"h\"): Creates an hourly grid</li> <li>closed/label: Control how time intervals are labeled</li> <li>.agg({...: by}): Aggregates values within each time bin</li> </ul> <p>Examples::     &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import agg_and_resample_data     &gt;&gt;&gt; import pandas as pd     &gt;&gt;&gt; date_rng = pd.date_range(start='2023-01-01', end='2023-01-02', freq='15T')     &gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=['date'])     &gt;&gt;&gt; data.set_index('date', inplace=True)     &gt;&gt;&gt; data['value'] = range(len(data))     &gt;&gt;&gt; resampled_data = agg_and_resample_data(data, rule='h', by='mean')     &gt;&gt;&gt; print(resampled_data.head())</p> Source code in <code>src/spotforecast2/preprocessing/curate_data.py</code> <pre><code>def agg_and_resample_data(\n    data: pd.DataFrame,\n    rule: str = \"h\",\n    closed: str = \"left\",\n    label: str = \"left\",\n    by=\"mean\",\n    verbose: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Aggregates and resamples the data to (e.g.,hourly) frequency by computing the specified aggregation (e.g. for each hour).\n\n    Args:\n        data (pd.DataFrame):\n            The dataset with a datetime index.\n        rule (str):\n            The resample rule (e.g., 'h' for hourly, 'D' for daily).\n            Default is 'h' which creates an hourly grid.\n        closed (str):\n            Which side of bin interval is closed. Default is 'left'.\n            Using `closed=\"left\", label=\"left\"` specifies that a time interval\n            (e.g., 10:00 to 11:00) is labeled with the start timestamp (10:00).\n            For consumption data, a different representation is usually more common:\n            `closed=\"left\", label=\"right\"`, so the interval is labeled with the end\n            timestamp (11:00), since consumption is typically reported after one hour.\n        label (str):\n            Which bin edge label to use. Default is 'left'.\n            See 'closed' parameter for details on labeling behavior.\n        by (str or callable):\n            Aggregation method to apply (e.g., 'mean', 'sum', 'median').\n            Default is 'mean'.\n            The aggregation serves robustness: if the data were more finely resolved\n            (e.g., quarter-hourly), asfreq would only pick one value (sampling),\n            while .agg(\"mean\") forms the correct average over the hour.\n            If the data is already hourly, .agg doesn't change anything but ensures\n            that no duplicates exist.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        pd.DataFrame: Resampled and aggregated dataframe.\n\n    Notes:\n        - resample(rule=\"h\"): Creates an hourly grid\n        - closed/label: Control how time intervals are labeled\n        - .agg({...: by}): Aggregates values within each time bin\n\n    Examples::\n        &gt;&gt;&gt; from spotforecast2.preprocessing.curate_data import agg_and_resample_data\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; date_rng = pd.date_range(start='2023-01-01', end='2023-01-02', freq='15T')\n        &gt;&gt;&gt; data = pd.DataFrame(date_rng, columns=['date'])\n        &gt;&gt;&gt; data.set_index('date', inplace=True)\n        &gt;&gt;&gt; data['value'] = range(len(data))\n        &gt;&gt;&gt; resampled_data = agg_and_resample_data(data, rule='h', by='mean')\n        &gt;&gt;&gt; print(resampled_data.head())\n    \"\"\"\n    if verbose:\n        print(f\"Original data shape: {data.shape}\")\n    # Create aggregation dictionary for all columns\n    agg_dict = {col: by for col in data.columns}\n\n    data = data.resample(rule=rule, closed=closed, label=label).agg(agg_dict)\n    if verbose:\n        print(\n            f\"Data resampled with rule='{rule}', closed='{closed}', label='{label}', aggregation='{by}'.\"\n        )\n        print(f\"Resampled data shape: {data.shape}\")\n    return data\n</code></pre>"},{"location":"api/preprocessing/#outlier-detection","title":"Outlier Detection","text":""},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.outlier.mark_outliers","title":"<code>spotforecast2_safe.preprocessing.outlier.mark_outliers(data, contamination=0.1, random_state=1234, verbose=False)</code>","text":"<p>Marks outliers as NaN in the dataset using Isolation Forest.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input dataset.</p> required <code>contamination</code> <code>float</code> <p>The (estimated) proportion of outliers in the dataset.</p> <code>0.1</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default is 1234.</p> <code>1234</code> <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, ndarray]</code> <p>tuple[pd.DataFrame, np.ndarray]: A tuple containing the modified dataset with outliers marked as NaN and the outlier labels.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.outlier import mark_outliers\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; cleaned_data, outlier_labels = mark_outliers(data, contamination=0.1, random_state=42, verbose=True)\n</code></pre> Source code in <code>spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def mark_outliers(\n    data: pd.DataFrame,\n    contamination: float = 0.1,\n    random_state: int = 1234,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, np.ndarray]:\n    \"\"\"Marks outliers as NaN in the dataset using Isolation Forest.\n\n    Args:\n        data (pd.DataFrame):\n            The input dataset.\n        contamination (float):\n            The (estimated) proportion of outliers in the dataset.\n        random_state (int):\n            Random seed for reproducibility. Default is 1234.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        tuple[pd.DataFrame, np.ndarray]: A tuple containing the modified dataset with outliers marked as NaN and the outlier labels.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.outlier import mark_outliers\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; cleaned_data, outlier_labels = mark_outliers(data, contamination=0.1, random_state=42, verbose=True)\n    \"\"\"\n    for col in data.columns:\n        iso = IsolationForest(contamination=contamination, random_state=random_state)\n        # Fit and predict (-1 for outliers, 1 for inliers)\n        outliers = iso.fit_predict(data[[col]])\n\n        # Mark outliers as NaN\n        data.loc[outliers == -1, col] = np.nan\n\n        pct_outliers = (outliers == -1).mean() * 100\n        if verbose:\n            print(\n                f\"Column '{col}': Marked {pct_outliers:.4f}% of data points as outliers.\"\n            )\n    return data, outliers\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.outlier.manual_outlier_removal","title":"<code>spotforecast2_safe.preprocessing.outlier.manual_outlier_removal(data, column, lower_threshold=None, upper_threshold=None, verbose=False)</code>","text":"<p>Manual outlier removal function. Args:     data (pd.DataFrame):         The input dataset.     column (str):         The column name in which to perform manual outlier removal.     lower_threshold (float | None):         The lower threshold below which values are considered outliers.         If None, no lower threshold is applied.     upper_threshold (float | None):         The upper threshold above which values are considered outliers.         If None, no upper threshold is applied.     verbose (bool):         Whether to print additional information.</p> <p>Returns:</p> Type Description <code>tuple[DataFrame, int]</code> <p>tuple[pd.DataFrame, int]: A tuple containing the modified dataset with outliers marked as NaN and the number of outliers marked.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.outlier import manual_outlier_removal\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; data, n_manual_outliers = manual_outlier_removal(\n...     data,\n...     column='ABC',\n...     lower_threshold=50,\n...     upper_threshold=700,\n...     verbose=True\n</code></pre> Source code in <code>spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def manual_outlier_removal(\n    data: pd.DataFrame,\n    column: str,\n    lower_threshold: float | None = None,\n    upper_threshold: float | None = None,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, int]:\n    \"\"\"Manual outlier removal function.\n    Args:\n        data (pd.DataFrame):\n            The input dataset.\n        column (str):\n            The column name in which to perform manual outlier removal.\n        lower_threshold (float | None):\n            The lower threshold below which values are considered outliers.\n            If None, no lower threshold is applied.\n        upper_threshold (float | None):\n            The upper threshold above which values are considered outliers.\n            If None, no upper threshold is applied.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        tuple[pd.DataFrame, int]: A tuple containing the modified dataset with outliers marked as NaN and the number of outliers marked.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.outlier import manual_outlier_removal\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; data, n_manual_outliers = manual_outlier_removal(\n        ...     data,\n        ...     column='ABC',\n        ...     lower_threshold=50,\n        ...     upper_threshold=700,\n        ...     verbose=True\n    \"\"\"\n    if lower_threshold is None and upper_threshold is None:\n        if verbose:\n            print(f\"No thresholds provided for {column}; no outliers marked.\")\n        return data, 0\n\n    if lower_threshold is not None and upper_threshold is not None:\n        mask = (data[column] &gt; upper_threshold) | (data[column] &lt; lower_threshold)\n    elif lower_threshold is not None:\n        mask = data[column] &lt; lower_threshold\n    else:\n        mask = data[column] &gt; upper_threshold\n\n    n_manual_outliers = mask.sum()\n\n    data.loc[mask, column] = np.nan\n\n    if verbose:\n        if lower_threshold is not None and upper_threshold is not None:\n            print(\n                f\"Manually marked {n_manual_outliers} values &gt; {upper_threshold} or &lt; {lower_threshold} as outliers in {column}.\"\n            )\n        elif lower_threshold is not None:\n            print(\n                f\"Manually marked {n_manual_outliers} values &lt; {lower_threshold} as outliers in {column}.\"\n            )\n        else:\n            print(\n                f\"Manually marked {n_manual_outliers} values &gt; {upper_threshold} as outliers in {column}.\"\n            )\n    return data, n_manual_outliers\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2_safe.preprocessing.outlier.get_outliers","title":"<code>spotforecast2_safe.preprocessing.outlier.get_outliers(data, data_original=None, contamination=0.01, random_state=1234)</code>","text":"<p>Detect outliers in each column using Isolation Forest.</p> <p>This function uses scikit-learn's IsolationForest algorithm to detect outliers in each column of the input DataFrame. The original data (before any NaN values were introduced) can be provided to identify which values were marked as NaN due to outlier detection.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input DataFrame to check for outliers.</p> required <code>data_original</code> <code>Optional[DataFrame]</code> <p>Optional original DataFrame before outlier marking. If provided, helps identify which values became NaN due to outlier detection. Default: None.</p> <code>None</code> <code>contamination</code> <code>float</code> <p>The estimated proportion of outliers in the dataset. Default: 0.01.</p> <code>0.01</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default: 1234.</p> <code>1234</code> <p>Returns:</p> Type Description <code>Dict[str, Series]</code> <p>A dictionary mapping column names to Series of outlier values.</p> <code>Dict[str, Series]</code> <p>For columns without outliers, an empty Series is returned.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data is empty or contains no columns.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2_safe.preprocessing.outlier import get_outliers\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample data with outliers\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; data = pd.DataFrame({\n...     'A': np.concatenate([np.random.normal(0, 1, 100), [10, 11, 12]]),\n...     'B': np.concatenate([np.random.normal(5, 2, 100), [100, 110, 120]])\n... })\n&gt;&gt;&gt; data_original = data.copy()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Detect outliers\n&gt;&gt;&gt; outliers = get_outliers(data_original, contamination=0.03)\n&gt;&gt;&gt; for col, outlier_vals in outliers.items():\n...     print(f\"{col}: {len(outlier_vals)} outliers detected\")\n</code></pre> Source code in <code>spotforecast2_safe/preprocessing/outlier.py</code> <pre><code>def get_outliers(\n    data: pd.DataFrame,\n    data_original: Optional[pd.DataFrame] = None,\n    contamination: float = 0.01,\n    random_state: int = 1234,\n) -&gt; Dict[str, pd.Series]:\n    \"\"\"Detect outliers in each column using Isolation Forest.\n\n    This function uses scikit-learn's IsolationForest algorithm to detect outliers\n    in each column of the input DataFrame. The original data (before any NaN values\n    were introduced) can be provided to identify which values were marked as NaN due\n    to outlier detection.\n\n    Args:\n        data: The input DataFrame to check for outliers.\n        data_original: Optional original DataFrame before outlier marking. If provided,\n            helps identify which values became NaN due to outlier detection.\n            Default: None.\n        contamination: The estimated proportion of outliers in the dataset.\n            Default: 0.01.\n        random_state: Random seed for reproducibility. Default: 1234.\n\n    Returns:\n        A dictionary mapping column names to Series of outlier values.\n        For columns without outliers, an empty Series is returned.\n\n    Raises:\n        ValueError: If data is empty or contains no columns.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2_safe.preprocessing.outlier import get_outliers\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample data with outliers\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; data = pd.DataFrame({\n        ...     'A': np.concatenate([np.random.normal(0, 1, 100), [10, 11, 12]]),\n        ...     'B': np.concatenate([np.random.normal(5, 2, 100), [100, 110, 120]])\n        ... })\n        &gt;&gt;&gt; data_original = data.copy()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Detect outliers\n        &gt;&gt;&gt; outliers = get_outliers(data_original, contamination=0.03)\n        &gt;&gt;&gt; for col, outlier_vals in outliers.items():\n        ...     print(f\"{col}: {len(outlier_vals)} outliers detected\")\n    \"\"\"\n    if data.empty:\n        raise ValueError(\"Input data is empty\")\n    if len(data.columns) == 0:\n        raise ValueError(\"Input data contains no columns\")\n\n    outliers_dict = {}\n\n    for col in data.columns:\n        iso = IsolationForest(contamination=contamination, random_state=random_state)\n        # Fit and predict (-1 for outliers, 1 for inliers)\n        predictions = iso.fit_predict(data[[col]])\n\n        # Get outlier values\n        if data_original is not None:\n            # Use original data to identify outlier values\n            outlier_mask = predictions == -1\n            outliers_dict[col] = data_original.loc[outlier_mask, col]\n        else:\n            # Use current data\n            outlier_mask = predictions == -1\n            outliers_dict[col] = data.loc[outlier_mask, col]\n\n    return outliers_dict\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2.preprocessing.outlier_plots.visualize_outliers_hist","title":"<code>spotforecast2.preprocessing.outlier_plots.visualize_outliers_hist(data, data_original, columns=None, contamination=0.01, random_state=1234, figsize=(10, 5), bins=50, **kwargs)</code>","text":"<p>Visualize outliers in DataFrame using stacked histograms.</p> <p>Creates a histogram for each specified column, displaying both regular data and detected outliers in different colors. Uses IsolationForest for outlier detection.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame with cleaned data (outliers may be NaN).</p> required <code>data_original</code> <code>DataFrame</code> <p>The original DataFrame before outlier detection.</p> required <code>columns</code> <code>Optional[list[str]]</code> <p>List of column names to visualize. If None, all columns are used. Default: None.</p> <code>None</code> <code>contamination</code> <code>float</code> <p>The estimated proportion of outliers in the dataset. Default: 0.01.</p> <code>0.01</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default: 1234.</p> <code>1234</code> <code>figsize</code> <code>tuple[int, int]</code> <p>Figure size as (width, height). Default: (10, 5).</p> <code>(10, 5)</code> <code>bins</code> <code>int</code> <p>Number of histogram bins. Default: 50.</p> <code>50</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to plt.hist() (e.g., color, alpha, edgecolor, etc.).</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None. Displays matplotlib figures.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data or data_original is empty, or if specified columns don't exist.</p> <code>ImportError</code> <p>If matplotlib is not installed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier_plots import visualize_outliers_hist\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; data_original = pd.DataFrame({\n...     'temperature': np.concatenate([\n...         np.random.normal(20, 5, 100),\n...         [50, 60, 70]  # outliers\n...     ]),\n...     'humidity': np.concatenate([\n...         np.random.normal(60, 10, 100),\n...         [95, 98, 99]  # outliers\n...     ])\n... })\n&gt;&gt;&gt; data_cleaned = data_original.copy()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Visualize outliers\n&gt;&gt;&gt; visualize_outliers_hist(\n...     data_cleaned,\n...     data_original,\n...     contamination=0.03,\n...     figsize=(12, 5),\n...     alpha=0.7\n... )\n</code></pre> Source code in <code>src/spotforecast2/preprocessing/outlier_plots.py</code> <pre><code>def visualize_outliers_hist(\n    data: pd.DataFrame,\n    data_original: pd.DataFrame,\n    columns: Optional[list[str]] = None,\n    contamination: float = 0.01,\n    random_state: int = 1234,\n    figsize: tuple[int, int] = (10, 5),\n    bins: int = 50,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Visualize outliers in DataFrame using stacked histograms.\n\n    Creates a histogram for each specified column, displaying both regular data\n    and detected outliers in different colors. Uses IsolationForest for outlier\n    detection.\n\n    Args:\n        data: The DataFrame with cleaned data (outliers may be NaN).\n        data_original: The original DataFrame before outlier detection.\n        columns: List of column names to visualize. If None, all columns are used.\n            Default: None.\n        contamination: The estimated proportion of outliers in the dataset.\n            Default: 0.01.\n        random_state: Random seed for reproducibility. Default: 1234.\n        figsize: Figure size as (width, height). Default: (10, 5).\n        bins: Number of histogram bins. Default: 50.\n        **kwargs: Additional keyword arguments passed to plt.hist() (e.g., color,\n            alpha, edgecolor, etc.).\n\n    Returns:\n        None. Displays matplotlib figures.\n\n    Raises:\n        ValueError: If data or data_original is empty, or if specified columns\n            don't exist.\n        ImportError: If matplotlib is not installed.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing.outlier_plots import visualize_outliers_hist\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample data\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; data_original = pd.DataFrame({\n        ...     'temperature': np.concatenate([\n        ...         np.random.normal(20, 5, 100),\n        ...         [50, 60, 70]  # outliers\n        ...     ]),\n        ...     'humidity': np.concatenate([\n        ...         np.random.normal(60, 10, 100),\n        ...         [95, 98, 99]  # outliers\n        ...     ])\n        ... })\n        &gt;&gt;&gt; data_cleaned = data_original.copy()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Visualize outliers\n        &gt;&gt;&gt; visualize_outliers_hist(\n        ...     data_cleaned,\n        ...     data_original,\n        ...     contamination=0.03,\n        ...     figsize=(12, 5),\n        ...     alpha=0.7\n        ... )\n    \"\"\"\n    if data.empty or data_original.empty:\n        raise ValueError(\"Input data is empty\")\n\n    columns_to_plot = columns if columns is not None else data.columns\n\n    # Validate columns exist\n    missing_cols = set(columns_to_plot) - set(data.columns)\n    if missing_cols:\n        raise ValueError(f\"Columns not found in data: {missing_cols}\")\n\n    # Detect outliers\n    outliers = get_outliers(\n        data_original,\n        data_original=data_original,\n        contamination=contamination,\n        random_state=random_state,\n    )\n\n    for col in columns_to_plot:\n        # Get inliers (non-NaN values in cleaned data)\n        inliers = data[col].dropna()\n\n        # Get outlier values\n        outlier_vals = outliers[col]\n\n        # Calculate percentage\n        pct_outliers = (len(outlier_vals) / len(data_original)) * 100\n\n        # Create figure\n        plt.figure(figsize=figsize)\n        plt.hist(\n            [inliers, outlier_vals],\n            bins=bins,\n            stacked=True,\n            color=[\"lightgrey\", \"red\"],\n            label=[\"Regular Data\", \"Outliers\"],\n            **kwargs,\n        )\n        plt.grid(True, alpha=0.3)\n        plt.title(f\"{col} Distribution with Outliers ({pct_outliers:.2f}%)\")\n        plt.xlabel(\"Value\")\n        plt.ylabel(\"Frequency\")\n        plt.legend()\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2.preprocessing.outlier_plots.visualize_outliers_plotly_scatter","title":"<code>spotforecast2.preprocessing.outlier_plots.visualize_outliers_plotly_scatter(data, data_original, columns=None, contamination=0.01, random_state=1234, **kwargs)</code>","text":"<p>Visualize outliers in time series using Plotly scatter plots.</p> <p>Creates an interactive time series plot for each specified column, showing regular data as a line and detected outliers as scatter points. Uses IsolationForest for outlier detection.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame with cleaned data (outliers may be NaN).</p> required <code>data_original</code> <code>DataFrame</code> <p>The original DataFrame before outlier detection.</p> required <code>columns</code> <code>Optional[list[str]]</code> <p>List of column names to visualize. If None, all columns are used. Default: None.</p> <code>None</code> <code>contamination</code> <code>float</code> <p>The estimated proportion of outliers in the dataset. Default: 0.01.</p> <code>0.01</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Default: 1234.</p> <code>1234</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to go.Figure.update_layout() (e.g., template, height, etc.).</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None. Displays Plotly figures.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data or data_original is empty, or if specified columns don't exist.</p> <code>ImportError</code> <p>If plotly is not installed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.outlier_plots import visualize_outliers_plotly_scatter\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample time series data\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=103, freq='h')\n&gt;&gt;&gt; data_original = pd.DataFrame({\n...     'temperature': np.concatenate([\n...         np.random.normal(20, 5, 100),\n...         [50, 60, 70]  # outliers\n...     ]),\n...     'humidity': np.concatenate([\n...         np.random.normal(60, 10, 100),\n...         [95, 98, 99]  # outliers\n...     ])\n... }, index=dates)\n&gt;&gt;&gt; data_cleaned = data_original.copy()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Visualize outliers\n&gt;&gt;&gt; visualize_outliers_plotly_scatter(\n...     data_cleaned,\n...     data_original,\n...     contamination=0.03,\n...     template='plotly_white'\n... )\n</code></pre> Source code in <code>src/spotforecast2/preprocessing/outlier_plots.py</code> <pre><code>def visualize_outliers_plotly_scatter(\n    data: pd.DataFrame,\n    data_original: pd.DataFrame,\n    columns: Optional[list[str]] = None,\n    contamination: float = 0.01,\n    random_state: int = 1234,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Visualize outliers in time series using Plotly scatter plots.\n\n    Creates an interactive time series plot for each specified column, showing\n    regular data as a line and detected outliers as scatter points. Uses\n    IsolationForest for outlier detection.\n\n    Args:\n        data: The DataFrame with cleaned data (outliers may be NaN).\n        data_original: The original DataFrame before outlier detection.\n        columns: List of column names to visualize. If None, all columns are used.\n            Default: None.\n        contamination: The estimated proportion of outliers in the dataset.\n            Default: 0.01.\n        random_state: Random seed for reproducibility. Default: 1234.\n        **kwargs: Additional keyword arguments passed to go.Figure.update_layout()\n            (e.g., template, height, etc.).\n\n    Returns:\n        None. Displays Plotly figures.\n\n    Raises:\n        ValueError: If data or data_original is empty, or if specified columns\n            don't exist.\n        ImportError: If plotly is not installed.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing.outlier_plots import visualize_outliers_plotly_scatter\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample time series data\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=103, freq='h')\n        &gt;&gt;&gt; data_original = pd.DataFrame({\n        ...     'temperature': np.concatenate([\n        ...         np.random.normal(20, 5, 100),\n        ...         [50, 60, 70]  # outliers\n        ...     ]),\n        ...     'humidity': np.concatenate([\n        ...         np.random.normal(60, 10, 100),\n        ...         [95, 98, 99]  # outliers\n        ...     ])\n        ... }, index=dates)\n        &gt;&gt;&gt; data_cleaned = data_original.copy()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Visualize outliers\n        &gt;&gt;&gt; visualize_outliers_plotly_scatter(\n        ...     data_cleaned,\n        ...     data_original,\n        ...     contamination=0.03,\n        ...     template='plotly_white'\n        ... )\n    \"\"\"\n    if go is None:\n        raise ImportError(\n            \"plotly is required for this function. \" \"Install with: pip install plotly\"\n        )\n\n    if data.empty or data_original.empty:\n        raise ValueError(\"Input data is empty\")\n\n    columns_to_plot = columns if columns is not None else data.columns\n\n    # Validate columns exist\n    missing_cols = set(columns_to_plot) - set(data.columns)\n    if missing_cols:\n        raise ValueError(f\"Columns not found in data: {missing_cols}\")\n\n    # Detect outliers\n    outliers = get_outliers(\n        data_original,\n        data_original=data_original,\n        contamination=contamination,\n        random_state=random_state,\n    )\n\n    for col in columns_to_plot:\n        fig = go.Figure()\n\n        # Add regular data as line\n        fig.add_trace(\n            go.Scatter(\n                x=data.index,\n                y=data[col],\n                mode=\"lines\",\n                name=\"Regular Data\",\n                line=dict(color=\"lightgrey\"),\n            )\n        )\n\n        # Add outliers as scatter points\n        outlier_vals = outliers[col]\n        if not outlier_vals.empty:\n            fig.add_trace(\n                go.Scatter(\n                    x=outlier_vals.index,\n                    y=outlier_vals,\n                    mode=\"markers\",\n                    name=\"Outliers\",\n                    marker=dict(color=\"red\", size=8, symbol=\"x\"),\n                )\n            )\n\n        # Calculate percentage\n        pct_outliers = (len(outlier_vals) / len(data_original)) * 100\n\n        # Update layout with custom kwargs\n        layout_kwargs = {\n            \"title\": f\"{col} Time Series with Outliers ({pct_outliers:.2f}%)\",\n            \"xaxis_title\": \"Time\",\n            \"yaxis_title\": \"Value\",\n            \"template\": \"plotly_white\",\n            \"legend\": dict(\n                orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1\n            ),\n        }\n        layout_kwargs.update(kwargs)\n        fig.update_layout(**layout_kwargs)\n        fig.show()\n</code></pre>"},{"location":"api/preprocessing/#time-series-visualization","title":"Time Series Visualization","text":""},{"location":"api/preprocessing/#spotforecast2.preprocessing.time_series_visualization.visualize_ts_plotly","title":"<code>spotforecast2.preprocessing.time_series_visualization.visualize_ts_plotly(dataframes, columns=None, title_suffix='', figsize=(1000, 500), template='plotly_white', colors=None, **kwargs)</code>","text":"<p>Visualize multiple time series datasets interactively with Plotly.</p> <p>Creates interactive Plotly scatter plots for specified columns across multiple datasets (e.g., train, validation, test splits). Each dataset is displayed as a separate line with a unique color and name in the legend.</p> <p>Parameters:</p> Name Type Description Default <code>dataframes</code> <code>Dict[str, DataFrame]</code> <p>Dictionary mapping dataset names to pandas DataFrames with datetime index. Example: {'Train': df_train, 'Validation': df_val, 'Test': df_test}</p> required <code>columns</code> <code>Optional[List[str]]</code> <p>List of column names to visualize. If None, all columns are used. Default: None.</p> <code>None</code> <code>title_suffix</code> <code>str</code> <p>Suffix to append to the column name in the title. Useful for adding units or descriptions. Default: \"\".</p> <code>''</code> <code>figsize</code> <code>tuple[int, int]</code> <p>Figure size as (width, height) in pixels. Default: (1000, 500).</p> <code>(1000, 500)</code> <code>template</code> <code>str</code> <p>Plotly template name for styling. Options include 'plotly_white', 'plotly_dark', 'plotly', 'ggplot2', etc. Default: 'plotly_white'.</p> <code>'plotly_white'</code> <code>colors</code> <code>Optional[Dict[str, str]]</code> <p>Dictionary mapping dataset names to colors. If None, uses Plotly default colors. Example: {'Train': 'blue', 'Validation': 'orange'}. Default: None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to go.Scatter() (e.g., mode='lines+markers', line=dict(dash='dash')).</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None. Displays Plotly figures.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dataframes dict is empty, contains no columns, or if specified columns don't exist in all dataframes.</p> <code>ImportError</code> <p>If plotly is not installed.</p> <code>TypeError</code> <p>If dataframes parameter is not a dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import visualize_ts_plotly\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample time series data\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; dates_train = pd.date_range('2024-01-01', periods=100, freq='h')\n&gt;&gt;&gt; dates_val = pd.date_range('2024-05-11', periods=50, freq='h')\n&gt;&gt;&gt; dates_test = pd.date_range('2024-07-01', periods=30, freq='h')\n&gt;&gt;&gt;\n&gt;&gt;&gt; data_train = pd.DataFrame({\n...     'temperature': np.random.normal(20, 5, 100),\n...     'humidity': np.random.normal(60, 10, 100)\n... }, index=dates_train)\n&gt;&gt;&gt;\n&gt;&gt;&gt; data_val = pd.DataFrame({\n...     'temperature': np.random.normal(22, 5, 50),\n...     'humidity': np.random.normal(55, 10, 50)\n... }, index=dates_val)\n&gt;&gt;&gt;\n&gt;&gt;&gt; data_test = pd.DataFrame({\n...     'temperature': np.random.normal(25, 5, 30),\n...     'humidity': np.random.normal(50, 10, 30)\n... }, index=dates_test)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Visualize all datasets\n&gt;&gt;&gt; dataframes = {\n...     'Train': data_train,\n...     'Validation': data_val,\n...     'Test': data_test\n... }\n&gt;&gt;&gt; visualize_ts_plotly(dataframes)\n</code></pre> <p>Single dataset example:</p> <pre><code>&gt;&gt;&gt; # Visualize single dataset\n&gt;&gt;&gt; dataframes = {'Data': data_train}\n&gt;&gt;&gt; visualize_ts_plotly(dataframes, columns=['temperature'])\n</code></pre> <p>Custom styling:</p> <pre><code>&gt;&gt;&gt; visualize_ts_plotly(\n...     dataframes,\n...     columns=['temperature'],\n...     template='plotly_dark',\n...     colors={'Train': 'blue', 'Validation': 'green', 'Test': 'red'},\n...     mode='lines+markers'\n... )\n</code></pre> Source code in <code>src/spotforecast2/preprocessing/time_series_visualization.py</code> <pre><code>def visualize_ts_plotly(\n    dataframes: Dict[str, pd.DataFrame],\n    columns: Optional[List[str]] = None,\n    title_suffix: str = \"\",\n    figsize: tuple[int, int] = (1000, 500),\n    template: str = \"plotly_white\",\n    colors: Optional[Dict[str, str]] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Visualize multiple time series datasets interactively with Plotly.\n\n    Creates interactive Plotly scatter plots for specified columns across multiple\n    datasets (e.g., train, validation, test splits). Each dataset is displayed as\n    a separate line with a unique color and name in the legend.\n\n    Args:\n        dataframes: Dictionary mapping dataset names to pandas DataFrames with datetime\n            index. Example: {'Train': df_train, 'Validation': df_val, 'Test': df_test}\n        columns: List of column names to visualize. If None, all columns are used.\n            Default: None.\n        title_suffix: Suffix to append to the column name in the title. Useful for\n            adding units or descriptions. Default: \"\".\n        figsize: Figure size as (width, height) in pixels. Default: (1000, 500).\n        template: Plotly template name for styling. Options include 'plotly_white',\n            'plotly_dark', 'plotly', 'ggplot2', etc. Default: 'plotly_white'.\n        colors: Dictionary mapping dataset names to colors. If None, uses Plotly\n            default colors. Example: {'Train': 'blue', 'Validation': 'orange'}.\n            Default: None.\n        **kwargs: Additional keyword arguments passed to go.Scatter() (e.g.,\n            mode='lines+markers', line=dict(dash='dash')).\n\n    Returns:\n        None. Displays Plotly figures.\n\n    Raises:\n        ValueError: If dataframes dict is empty, contains no columns, or if\n            specified columns don't exist in all dataframes.\n        ImportError: If plotly is not installed.\n        TypeError: If dataframes parameter is not a dictionary.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import visualize_ts_plotly\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample time series data\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; dates_train = pd.date_range('2024-01-01', periods=100, freq='h')\n        &gt;&gt;&gt; dates_val = pd.date_range('2024-05-11', periods=50, freq='h')\n        &gt;&gt;&gt; dates_test = pd.date_range('2024-07-01', periods=30, freq='h')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; data_train = pd.DataFrame({\n        ...     'temperature': np.random.normal(20, 5, 100),\n        ...     'humidity': np.random.normal(60, 10, 100)\n        ... }, index=dates_train)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; data_val = pd.DataFrame({\n        ...     'temperature': np.random.normal(22, 5, 50),\n        ...     'humidity': np.random.normal(55, 10, 50)\n        ... }, index=dates_val)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; data_test = pd.DataFrame({\n        ...     'temperature': np.random.normal(25, 5, 30),\n        ...     'humidity': np.random.normal(50, 10, 30)\n        ... }, index=dates_test)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Visualize all datasets\n        &gt;&gt;&gt; dataframes = {\n        ...     'Train': data_train,\n        ...     'Validation': data_val,\n        ...     'Test': data_test\n        ... }\n        &gt;&gt;&gt; visualize_ts_plotly(dataframes)\n\n        Single dataset example:\n\n        &gt;&gt;&gt; # Visualize single dataset\n        &gt;&gt;&gt; dataframes = {'Data': data_train}\n        &gt;&gt;&gt; visualize_ts_plotly(dataframes, columns=['temperature'])\n\n        Custom styling:\n\n        &gt;&gt;&gt; visualize_ts_plotly(\n        ...     dataframes,\n        ...     columns=['temperature'],\n        ...     template='plotly_dark',\n        ...     colors={'Train': 'blue', 'Validation': 'green', 'Test': 'red'},\n        ...     mode='lines+markers'\n        ... )\n    \"\"\"\n    if go is None:\n        raise ImportError(\n            \"plotly is required for this function. \" \"Install with: pip install plotly\"\n        )\n\n    if not isinstance(dataframes, dict):\n        raise TypeError(\"dataframes parameter must be a dictionary\")\n\n    if not dataframes:\n        raise ValueError(\"dataframes dictionary is empty\")\n\n    # Validate all dataframes have data\n    for name, df in dataframes.items():\n        if df.empty:\n            raise ValueError(f\"DataFrame '{name}' is empty\")\n        if len(df.columns) == 0:\n            raise ValueError(f\"DataFrame '{name}' contains no columns\")\n\n    # Determine columns to plot\n    all_columns = set()\n    for df in dataframes.values():\n        all_columns.update(df.columns)\n\n    if not all_columns:\n        raise ValueError(\"No columns found in any dataframe\")\n\n    columns_to_plot = columns if columns is not None else sorted(list(all_columns))\n\n    # Validate columns exist in all dataframes\n    for col in columns_to_plot:\n        for name, df in dataframes.items():\n            if col not in df.columns:\n                raise ValueError(f\"Column '{col}' not found in dataframe '{name}'\")\n\n    # Default colors if not provided\n    if colors is None:\n        # Use a set of distinct colors\n        default_colors = [\n            \"#1f77b4\",  # blue\n            \"#ff7f0e\",  # orange\n            \"#2ca02c\",  # green\n            \"#d62728\",  # red\n            \"#9467bd\",  # purple\n            \"#8c564b\",  # brown\n            \"#e377c2\",  # pink\n            \"#7f7f7f\",  # gray\n            \"#bcbd22\",  # olive\n            \"#17becf\",  # cyan\n        ]\n        colors = {\n            name: default_colors[i % len(default_colors)]\n            for i, name in enumerate(dataframes.keys())\n        }\n\n    # Create figures for each column\n    for col in columns_to_plot:\n        fig = go.Figure()\n\n        # Add trace for each dataset\n        for dataset_name, df in dataframes.items():\n            fig.add_trace(\n                go.Scatter(\n                    x=df.index,\n                    y=df[col],\n                    mode=\"lines\",\n                    name=dataset_name,\n                    line=dict(color=colors[dataset_name]),\n                    **kwargs,\n                )\n            )\n\n        # Create title\n        title = col\n        if title_suffix:\n            title = f\"{col} {title_suffix}\"\n\n        # Update layout\n        fig.update_layout(\n            title=title,\n            xaxis_title=\"Time\",\n            yaxis_title=col,\n            width=figsize[0],\n            height=figsize[1],\n            template=template,\n            legend=dict(\n                orientation=\"h\",\n                yanchor=\"bottom\",\n                y=1.02,\n                xanchor=\"right\",\n                x=1,\n            ),\n            hovermode=\"x unified\",\n        )\n\n        fig.show()\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2.preprocessing.time_series_visualization.visualize_ts_comparison","title":"<code>spotforecast2.preprocessing.time_series_visualization.visualize_ts_comparison(dataframes, columns=None, title_suffix='', figsize=(1000, 500), template='plotly_white', colors=None, show_mean=False, **kwargs)</code>","text":"<p>Visualize time series with optional statistical overlays.</p> <p>Similar to visualize_ts_plotly but adds options for statistical overlays like mean values across all datasets.</p> <p>Parameters:</p> Name Type Description Default <code>dataframes</code> <code>Dict[str, DataFrame]</code> <p>Dictionary mapping dataset names to pandas DataFrames.</p> required <code>columns</code> <code>Optional[List[str]]</code> <p>List of column names to visualize. If None, all columns are used. Default: None.</p> <code>None</code> <code>title_suffix</code> <code>str</code> <p>Suffix to append to column names. Default: \"\".</p> <code>''</code> <code>figsize</code> <code>tuple[int, int]</code> <p>Figure size as (width, height) in pixels. Default: (1000, 500).</p> <code>(1000, 500)</code> <code>template</code> <code>str</code> <p>Plotly template. Default: 'plotly_white'.</p> <code>'plotly_white'</code> <code>colors</code> <code>Optional[Dict[str, str]]</code> <p>Dictionary mapping dataset names to colors. Default: None.</p> <code>None</code> <code>show_mean</code> <code>bool</code> <p>If True, overlay the mean of all datasets. Default: False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for go.Scatter().</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None. Displays Plotly figures.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dataframes is empty.</p> <code>ImportError</code> <p>If plotly is not installed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import visualize_ts_comparison\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; dates1 = pd.date_range('2024-01-01', periods=100, freq='h')\n&gt;&gt;&gt; dates2 = pd.date_range('2024-05-11', periods=100, freq='h')\n&gt;&gt;&gt;\n&gt;&gt;&gt; df1 = pd.DataFrame({\n...     'temperature': np.random.normal(20, 5, 100)\n... }, index=dates1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; df2 = pd.DataFrame({\n...     'temperature': np.random.normal(22, 5, 100)\n... }, index=dates2)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Compare with mean overlay\n&gt;&gt;&gt; visualize_ts_comparison(\n...     {'Dataset1': df1, 'Dataset2': df2},\n...     show_mean=True\n... )\n</code></pre> Source code in <code>src/spotforecast2/preprocessing/time_series_visualization.py</code> <pre><code>def visualize_ts_comparison(\n    dataframes: Dict[str, pd.DataFrame],\n    columns: Optional[List[str]] = None,\n    title_suffix: str = \"\",\n    figsize: tuple[int, int] = (1000, 500),\n    template: str = \"plotly_white\",\n    colors: Optional[Dict[str, str]] = None,\n    show_mean: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Visualize time series with optional statistical overlays.\n\n    Similar to visualize_ts_plotly but adds options for statistical overlays\n    like mean values across all datasets.\n\n    Args:\n        dataframes: Dictionary mapping dataset names to pandas DataFrames.\n        columns: List of column names to visualize. If None, all columns are used.\n            Default: None.\n        title_suffix: Suffix to append to column names. Default: \"\".\n        figsize: Figure size as (width, height) in pixels. Default: (1000, 500).\n        template: Plotly template. Default: 'plotly_white'.\n        colors: Dictionary mapping dataset names to colors. Default: None.\n        show_mean: If True, overlay the mean of all datasets. Default: False.\n        **kwargs: Additional keyword arguments for go.Scatter().\n\n    Returns:\n        None. Displays Plotly figures.\n\n    Raises:\n        ValueError: If dataframes is empty.\n        ImportError: If plotly is not installed.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing.time_series_visualization import visualize_ts_comparison\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample data\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; dates1 = pd.date_range('2024-01-01', periods=100, freq='h')\n        &gt;&gt;&gt; dates2 = pd.date_range('2024-05-11', periods=100, freq='h')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; df1 = pd.DataFrame({\n        ...     'temperature': np.random.normal(20, 5, 100)\n        ... }, index=dates1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; df2 = pd.DataFrame({\n        ...     'temperature': np.random.normal(22, 5, 100)\n        ... }, index=dates2)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Compare with mean overlay\n        &gt;&gt;&gt; visualize_ts_comparison(\n        ...     {'Dataset1': df1, 'Dataset2': df2},\n        ...     show_mean=True\n        ... )\n    \"\"\"\n    if go is None:\n        raise ImportError(\n            \"plotly is required for this function. \" \"Install with: pip install plotly\"\n        )\n\n    if not dataframes:\n        raise ValueError(\"dataframes dictionary is empty\")\n\n    # First visualize normally\n    visualize_ts_plotly(\n        dataframes,\n        columns=columns,\n        title_suffix=title_suffix,\n        figsize=figsize,\n        template=template,\n        colors=colors,\n        **kwargs,\n    )\n\n    # If show_mean, create additional mean plot\n    if show_mean:\n        # Determine columns to plot\n        all_columns = set()\n        for df in dataframes.values():\n            all_columns.update(df.columns)\n\n        columns_to_plot = columns if columns is not None else sorted(list(all_columns))\n\n        for col in columns_to_plot:\n            fig = go.Figure()\n\n            # Add individual traces\n            if colors is None:\n                default_colors = [\n                    \"#1f77b4\",\n                    \"#ff7f0e\",\n                    \"#2ca02c\",\n                    \"#d62728\",\n                    \"#9467bd\",\n                ]\n                colors_dict = {\n                    name: default_colors[i % len(default_colors)]\n                    for i, name in enumerate(dataframes.keys())\n                }\n            else:\n                colors_dict = colors\n\n            for dataset_name, df in dataframes.items():\n                fig.add_trace(\n                    go.Scatter(\n                        x=df.index,\n                        y=df[col],\n                        mode=\"lines\",\n                        name=dataset_name,\n                        line=dict(color=colors_dict[dataset_name], width=1),\n                        opacity=0.5,\n                        **kwargs,\n                    )\n                )\n\n            # Calculate and add mean\n            # Align all dataframes by index and compute mean\n            aligned_dfs = [\n                dataframes[name][[col]].rename(columns={col: name})\n                for name in dataframes.keys()\n            ]\n            combined = pd.concat(aligned_dfs, axis=1)\n            mean_values = combined.mean(axis=1)\n\n            fig.add_trace(\n                go.Scatter(\n                    x=mean_values.index,\n                    y=mean_values,\n                    mode=\"lines\",\n                    name=\"Mean\",\n                    line=dict(color=\"black\", width=3, dash=\"dash\"),\n                )\n            )\n\n            title = f\"{col} (with mean){title_suffix}\"\n\n            fig.update_layout(\n                title=title,\n                xaxis_title=\"Time\",\n                yaxis_title=col,\n                width=figsize[0],\n                height=figsize[1],\n                template=template,\n                legend=dict(\n                    orientation=\"h\",\n                    yanchor=\"bottom\",\n                    y=1.02,\n                    xanchor=\"right\",\n                    x=1,\n                ),\n                hovermode=\"x unified\",\n            )\n\n            fig.show()\n</code></pre>"},{"location":"api/preprocessing/#imputation","title":"Imputation","text":""},{"location":"api/preprocessing/#spotforecast2.preprocessing.imputation.custom_weights","title":"<code>spotforecast2.preprocessing.imputation.custom_weights(index, weights_series)</code>","text":"<p>Return 0 if index is in or near any gap.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Index</code> <p>The index to check.</p> required <code>weights_series</code> <code>Series</code> <p>Series containing weights.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The weight corresponding to the index.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.imputation import custom_weights\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; _, missing_weights = get_missing_weights(data, window_size=72, verbose=False)\n&gt;&gt;&gt; for idx in data.index[:5]:\n...     weight = custom_weights(idx, missing_weights)\n...     print(f\"Index: {idx}, Weight: {weight}\")\n</code></pre> Source code in <code>src/spotforecast2/preprocessing/imputation.py</code> <pre><code>def custom_weights(index, weights_series: pd.Series) -&gt; float:\n    \"\"\"\n    Return 0 if index is in or near any gap.\n\n    Args:\n        index (pd.Index):\n            The index to check.\n        weights_series (pd.Series):\n            Series containing weights.\n\n    Returns:\n        float: The weight corresponding to the index.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.imputation import custom_weights\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; _, missing_weights = get_missing_weights(data, window_size=72, verbose=False)\n        &gt;&gt;&gt; for idx in data.index[:5]:\n        ...     weight = custom_weights(idx, missing_weights)\n        ...     print(f\"Index: {idx}, Weight: {weight}\")\n    \"\"\"\n    # do plausibility check\n    if isinstance(index, pd.Index):\n        if not index.isin(weights_series.index).all():\n            raise ValueError(\"Index not found in weights_series.\")\n        return weights_series.loc[index].values\n\n    if index not in weights_series.index:\n        raise ValueError(\"Index not found in weights_series.\")\n    return weights_series.loc[index]\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2.preprocessing.imputation.get_missing_weights","title":"<code>spotforecast2.preprocessing.imputation.get_missing_weights(data, window_size=72, verbose=False)</code>","text":"<p>Return imputed DataFrame and a series indicating missing weights.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input dataset.</p> required <code>window_size</code> <code>int</code> <p>The size of the rolling window to consider for missing values.</p> <code>72</code> <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, Series]</code> <p>Tuple[pd.DataFrame, pd.Series]: A tuple containing the forward and backward filled DataFrame and a boolean series where True indicates missing weights.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.imputation import get_missing_weights\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; filled_data, missing_weights = get_missing_weights(data, window_size=72, verbose=True)\n</code></pre> Source code in <code>src/spotforecast2/preprocessing/imputation.py</code> <pre><code>def get_missing_weights(\n    data: pd.DataFrame, window_size: int = 72, verbose: bool = False\n) -&gt; tuple[pd.DataFrame, pd.Series]:\n    \"\"\"\n    Return imputed DataFrame and a series indicating missing weights.\n\n    Args:\n        data (pd.DataFrame):\n            The input dataset.\n        window_size (int):\n            The size of the rolling window to consider for missing values.\n        verbose (bool):\n            Whether to print additional information.\n\n    Returns:\n        Tuple[pd.DataFrame, pd.Series]:\n            A tuple containing the forward and backward filled DataFrame and a boolean series where True indicates missing weights.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.imputation import get_missing_weights\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; filled_data, missing_weights = get_missing_weights(data, window_size=72, verbose=True)\n\n    \"\"\"\n    # first perform some checks if dataframe has enough data and if window_size is appropriate\n    if data.shape[0] == 0:\n        raise ValueError(\"Input data is empty.\")\n    if window_size &lt;= 0:\n        raise ValueError(\"window_size must be a positive integer.\")\n    if window_size &gt;= data.shape[0]:\n        raise ValueError(\"window_size must be smaller than the number of rows in data.\")\n\n    missing_indices = data.index[data.isnull().any(axis=1)]\n    n_missing = len(missing_indices)\n    if verbose:\n        pct_missing = (n_missing / len(data)) * 100\n        print(f\"Number of rows with missing values: {n_missing}\")\n        print(f\"Percentage of rows with missing values: {pct_missing:.2f}%\")\n        print(f\"missing_indices: {missing_indices}\")\n    data = data.ffill()\n    data = data.bfill()\n\n    is_missing = pd.Series(0, index=data.index)\n    is_missing.loc[missing_indices] = 1\n    weights_series = 1 - is_missing.rolling(window=window_size + 1, min_periods=1).max()\n    if verbose:\n        n_missing_after = weights_series.isna().sum()\n        pct_missing_after = (n_missing_after / len(data)) * 100\n        print(\n            f\"Number of rows with missing weights after processing: {n_missing_after}\"\n        )\n        print(\n            f\"Percentage of rows with missing weights after processing: {pct_missing_after:.2f}%\"\n        )\n    return data, weights_series.isna()\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2.preprocessing.WeightFunction","title":"<code>spotforecast2.preprocessing.WeightFunction</code>","text":"<p>Callable class for sample weights that can be pickled.</p> <p>This class wraps the weights_series and provides a callable interface compatible with ForecasterRecursive's weight_func parameter. Unlike local functions with closures, instances of this class can be pickled using standard pickle/joblib.</p> <p>Parameters:</p> Name Type Description Default <code>weights_series</code> <code>Series</code> <p>Series containing weight values for each index.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import pickle\n&gt;&gt;&gt; weights = pd.Series([1.0, 0.9, 0.8], index=[0, 1, 2])\n&gt;&gt;&gt; weight_func = WeightFunction(weights)\n&gt;&gt;&gt; weight_func(pd.Index([0, 1]))\narray([1. , 0.9])\n&gt;&gt;&gt; # Can be pickled\n&gt;&gt;&gt; pickled = pickle.dumps(weight_func)\n&gt;&gt;&gt; unpickled = pickle.loads(pickled)\n&gt;&gt;&gt; unpickled(pd.Index([0, 1]))\narray([1. , 0.9])\n</code></pre> Source code in <code>spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>class WeightFunction:\n    \"\"\"Callable class for sample weights that can be pickled.\n\n    This class wraps the weights_series and provides a callable interface\n    compatible with ForecasterRecursive's weight_func parameter. Unlike\n    local functions with closures, instances of this class can be pickled\n    using standard pickle/joblib.\n\n    Args:\n        weights_series: Series containing weight values for each index.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import pickle\n        &gt;&gt;&gt; weights = pd.Series([1.0, 0.9, 0.8], index=[0, 1, 2])\n        &gt;&gt;&gt; weight_func = WeightFunction(weights)\n        &gt;&gt;&gt; weight_func(pd.Index([0, 1]))\n        array([1. , 0.9])\n        &gt;&gt;&gt; # Can be pickled\n        &gt;&gt;&gt; pickled = pickle.dumps(weight_func)\n        &gt;&gt;&gt; unpickled = pickle.loads(pickled)\n        &gt;&gt;&gt; unpickled(pd.Index([0, 1]))\n        array([1. , 0.9])\n    \"\"\"\n\n    def __init__(self, weights_series: pd.Series):\n        \"\"\"Initialize with a weights series.\n\n        Args:\n            weights_series: Series containing weight values for each index.\n        \"\"\"\n        self.weights_series = weights_series\n\n    def __call__(\n        self, index: Union[pd.Index, np.ndarray, list]\n    ) -&gt; Union[float, np.ndarray]:\n        \"\"\"Return sample weights for given index.\n\n        Args:\n            index: Index or indices to get weights for.\n\n        Returns:\n            Weight value(s) corresponding to the index.\n        \"\"\"\n        return custom_weights(index, self.weights_series)\n\n    def __repr__(self):\n        \"\"\"String representation.\"\"\"\n        return f\"WeightFunction(weights_series with {len(self.weights_series)} entries)\"\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2.preprocessing.WeightFunction.__call__","title":"<code>__call__(index)</code>","text":"<p>Return sample weights for given index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Union[Index, ndarray, list]</code> <p>Index or indices to get weights for.</p> required <p>Returns:</p> Type Description <code>Union[float, ndarray]</code> <p>Weight value(s) corresponding to the index.</p> Source code in <code>spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def __call__(\n    self, index: Union[pd.Index, np.ndarray, list]\n) -&gt; Union[float, np.ndarray]:\n    \"\"\"Return sample weights for given index.\n\n    Args:\n        index: Index or indices to get weights for.\n\n    Returns:\n        Weight value(s) corresponding to the index.\n    \"\"\"\n    return custom_weights(index, self.weights_series)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2.preprocessing.WeightFunction.__init__","title":"<code>__init__(weights_series)</code>","text":"<p>Initialize with a weights series.</p> <p>Parameters:</p> Name Type Description Default <code>weights_series</code> <code>Series</code> <p>Series containing weight values for each index.</p> required Source code in <code>spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def __init__(self, weights_series: pd.Series):\n    \"\"\"Initialize with a weights series.\n\n    Args:\n        weights_series: Series containing weight values for each index.\n    \"\"\"\n    self.weights_series = weights_series\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2.preprocessing.WeightFunction.__repr__","title":"<code>__repr__()</code>","text":"<p>String representation.</p> Source code in <code>spotforecast2_safe/preprocessing/imputation.py</code> <pre><code>def __repr__(self):\n    \"\"\"String representation.\"\"\"\n    return f\"WeightFunction(weights_series with {len(self.weights_series)} entries)\"\n</code></pre>"},{"location":"api/preprocessing/#split","title":"Split","text":""},{"location":"api/preprocessing/#spotforecast2.preprocessing.split.split_abs_train_val_test","title":"<code>spotforecast2.preprocessing.split.split_abs_train_val_test(data, end_train, end_validation, verbose=False)</code>","text":"<p>Splits a time series DataFrame into training, validation, and test sets based on absolute timestamps.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The time series data with a DateTimeIndex.</p> required <code>end_train</code> <code>Timestamp</code> <p>The end date for the training set.</p> required <code>end_validation</code> <code>Timestamp</code> <p>The end date for the validation set.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[DataFrame, DataFrame, DataFrame]</code> <p>A tuple containing: - data_train (pd.DataFrame): The training set. - data_val (pd.DataFrame): The validation set. - data_test (pd.DataFrame): The test set.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.split import split_train_val_test\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; end_train = pd.Timestamp('2020-12-31 23:00:00')\n&gt;&gt;&gt; end_validation = pd.Timestamp('2021-06-30 23:00:00')\n&gt;&gt;&gt; data_train, data_val, data_test = split_train_val_test(\n...     data,\n...     end_train=end_train,\n...     end_validation=end_validation,\n...     verbose=True\n... )\n</code></pre> Source code in <code>src/spotforecast2/preprocessing/split.py</code> <pre><code>def split_abs_train_val_test(\n    data: pd.DataFrame,\n    end_train: pd.Timestamp,\n    end_validation: pd.Timestamp,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"Splits a time series DataFrame into training, validation, and test sets based on absolute timestamps.\n\n    Args:\n        data (pd.DataFrame): The time series data with a DateTimeIndex.\n        end_train (pd.Timestamp): The end date for the training set.\n        end_validation (pd.Timestamp): The end date for the validation set.\n\n    Returns:\n        tuple: A tuple containing:\n            - data_train (pd.DataFrame): The training set.\n            - data_val (pd.DataFrame): The validation set.\n            - data_test (pd.DataFrame): The test set.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.split import split_train_val_test\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; end_train = pd.Timestamp('2020-12-31 23:00:00')\n        &gt;&gt;&gt; end_validation = pd.Timestamp('2021-06-30 23:00:00')\n        &gt;&gt;&gt; data_train, data_val, data_test = split_train_val_test(\n        ...     data,\n        ...     end_train=end_train,\n        ...     end_validation=end_validation,\n        ...     verbose=True\n        ... )\n    \"\"\"\n    data = data.copy()\n    start_date = data.index.min()\n    end_date = data.index.max()\n    if verbose:\n        print(f\"Start date: {start_date}\")\n        print(f\"End date: {end_date}\")\n    data_train = data.loc[:end_train, :].copy()\n    data_val = data.loc[end_train:end_validation, :].copy()\n    data_test = data.loc[end_validation:, :].copy()\n\n    if verbose:\n        print(\n            f\"Train: {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\"\n        )\n        print(\n            f\"Val: {data_val.index.min()} --- {data_val.index.max()}  (n={len(data_val)})\"\n        )\n        print(\n            f\"Test: {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\"\n        )\n\n    return data_train, data_val, data_test\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2.preprocessing.split.split_rel_train_val_test","title":"<code>spotforecast2.preprocessing.split.split_rel_train_val_test(data, perc_train, perc_val, verbose=False)</code>","text":"<p>Splits a time series DataFrame into training, validation, and test sets by percentages.</p> <p>The test percentage is computed as 1 - perc_train - perc_val. Sizes are rounded to ensure the splits sum to the full dataset size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The time series data with a DateTimeIndex.</p> required <code>perc_train</code> <code>float</code> <p>Fraction of data used for training.</p> required <code>perc_val</code> <code>float</code> <p>Fraction of data used for validation.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print additional information.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[DataFrame, DataFrame, DataFrame]</code> <p>A tuple containing: - data_train (pd.DataFrame): The training set. - data_val (pd.DataFrame): The validation set. - data_test (pd.DataFrame): The test set.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n&gt;&gt;&gt; from spotforecast2.preprocessing.split import split_rel_train_val_test\n&gt;&gt;&gt; data = fetch_data()\n&gt;&gt;&gt; data_train, data_val, data_test = split_rel_train_val_test(\n...     data,\n...     perc_train=0.7,\n...     perc_val=0.2,\n...     verbose=True\n... )\n</code></pre> Source code in <code>src/spotforecast2/preprocessing/split.py</code> <pre><code>def split_rel_train_val_test(\n    data: pd.DataFrame,\n    perc_train: float,\n    perc_val: float,\n    verbose: bool = False,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"Splits a time series DataFrame into training, validation, and test sets by percentages.\n\n    The test percentage is computed as 1 - perc_train - perc_val.\n    Sizes are rounded to ensure the splits sum to the full dataset size.\n\n    Args:\n        data (pd.DataFrame): The time series data with a DateTimeIndex.\n        perc_train (float): Fraction of data used for training.\n        perc_val (float): Fraction of data used for validation.\n        verbose (bool): Whether to print additional information.\n\n    Returns:\n        tuple: A tuple containing:\n            - data_train (pd.DataFrame): The training set.\n            - data_val (pd.DataFrame): The validation set.\n            - data_test (pd.DataFrame): The test set.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2_safe.data.fetch_data import fetch_data\n        &gt;&gt;&gt; from spotforecast2.preprocessing.split import split_rel_train_val_test\n        &gt;&gt;&gt; data = fetch_data()\n        &gt;&gt;&gt; data_train, data_val, data_test = split_rel_train_val_test(\n        ...     data,\n        ...     perc_train=0.7,\n        ...     perc_val=0.2,\n        ...     verbose=True\n        ... )\n    \"\"\"\n    data = data.copy()\n    if data.shape[0] == 0:\n        raise ValueError(\"Input data is empty.\")\n    if not (0 &lt;= perc_train &lt;= 1) or not (0 &lt;= perc_val &lt;= 1):\n        raise ValueError(\"perc_train and perc_val must be between 0 and 1 (inclusive).\")\n\n    perc_test = 1 - perc_train - perc_val\n    if verbose:\n        print(\n            f\"Splitting data into train/val/test with percentages: \"\n            f\"{perc_train:.4%} / {perc_val:.4%} / {perc_test:.4%}\"\n        )\n    if round(perc_test, 10) &lt; 0.0:\n        print(\n            f\"Splitting data into train/val/test with percentages: \"\n            f\"{perc_train:.4%} / {perc_val:.4%} / {perc_test:.4%}\"\n        )\n        raise ValueError(\n            \"perc_train and perc_val must sum to 1 or less to leave room for a test set.\"\n        )\n\n    n_total = len(data)\n    n_train = int(round(n_total * perc_train))\n    n_val = int(round(n_total * perc_val))\n    n_test = n_total - n_train - n_val\n\n    if n_test &lt; 0:\n        n_test = 0\n        n_val = n_total - n_train\n    if n_val &lt; 0:\n        n_val = 0\n        n_train = n_total\n\n    end_train_idx = n_train\n    end_val_idx = n_train + n_val\n\n    data_train = data.iloc[:end_train_idx, :].copy()\n    data_val = data.iloc[end_train_idx:end_val_idx, :].copy()\n    data_test = data.iloc[end_val_idx:, :].copy()\n\n    if verbose:\n        print(f\"Train size: {len(data_train)} ({len(data_train) / n_total:.2%})\")\n        print(f\"Val size: {len(data_val)} ({len(data_val) / n_total:.2%})\")\n        print(f\"Test size: {len(data_test)} ({len(data_test) / n_total:.2%})\")\n\n    return data_train, data_val, data_test\n</code></pre>"},{"location":"api/preprocessing/#transformers","title":"Transformers","text":""},{"location":"api/preprocessing/#spotforecast2.preprocessing._differentiator.TimeSeriesDifferentiator","title":"<code>spotforecast2.preprocessing._differentiator.TimeSeriesDifferentiator</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Transforms a time series into a differenced time series.</p> <p>Parameters:</p> Name Type Description Default <code>order</code> <code>int</code> <p>Order of differentiation. Defaults to 1.</p> <code>1</code> <code>initial_values</code> <code>list, numpy ndarray</code> <p>Values to be used for the inverse transformation (reverting differentiation). If None, the first <code>order</code> values of the training data <code>X</code> are stored during <code>fit</code>.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>initial_values_</code> <code>list</code> <p>Values stored for inverse transformation.</p> <code>last_values_</code> <code>list</code> <p>Last values of the differenced time series.</p> Source code in <code>spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>class TimeSeriesDifferentiator(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transforms a time series into a differenced time series.\n\n    Args:\n        order (int, optional): Order of differentiation. Defaults to 1.\n        initial_values (list, numpy ndarray, optional): Values to be used for the inverse transformation (reverting differentiation).\n            If None, the first `order` values of the training data `X` are stored during `fit`.\n\n    Attributes:\n        initial_values_ (list): Values stored for inverse transformation.\n        last_values_ (list): Last values of the differenced time series.\n    \"\"\"\n\n    def __init__(self, order: int = 1, initial_values: list | np.ndarray | None = None):\n        self.order = order\n        self.initial_values = initial_values\n\n    @_check_X_numpy_ndarray_1d(ensure_1d=True)\n    def fit(self, X: np.ndarray, y: object = None) -&gt; object:\n        \"\"\"\n        Store initial values if not provided.\n        \"\"\"\n        if self.order &lt; 1:\n            raise ValueError(\"`order` must be a positive integer.\")\n\n        if self.initial_values is None:\n            if len(X) &lt; self.order:\n                raise ValueError(\n                    f\"The time series must have at least {self.order} values \"\n                    f\"to compute the differentiation of order {self.order}.\"\n                )\n            self.initial_values_ = list(X[: self.order])\n        else:\n            if len(self.initial_values) != self.order:\n                raise ValueError(\n                    f\"The length of `initial_values` must be equal to the order \"\n                    f\"of differentiation ({self.order}).\"\n                )\n            self.initial_values_ = list(self.initial_values)\n\n        self.last_values_ = X[-self.order :]\n\n        return self\n\n    @_check_X_numpy_ndarray_1d(ensure_1d=True)\n    def transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n        \"\"\"\n        Compute the differences.\n        \"\"\"\n        if not hasattr(self, \"initial_values_\") and self.initial_values is not None:\n            self.fit(X)\n        elif not hasattr(self, \"initial_values_\"):\n            check_is_fitted(self, [\"initial_values_\"])\n\n        X_diff = np.diff(X, n=self.order)\n        # Pad with NaNs to keep same length\n        X_diff = np.concatenate([np.full(self.order, np.nan), X_diff])\n\n        # Update last values seen (for next window inverse)\n        self.last_values_ = X[-self.order :]\n\n        return X_diff\n\n    def inverse_transform_next_window(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Inverse transform for the next window of predictions.\n        \"\"\"\n        check_is_fitted(self, [\"initial_values_\", \"last_values_\"])\n\n        if self.order == 1:\n            result = np.cumsum(X) + self.last_values_[-1]\n        else:\n            # Recursive or iterative approach for higher orders\n            # Simplified: Assuming order 1 is sufficient for now or throwing error\n            raise NotImplementedError(\n                \"inverse_transform_next_window not implemented for order &gt; 1\"\n            )\n\n        return result\n\n    @_check_X_numpy_ndarray_1d(ensure_1d=True)\n    def inverse_transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n        \"\"\"\n        Revert the differences.\n        \"\"\"\n        check_is_fitted(self, [\"initial_values_\"])\n\n        # X contains the differenced series (with NaNs at the beginning potentially)\n        # remove NaNs at the start corresponding to order\n        X_clean = X[self.order :]\n\n        if len(X_clean) == 0:\n            # Just return initial values if only NaNs were passed\n            return np.array(self.initial_values_)\n\n        result = list(self.initial_values_)\n\n        if self.order == 1:\n            current_value = result[-1]\n            restored = []\n            for diff_val in X_clean:\n                current_value += diff_val\n                restored.append(current_value)\n            result.extend(restored)\n        else:\n            # Recursive reconstruction for higher orders logic check\n            # For order &gt; 1, np.diff does repeated diffs.\n            # To invert, we need to do repeated cumsum.\n            # But we need appropriate initial values for each level of integration.\n            # This is a simplified version.\n\n            raise NotImplementedError(\n                \"Inverse transform for order &gt; 1 is currently not fully implemented in this port.\"\n            )\n\n        return np.array(result)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2.preprocessing._differentiator.TimeSeriesDifferentiator.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Store initial values if not provided.</p> Source code in <code>spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>@_check_X_numpy_ndarray_1d(ensure_1d=True)\ndef fit(self, X: np.ndarray, y: object = None) -&gt; object:\n    \"\"\"\n    Store initial values if not provided.\n    \"\"\"\n    if self.order &lt; 1:\n        raise ValueError(\"`order` must be a positive integer.\")\n\n    if self.initial_values is None:\n        if len(X) &lt; self.order:\n            raise ValueError(\n                f\"The time series must have at least {self.order} values \"\n                f\"to compute the differentiation of order {self.order}.\"\n            )\n        self.initial_values_ = list(X[: self.order])\n    else:\n        if len(self.initial_values) != self.order:\n            raise ValueError(\n                f\"The length of `initial_values` must be equal to the order \"\n                f\"of differentiation ({self.order}).\"\n            )\n        self.initial_values_ = list(self.initial_values)\n\n    self.last_values_ = X[-self.order :]\n\n    return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2.preprocessing._differentiator.TimeSeriesDifferentiator.inverse_transform","title":"<code>inverse_transform(X, y=None)</code>","text":"<p>Revert the differences.</p> Source code in <code>spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>@_check_X_numpy_ndarray_1d(ensure_1d=True)\ndef inverse_transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n    \"\"\"\n    Revert the differences.\n    \"\"\"\n    check_is_fitted(self, [\"initial_values_\"])\n\n    # X contains the differenced series (with NaNs at the beginning potentially)\n    # remove NaNs at the start corresponding to order\n    X_clean = X[self.order :]\n\n    if len(X_clean) == 0:\n        # Just return initial values if only NaNs were passed\n        return np.array(self.initial_values_)\n\n    result = list(self.initial_values_)\n\n    if self.order == 1:\n        current_value = result[-1]\n        restored = []\n        for diff_val in X_clean:\n            current_value += diff_val\n            restored.append(current_value)\n        result.extend(restored)\n    else:\n        # Recursive reconstruction for higher orders logic check\n        # For order &gt; 1, np.diff does repeated diffs.\n        # To invert, we need to do repeated cumsum.\n        # But we need appropriate initial values for each level of integration.\n        # This is a simplified version.\n\n        raise NotImplementedError(\n            \"Inverse transform for order &gt; 1 is currently not fully implemented in this port.\"\n        )\n\n    return np.array(result)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2.preprocessing._differentiator.TimeSeriesDifferentiator.inverse_transform_next_window","title":"<code>inverse_transform_next_window(X)</code>","text":"<p>Inverse transform for the next window of predictions.</p> Source code in <code>spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>def inverse_transform_next_window(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Inverse transform for the next window of predictions.\n    \"\"\"\n    check_is_fitted(self, [\"initial_values_\", \"last_values_\"])\n\n    if self.order == 1:\n        result = np.cumsum(X) + self.last_values_[-1]\n    else:\n        # Recursive or iterative approach for higher orders\n        # Simplified: Assuming order 1 is sufficient for now or throwing error\n        raise NotImplementedError(\n            \"inverse_transform_next_window not implemented for order &gt; 1\"\n        )\n\n    return result\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2.preprocessing._differentiator.TimeSeriesDifferentiator.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Compute the differences.</p> Source code in <code>spotforecast2_safe/preprocessing/_differentiator.py</code> <pre><code>@_check_X_numpy_ndarray_1d(ensure_1d=True)\ndef transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n    \"\"\"\n    Compute the differences.\n    \"\"\"\n    if not hasattr(self, \"initial_values_\") and self.initial_values is not None:\n        self.fit(X)\n    elif not hasattr(self, \"initial_values_\"):\n        check_is_fitted(self, [\"initial_values_\"])\n\n    X_diff = np.diff(X, n=self.order)\n    # Pad with NaNs to keep same length\n    X_diff = np.concatenate([np.full(self.order, np.nan), X_diff])\n\n    # Update last values seen (for next window inverse)\n    self.last_values_ = X[-self.order :]\n\n    return X_diff\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2.preprocessing._binner.QuantileBinner","title":"<code>spotforecast2.preprocessing._binner.QuantileBinner</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Bin data into quantile-based bins using numpy.percentile.</p> <p>This class is similar to sklearn's KBinsDiscretizer but optimized for performance using numpy.searchsorted for fast bin assignment. Bin intervals are defined following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside the range are clipped to the first or last bin.</p> <p>Parameters:</p> Name Type Description Default <code>n_bins</code> <code>int</code> <p>The number of quantile-based bins to create. Must be &gt;= 2.</p> required <code>method</code> <code>str</code> <p>The method used to compute quantiles, passed to numpy.percentile. Default is 'linear'. Valid values: \"inverse_cdf\", \"averaged_inverse_cdf\", \"closest_observation\", \"interpolated_inverse_cdf\", \"hazen\", \"weibull\", \"linear\", \"median_unbiased\", \"normal_unbiased\".</p> <code>'linear'</code> <code>subsample</code> <code>int</code> <p>Maximum number of samples for computing quantiles. If dataset has more samples, a random subset is used. Default 200000.</p> <code>200000</code> <code>dtype</code> <code>type</code> <p>Data type for bin indices. Default is numpy.float64.</p> <code>float64</code> <code>random_state</code> <code>int</code> <p>Random seed for subset generation. Default 789654.</p> <code>789654</code> <p>Attributes:</p> Name Type Description <code>n_bins</code> <code>int</code> <p>Number of bins to create.</p> <code>method</code> <code>str</code> <p>Quantile computation method.</p> <code>subsample</code> <code>int</code> <p>Maximum samples for quantile computation.</p> <code>dtype</code> <code>type</code> <p>Data type for bin indices.</p> <code>random_state</code> <code>int</code> <p>Random seed.</p> <code>n_bins_</code> <code>int</code> <p>Actual number of bins after fitting (may differ from n_bins if duplicate edges are found).</p> <code>bin_edges_</code> <code>ndarray</code> <p>Edges of the bins learned during fitting.</p> <code>internal_edges_</code> <code>ndarray</code> <p>Internal edges for optimized bin assignment.</p> <code>intervals_</code> <code>dict</code> <p>Mapping from bin index to (lower, upper) interval bounds.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Basic usage: create 3 quantile bins\n&gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; _ = binner.fit(X)\n&gt;&gt;&gt; result = binner.transform(np.array([1.5, 5.5, 9.5]))\n&gt;&gt;&gt; print(result)\n[0. 1. 2.]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check bin intervals\n&gt;&gt;&gt; print(binner.n_bins_)\n3\n&gt;&gt;&gt; assert len(binner.intervals_) == 3\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use fit_transform for one-step operation\n&gt;&gt;&gt; X2 = np.array([10, 20, 30, 40, 50])\n&gt;&gt;&gt; binner2 = QuantileBinner(n_bins=2)\n&gt;&gt;&gt; bins = binner2.fit_transform(X2)\n&gt;&gt;&gt; print(bins)\n[0. 0. 1. 1. 1.]\n</code></pre> Source code in <code>src/spotforecast2/preprocessing/_binner.py</code> <pre><code>class QuantileBinner(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Bin data into quantile-based bins using numpy.percentile.\n\n    This class is similar to sklearn's KBinsDiscretizer but optimized for\n    performance using numpy.searchsorted for fast bin assignment. Bin intervals\n    are defined following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values\n    outside the range are clipped to the first or last bin.\n\n    Args:\n        n_bins: The number of quantile-based bins to create. Must be &gt;= 2.\n        method: The method used to compute quantiles, passed to numpy.percentile.\n            Default is 'linear'. Valid values: \"inverse_cdf\",\n            \"averaged_inverse_cdf\", \"closest_observation\",\n            \"interpolated_inverse_cdf\", \"hazen\", \"weibull\", \"linear\",\n            \"median_unbiased\", \"normal_unbiased\".\n        subsample: Maximum number of samples for computing quantiles. If dataset\n            has more samples, a random subset is used. Default 200000.\n        dtype: Data type for bin indices. Default is numpy.float64.\n        random_state: Random seed for subset generation. Default 789654.\n\n    Attributes:\n        n_bins (int): Number of bins to create.\n        method (str): Quantile computation method.\n        subsample (int): Maximum samples for quantile computation.\n        dtype (type): Data type for bin indices.\n        random_state (int): Random seed.\n        n_bins_ (int): Actual number of bins after fitting (may differ from n_bins\n            if duplicate edges are found).\n        bin_edges_ (np.ndarray): Edges of the bins learned during fitting.\n        internal_edges_ (np.ndarray): Internal edges for optimized bin assignment.\n        intervals_ (dict): Mapping from bin index to (lower, upper) interval bounds.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Basic usage: create 3 quantile bins\n        &gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n        &gt;&gt;&gt; _ = binner.fit(X)\n        &gt;&gt;&gt; result = binner.transform(np.array([1.5, 5.5, 9.5]))\n        &gt;&gt;&gt; print(result)\n        [0. 1. 2.]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check bin intervals\n        &gt;&gt;&gt; print(binner.n_bins_)\n        3\n        &gt;&gt;&gt; assert len(binner.intervals_) == 3\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Use fit_transform for one-step operation\n        &gt;&gt;&gt; X2 = np.array([10, 20, 30, 40, 50])\n        &gt;&gt;&gt; binner2 = QuantileBinner(n_bins=2)\n        &gt;&gt;&gt; bins = binner2.fit_transform(X2)\n        &gt;&gt;&gt; print(bins)\n        [0. 0. 1. 1. 1.]\n    \"\"\"\n\n    def __init__(\n        self,\n        n_bins: int,\n        method: str = \"linear\",\n        subsample: int = 200000,\n        dtype: type = np.float64,\n        random_state: int = 789654,\n    ) -&gt; None:\n\n        self._validate_params(n_bins, method, subsample, dtype, random_state)\n\n        self.n_bins = n_bins\n        self.method = method\n        self.subsample = subsample\n        self.dtype = dtype\n        self.random_state = random_state\n        self.n_bins_ = None\n        self.bin_edges_ = None\n        self.internal_edges_ = None\n        self.intervals_ = None\n\n    def _validate_params(\n        self, n_bins: int, method: str, subsample: int, dtype: type, random_state: int\n    ):\n        \"\"\"\n        Validate parameters passed to the class initializer.\n\n        Args:\n            n_bins: Number of quantile-based bins. Must be int &gt;= 2.\n            method: Quantile computation method for numpy.percentile.\n            subsample: Number of samples for computing quantiles. Must be int &gt;= 1.\n            dtype: Data type for bin indices. Must be a valid numpy dtype.\n            random_state: Random seed for subset generation. Must be int &gt;= 0.\n\n        Raises:\n            ValueError: If n_bins &lt; 2, method is invalid, subsample &lt; 1,\n                random_state &lt; 0, or dtype is not a valid type.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Valid parameters work fine\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=5, method='linear')\n            &gt;&gt;&gt; assert binner.n_bins == 5\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Invalid n_bins raises ValueError\n            &gt;&gt;&gt; try:\n            ...     binner = QuantileBinner(n_bins=1)\n            ... except ValueError as e:\n            ...     assert 'greater than 1' in str(e)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Invalid method raises ValueError\n            &gt;&gt;&gt; try:\n            ...     binner = QuantileBinner(n_bins=3, method='invalid')\n            ... except ValueError as e:\n            ...     assert 'must be one of' in str(e)\n        \"\"\"\n\n        if not isinstance(n_bins, int) or n_bins &lt; 2:\n            raise ValueError(f\"`n_bins` must be an int greater than 1. Got {n_bins}.\")\n\n        valid_methods = [\n            \"inverse_cdf\",\n            \"averaged_inverse_cdf\",\n            \"closest_observation\",\n            \"interpolated_inverse_cdf\",\n            \"hazen\",\n            \"weibull\",\n            \"linear\",\n            \"median_unbiased\",\n            \"normal_unbiased\",\n        ]\n        if method not in valid_methods:\n            raise ValueError(f\"`method` must be one of {valid_methods}. Got {method}.\")\n        if not isinstance(subsample, int) or subsample &lt; 1:\n            raise ValueError(\n                f\"`subsample` must be an integer greater than or equal to 1. \"\n                f\"Got {subsample}.\"\n            )\n        if not isinstance(random_state, int) or random_state &lt; 0:\n            raise ValueError(\n                f\"`random_state` must be an integer greater than or equal to 0. \"\n                f\"Got {random_state}.\"\n            )\n        if not isinstance(dtype, type):\n            raise ValueError(f\"`dtype` must be a valid numpy dtype. Got {dtype}.\")\n\n    def fit(self, X: np.ndarray, y: object = None) -&gt; object:\n        \"\"\"\n        Learn bin edges based on quantiles from training data.\n\n        Computes quantile-based bin edges using numpy.percentile. If the dataset\n        contains more samples than `subsample`, a random subset is used. Duplicate\n        edges (which can occur with repeated values) are removed automatically.\n\n        Args:\n            X: Training data (1D numpy array) for computing quantiles.\n            y: Ignored.\n\n        Returns:\n            Self for method chaining.\n\n        Raises:\n            ValueError: If input data X is empty.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Fit with basic data\n            &gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n            &gt;&gt;&gt; _ = binner.fit(X)\n            &gt;&gt;&gt; print(binner.n_bins_)\n            3\n            &gt;&gt;&gt; print(len(binner.bin_edges_))\n            4\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Fit with repeated values (may reduce number of bins)\n            &gt;&gt;&gt; X_repeated = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3])\n            &gt;&gt;&gt; binner2 = QuantileBinner(n_bins=5)\n            &gt;&gt;&gt; _ = binner2.fit(X_repeated)\n            &gt;&gt;&gt; # n_bins_ may be less than 5 due to duplicates\n            &gt;&gt;&gt; assert binner2.n_bins_ &lt;= 5\n        \"\"\"\n        # Note: Original implementation expects X, but sklearn TransformerMixin passes y=None.\n        # Adjusted signature to (self, X: np.ndarray, y: object = None)\n\n        if X.size == 0:\n            raise ValueError(\"Input data `X` cannot be empty.\")\n        if len(X) &gt; self.subsample:\n            rng = np.random.default_rng(self.random_state)\n            X = X[rng.integers(0, len(X), self.subsample)]\n\n        bin_edges = np.percentile(\n            a=X, q=np.linspace(0, 100, self.n_bins + 1), method=self.method\n        )\n\n        # Remove duplicate edges (can happen when data has many repeated values)\n        # to ensure bins are always numbered 0 to n_bins_-1\n        self.bin_edges_ = np.unique(bin_edges)\n\n        # Ensure at least 1 bin when all values are identical\n        if len(self.bin_edges_) == 1:\n            # Create artificial edges around the single value\n            self.bin_edges_ = np.array([self.bin_edges_.item(), self.bin_edges_.item()])\n\n        self.n_bins_ = len(self.bin_edges_) - 1\n\n        if self.n_bins_ != self.n_bins:\n            warnings.warn(\n                f\"The number of bins has been reduced from {self.n_bins} to \"\n                f\"{self.n_bins_} due to duplicated edges caused by repeated predicted \"\n                f\"values.\",\n                IgnoredArgumentWarning,\n            )\n\n        # Internal edges for optimized transform with searchsorted\n        self.internal_edges_ = self.bin_edges_[1:-1]\n        self.intervals_ = {\n            int(i): (float(self.bin_edges_[i]), float(self.bin_edges_[i + 1]))\n            for i in range(self.n_bins_)\n        }\n\n        return self\n\n    def transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n        \"\"\"\n        Assign new data to learned bins.\n\n        Uses numpy.searchsorted for efficient bin assignment. Values are assigned\n        to bins following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside\n        the fitted range are clipped to the first or last bin.\n\n        Args:\n            X: Data to assign to bins (1D numpy array).\n            y: Ignored.\n\n        Returns:\n            Bin indices as numpy array with dtype specified in __init__.\n\n        Raises:\n            NotFittedError: If fit() has not been called yet.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Fit and transform\n            &gt;&gt;&gt; X_train = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n            &gt;&gt;&gt; _ = binner.fit(X_train)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; X_test = np.array([1.5, 5.5, 9.5])\n            &gt;&gt;&gt; result = binner.transform(X_test)\n            &gt;&gt;&gt; print(result)\n            [0. 1. 2.]\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Values outside range are clipped\n            &gt;&gt;&gt; X_extreme = np.array([0, 100])\n            &gt;&gt;&gt; result_extreme = binner.transform(X_extreme)\n            &gt;&gt;&gt; print(result_extreme)  # Both clipped to valid bin indices\n            [0. 2.]\n        \"\"\"\n\n        if self.bin_edges_ is None:\n            raise NotFittedError(\n                \"The model has not been fitted yet. Call 'fit' with training data first.\"\n            )\n\n        bin_indices = np.searchsorted(self.internal_edges_, X, side=\"right\").astype(\n            self.dtype\n        )\n\n        return bin_indices\n\n    def fit_transform(self, X, y=None, **fit_params):\n        \"\"\"\n        Fit to data, then transform it.\n\n        Fits transformer to X and y with optional parameters fit_params\n        and returns a transformed version of X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs), \\\n                default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.\n\n        Returns\n        -------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.\n        \"\"\"\n        # fit_transform is usually provided by TransformerMixin but we can implement it\n        # or rely on inheritance. The original implementation had it explicitly.\n\n        self.fit(X, y)\n        return self.transform(X, y)\n\n    def get_params(self, deep: bool = True) -&gt; dict[str, Any]:\n        \"\"\"\n        Get parameters of the quantile binner.\n\n        Returns:\n            Dictionary containing n_bins, method, subsample, dtype, and\n            random_state parameters.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=5, method='median_unbiased', subsample=1000)\n            &gt;&gt;&gt; params = binner.get_params()\n            &gt;&gt;&gt; print(params['n_bins'])\n            5\n            &gt;&gt;&gt; print(params['method'])\n            median_unbiased\n            &gt;&gt;&gt; print(params['subsample'])\n            1000\n        \"\"\"\n\n        return {\n            \"n_bins\": self.n_bins,\n            \"method\": self.method,\n            \"subsample\": self.subsample,\n            \"dtype\": self.dtype,\n            \"random_state\": self.random_state,\n        }\n\n    def set_params(self, **params: Any) -&gt; \"QuantileBinner\":\n        \"\"\"\n        Set parameters of the QuantileBinner.\n\n        Args:\n            **params: Parameter names and values to set as keyword arguments.\n\n        Returns:\n            self: Returns the updated QuantileBinner instance.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n            &gt;&gt;&gt; print(binner.n_bins)\n            3\n            &gt;&gt;&gt; binner.set_params(n_bins=5, method='weibull')\n            &gt;&gt;&gt; print(binner.n_bins)\n            5\n            &gt;&gt;&gt; print(binner.method)\n            weibull\n        \"\"\"\n\n        for param, value in params.items():\n            setattr(self, param, value)\n        return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2.preprocessing._binner.QuantileBinner.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Learn bin edges based on quantiles from training data.</p> <p>Computes quantile-based bin edges using numpy.percentile. If the dataset contains more samples than <code>subsample</code>, a random subset is used. Duplicate edges (which can occur with repeated values) are removed automatically.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Training data (1D numpy array) for computing quantiles.</p> required <code>y</code> <code>object</code> <p>Ignored.</p> <code>None</code> <p>Returns:</p> Type Description <code>object</code> <p>Self for method chaining.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input data X is empty.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit with basic data\n&gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; _ = binner.fit(X)\n&gt;&gt;&gt; print(binner.n_bins_)\n3\n&gt;&gt;&gt; print(len(binner.bin_edges_))\n4\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit with repeated values (may reduce number of bins)\n&gt;&gt;&gt; X_repeated = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3])\n&gt;&gt;&gt; binner2 = QuantileBinner(n_bins=5)\n&gt;&gt;&gt; _ = binner2.fit(X_repeated)\n&gt;&gt;&gt; # n_bins_ may be less than 5 due to duplicates\n&gt;&gt;&gt; assert binner2.n_bins_ &lt;= 5\n</code></pre> Source code in <code>src/spotforecast2/preprocessing/_binner.py</code> <pre><code>def fit(self, X: np.ndarray, y: object = None) -&gt; object:\n    \"\"\"\n    Learn bin edges based on quantiles from training data.\n\n    Computes quantile-based bin edges using numpy.percentile. If the dataset\n    contains more samples than `subsample`, a random subset is used. Duplicate\n    edges (which can occur with repeated values) are removed automatically.\n\n    Args:\n        X: Training data (1D numpy array) for computing quantiles.\n        y: Ignored.\n\n    Returns:\n        Self for method chaining.\n\n    Raises:\n        ValueError: If input data X is empty.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit with basic data\n        &gt;&gt;&gt; X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n        &gt;&gt;&gt; _ = binner.fit(X)\n        &gt;&gt;&gt; print(binner.n_bins_)\n        3\n        &gt;&gt;&gt; print(len(binner.bin_edges_))\n        4\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit with repeated values (may reduce number of bins)\n        &gt;&gt;&gt; X_repeated = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3])\n        &gt;&gt;&gt; binner2 = QuantileBinner(n_bins=5)\n        &gt;&gt;&gt; _ = binner2.fit(X_repeated)\n        &gt;&gt;&gt; # n_bins_ may be less than 5 due to duplicates\n        &gt;&gt;&gt; assert binner2.n_bins_ &lt;= 5\n    \"\"\"\n    # Note: Original implementation expects X, but sklearn TransformerMixin passes y=None.\n    # Adjusted signature to (self, X: np.ndarray, y: object = None)\n\n    if X.size == 0:\n        raise ValueError(\"Input data `X` cannot be empty.\")\n    if len(X) &gt; self.subsample:\n        rng = np.random.default_rng(self.random_state)\n        X = X[rng.integers(0, len(X), self.subsample)]\n\n    bin_edges = np.percentile(\n        a=X, q=np.linspace(0, 100, self.n_bins + 1), method=self.method\n    )\n\n    # Remove duplicate edges (can happen when data has many repeated values)\n    # to ensure bins are always numbered 0 to n_bins_-1\n    self.bin_edges_ = np.unique(bin_edges)\n\n    # Ensure at least 1 bin when all values are identical\n    if len(self.bin_edges_) == 1:\n        # Create artificial edges around the single value\n        self.bin_edges_ = np.array([self.bin_edges_.item(), self.bin_edges_.item()])\n\n    self.n_bins_ = len(self.bin_edges_) - 1\n\n    if self.n_bins_ != self.n_bins:\n        warnings.warn(\n            f\"The number of bins has been reduced from {self.n_bins} to \"\n            f\"{self.n_bins_} due to duplicated edges caused by repeated predicted \"\n            f\"values.\",\n            IgnoredArgumentWarning,\n        )\n\n    # Internal edges for optimized transform with searchsorted\n    self.internal_edges_ = self.bin_edges_[1:-1]\n    self.intervals_ = {\n        int(i): (float(self.bin_edges_[i]), float(self.bin_edges_[i + 1]))\n        for i in range(self.n_bins_)\n    }\n\n    return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2.preprocessing._binner.QuantileBinner.fit_transform","title":"<code>fit_transform(X, y=None, **fit_params)</code>","text":"<p>Fit to data, then transform it.</p> <p>Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.</p>"},{"location":"api/preprocessing/#spotforecast2.preprocessing._binner.QuantileBinner.fit_transform--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_samples, n_features)     Input samples.</p> array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None <p>Target values (None for unsupervised transformations).</p> <p>**fit_params : dict     Additional fit parameters.</p>"},{"location":"api/preprocessing/#spotforecast2.preprocessing._binner.QuantileBinner.fit_transform--returns","title":"Returns","text":"<p>X_new : ndarray array of shape (n_samples, n_features_new)     Transformed array.</p> Source code in <code>src/spotforecast2/preprocessing/_binner.py</code> <pre><code>def fit_transform(self, X, y=None, **fit_params):\n    \"\"\"\n    Fit to data, then transform it.\n\n    Fits transformer to X and y with optional parameters fit_params\n    and returns a transformed version of X.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Input samples.\n\n    y :  array-like of shape (n_samples,) or (n_samples, n_outputs), \\\n            default=None\n        Target values (None for unsupervised transformations).\n\n    **fit_params : dict\n        Additional fit parameters.\n\n    Returns\n    -------\n    X_new : ndarray array of shape (n_samples, n_features_new)\n        Transformed array.\n    \"\"\"\n    # fit_transform is usually provided by TransformerMixin but we can implement it\n    # or rely on inheritance. The original implementation had it explicitly.\n\n    self.fit(X, y)\n    return self.transform(X, y)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2.preprocessing._binner.QuantileBinner.get_params","title":"<code>get_params(deep=True)</code>","text":"<p>Get parameters of the quantile binner.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing n_bins, method, subsample, dtype, and</p> <code>dict[str, Any]</code> <p>random_state parameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=5, method='median_unbiased', subsample=1000)\n&gt;&gt;&gt; params = binner.get_params()\n&gt;&gt;&gt; print(params['n_bins'])\n5\n&gt;&gt;&gt; print(params['method'])\nmedian_unbiased\n&gt;&gt;&gt; print(params['subsample'])\n1000\n</code></pre> Source code in <code>src/spotforecast2/preprocessing/_binner.py</code> <pre><code>def get_params(self, deep: bool = True) -&gt; dict[str, Any]:\n    \"\"\"\n    Get parameters of the quantile binner.\n\n    Returns:\n        Dictionary containing n_bins, method, subsample, dtype, and\n        random_state parameters.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=5, method='median_unbiased', subsample=1000)\n        &gt;&gt;&gt; params = binner.get_params()\n        &gt;&gt;&gt; print(params['n_bins'])\n        5\n        &gt;&gt;&gt; print(params['method'])\n        median_unbiased\n        &gt;&gt;&gt; print(params['subsample'])\n        1000\n    \"\"\"\n\n    return {\n        \"n_bins\": self.n_bins,\n        \"method\": self.method,\n        \"subsample\": self.subsample,\n        \"dtype\": self.dtype,\n        \"random_state\": self.random_state,\n    }\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2.preprocessing._binner.QuantileBinner.set_params","title":"<code>set_params(**params)</code>","text":"<p>Set parameters of the QuantileBinner.</p> <p>Parameters:</p> Name Type Description Default <code>**params</code> <code>Any</code> <p>Parameter names and values to set as keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>self</code> <code>'QuantileBinner'</code> <p>Returns the updated QuantileBinner instance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; print(binner.n_bins)\n3\n&gt;&gt;&gt; binner.set_params(n_bins=5, method='weibull')\n&gt;&gt;&gt; print(binner.n_bins)\n5\n&gt;&gt;&gt; print(binner.method)\nweibull\n</code></pre> Source code in <code>src/spotforecast2/preprocessing/_binner.py</code> <pre><code>def set_params(self, **params: Any) -&gt; \"QuantileBinner\":\n    \"\"\"\n    Set parameters of the QuantileBinner.\n\n    Args:\n        **params: Parameter names and values to set as keyword arguments.\n\n    Returns:\n        self: Returns the updated QuantileBinner instance.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n        &gt;&gt;&gt; print(binner.n_bins)\n        3\n        &gt;&gt;&gt; binner.set_params(n_bins=5, method='weibull')\n        &gt;&gt;&gt; print(binner.n_bins)\n        5\n        &gt;&gt;&gt; print(binner.method)\n        weibull\n    \"\"\"\n\n    for param, value in params.items():\n        setattr(self, param, value)\n    return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2.preprocessing._binner.QuantileBinner.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Assign new data to learned bins.</p> <p>Uses numpy.searchsorted for efficient bin assignment. Values are assigned to bins following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside the fitted range are clipped to the first or last bin.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Data to assign to bins (1D numpy array).</p> required <code>y</code> <code>object</code> <p>Ignored.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Bin indices as numpy array with dtype specified in init.</p> <p>Raises:</p> Type Description <code>NotFittedError</code> <p>If fit() has not been called yet.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit and transform\n&gt;&gt;&gt; X_train = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n&gt;&gt;&gt; _ = binner.fit(X_train)\n&gt;&gt;&gt;\n&gt;&gt;&gt; X_test = np.array([1.5, 5.5, 9.5])\n&gt;&gt;&gt; result = binner.transform(X_test)\n&gt;&gt;&gt; print(result)\n[0. 1. 2.]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Values outside range are clipped\n&gt;&gt;&gt; X_extreme = np.array([0, 100])\n&gt;&gt;&gt; result_extreme = binner.transform(X_extreme)\n&gt;&gt;&gt; print(result_extreme)  # Both clipped to valid bin indices\n[0. 2.]\n</code></pre> Source code in <code>src/spotforecast2/preprocessing/_binner.py</code> <pre><code>def transform(self, X: np.ndarray, y: object = None) -&gt; np.ndarray:\n    \"\"\"\n    Assign new data to learned bins.\n\n    Uses numpy.searchsorted for efficient bin assignment. Values are assigned\n    to bins following the convention: bins[i-1] &lt;= x &lt; bins[i]. Values outside\n    the fitted range are clipped to the first or last bin.\n\n    Args:\n        X: Data to assign to bins (1D numpy array).\n        y: Ignored.\n\n    Returns:\n        Bin indices as numpy array with dtype specified in __init__.\n\n    Raises:\n        NotFittedError: If fit() has not been called yet.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing import QuantileBinner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit and transform\n        &gt;&gt;&gt; X_train = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; binner = QuantileBinner(n_bins=3)\n        &gt;&gt;&gt; _ = binner.fit(X_train)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; X_test = np.array([1.5, 5.5, 9.5])\n        &gt;&gt;&gt; result = binner.transform(X_test)\n        &gt;&gt;&gt; print(result)\n        [0. 1. 2.]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Values outside range are clipped\n        &gt;&gt;&gt; X_extreme = np.array([0, 100])\n        &gt;&gt;&gt; result_extreme = binner.transform(X_extreme)\n        &gt;&gt;&gt; print(result_extreme)  # Both clipped to valid bin indices\n        [0. 2.]\n    \"\"\"\n\n    if self.bin_edges_ is None:\n        raise NotFittedError(\n            \"The model has not been fitted yet. Call 'fit' with training data first.\"\n        )\n\n    bin_indices = np.searchsorted(self.internal_edges_, X, side=\"right\").astype(\n        self.dtype\n    )\n\n    return bin_indices\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2.preprocessing._rolling.RollingFeatures","title":"<code>spotforecast2.preprocessing._rolling.RollingFeatures</code>","text":"<p>Compute rolling window statistics over time series data.</p> <p>This transformer computes rolling statistics (mean, std, min, max, sum, median) over windows of specified sizes from a time series. The class follows the scikit-learn transformer API with fit() and transform() methods, making it compatible with scikit-learn pipelines. It also provides transform_batch() for pandas Series input.</p> <p>Parameters:</p> Name Type Description Default <code>stats</code> <code>str | List[str] | List[Any]</code> <p>Rolling statistics to compute. Can be a single string ('mean', 'std', 'min', 'max', 'sum', 'median'), list of statistic names, or list of callable functions. Multiple statistics can be computed simultaneously.</p> required <code>window_sizes</code> <code>int | List[int]</code> <p>Window size(s) for rolling computation. Can be a single integer or list of integers. Multiple windows are applied to all statistics.</p> required <code>features_names</code> <code>List[str] | None</code> <p>Custom names for output features. If None, names are auto-generated from statistic names and window sizes (e.g., 'roll_mean_7', 'roll_std_14'). Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>stats</code> <p>Statistics specification as provided during initialization.</p> <code>window_sizes</code> <p>List of window sizes for rolling computation.</p> <code>features_names</code> <p>List of output feature names.</p> <code>stats_funcs</code> <p>List of compiled/numba-optimized statistical functions.</p> Note <ul> <li>Output contains NaN values for positions where the rolling window cannot   be fully computed (first window_size-1 positions).</li> <li>Statistics are computed using numba-optimized JIT functions for performance.</li> <li>The transformer returns numpy arrays from transform() and pandas DataFrames   from transform_batch() to maintain index alignment.</li> <li>Supports custom user-defined functions in the stats parameter.</li> </ul> <p>Examples:</p> <p>Create a transformer with single statistic and window size:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.preprocessing import RollingFeatures\n&gt;&gt;&gt; y = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0])\n&gt;&gt;&gt; rf = RollingFeatures(stats='mean', window_sizes=3)\n&gt;&gt;&gt; rf.fit(y)\n&gt;&gt;&gt; features = rf.transform(y)\n&gt;&gt;&gt; features.shape\n(10, 1)\n&gt;&gt;&gt; features[:4]  # First 3 values are NaN\narray([[nan],\n       [nan],\n       [2.],\n       [3.]])\n</code></pre> <p>Create a transformer with multiple statistics and window sizes:</p> <pre><code>&gt;&gt;&gt; rf = RollingFeatures(\n...     stats=['mean', 'std', 'min', 'max'],\n...     window_sizes=[3, 7]\n... )\n&gt;&gt;&gt; rf.fit(y)\n&gt;&gt;&gt; features = rf.transform(y)\n&gt;&gt;&gt; features.shape\n(10, 8)  # 4 stats \u00d7 2 window sizes\n&gt;&gt;&gt; rf.features_names\n['roll_mean_3', 'roll_std_3', 'roll_min_3', 'roll_max_3',\n 'roll_mean_7', 'roll_std_7', 'roll_min_7', 'roll_max_7']\n</code></pre> <p>Use with pandas Series to preserve index:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=10, freq='D')\n&gt;&gt;&gt; y_series = pd.Series(y, index=dates)\n&gt;&gt;&gt; rf = RollingFeatures(stats=['mean', 'max'], window_sizes=5)\n&gt;&gt;&gt; features_df = rf.transform_batch(y_series)\n&gt;&gt;&gt; features_df.shape\n(10, 2)\n&gt;&gt;&gt; features_df.index.equals(y_series.index)\nTrue\n</code></pre> <p>Use with custom feature names:</p> <pre><code>&gt;&gt;&gt; rf = RollingFeatures(\n...     stats='mean',\n...     window_sizes=[7, 14, 30],\n...     features_names=['ma_7', 'ma_14', 'ma_30']\n... )\n&gt;&gt;&gt; rf.fit(y)\n&gt;&gt;&gt; rf.features_names\n['ma_7', 'ma_14', 'ma_30']\n</code></pre> Source code in <code>src/spotforecast2/preprocessing/_rolling.py</code> <pre><code>class RollingFeatures:\n    \"\"\"\n    Compute rolling window statistics over time series data.\n\n    This transformer computes rolling statistics (mean, std, min, max, sum, median)\n    over windows of specified sizes from a time series. The class follows the\n    scikit-learn transformer API with fit() and transform() methods, making it\n    compatible with scikit-learn pipelines. It also provides transform_batch()\n    for pandas Series input.\n\n    Args:\n        stats: Rolling statistics to compute. Can be a single string ('mean', 'std',\n            'min', 'max', 'sum', 'median'), list of statistic names, or list of\n            callable functions. Multiple statistics can be computed simultaneously.\n        window_sizes: Window size(s) for rolling computation. Can be a single integer\n            or list of integers. Multiple windows are applied to all statistics.\n        features_names: Custom names for output features. If None, names are\n            auto-generated from statistic names and window sizes (e.g.,\n            'roll_mean_7', 'roll_std_14'). Defaults to None.\n\n    Attributes:\n        stats: Statistics specification as provided during initialization.\n        window_sizes: List of window sizes for rolling computation.\n        features_names: List of output feature names.\n        stats_funcs: List of compiled/numba-optimized statistical functions.\n\n    Note:\n        - Output contains NaN values for positions where the rolling window cannot\n          be fully computed (first window_size-1 positions).\n        - Statistics are computed using numba-optimized JIT functions for performance.\n        - The transformer returns numpy arrays from transform() and pandas DataFrames\n          from transform_batch() to maintain index alignment.\n        - Supports custom user-defined functions in the stats parameter.\n\n    Examples:\n        Create a transformer with single statistic and window size:\n\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.preprocessing import RollingFeatures\n        &gt;&gt;&gt; y = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0])\n        &gt;&gt;&gt; rf = RollingFeatures(stats='mean', window_sizes=3)\n        &gt;&gt;&gt; rf.fit(y)\n        &gt;&gt;&gt; features = rf.transform(y)\n        &gt;&gt;&gt; features.shape\n        (10, 1)\n        &gt;&gt;&gt; features[:4]  # First 3 values are NaN\n        array([[nan],\n               [nan],\n               [2.],\n               [3.]])\n\n        Create a transformer with multiple statistics and window sizes:\n\n        &gt;&gt;&gt; rf = RollingFeatures(\n        ...     stats=['mean', 'std', 'min', 'max'],\n        ...     window_sizes=[3, 7]\n        ... )\n        &gt;&gt;&gt; rf.fit(y)\n        &gt;&gt;&gt; features = rf.transform(y)\n        &gt;&gt;&gt; features.shape\n        (10, 8)  # 4 stats \u00d7 2 window sizes\n        &gt;&gt;&gt; rf.features_names\n        ['roll_mean_3', 'roll_std_3', 'roll_min_3', 'roll_max_3',\n         'roll_mean_7', 'roll_std_7', 'roll_min_7', 'roll_max_7']\n\n        Use with pandas Series to preserve index:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=10, freq='D')\n        &gt;&gt;&gt; y_series = pd.Series(y, index=dates)\n        &gt;&gt;&gt; rf = RollingFeatures(stats=['mean', 'max'], window_sizes=5)\n        &gt;&gt;&gt; features_df = rf.transform_batch(y_series)\n        &gt;&gt;&gt; features_df.shape\n        (10, 2)\n        &gt;&gt;&gt; features_df.index.equals(y_series.index)\n        True\n\n        Use with custom feature names:\n\n        &gt;&gt;&gt; rf = RollingFeatures(\n        ...     stats='mean',\n        ...     window_sizes=[7, 14, 30],\n        ...     features_names=['ma_7', 'ma_14', 'ma_30']\n        ... )\n        &gt;&gt;&gt; rf.fit(y)\n        &gt;&gt;&gt; rf.features_names\n        ['ma_7', 'ma_14', 'ma_30']\n    \"\"\"\n\n    def __init__(\n        self,\n        stats: str | List[str] | List[Any],\n        window_sizes: int | List[int],\n        features_names: List[str] | None = None,\n    ):\n        \"\"\"\n        Initialize the rolling features transformer.\n\n        Args:\n            stats: Rolling statistics to compute. Can be a single string or list\n                of statistics/functions.\n            window_sizes: Window size(s) for rolling statistics.\n            features_names: Custom names for output features. If None, auto-generated.\n                Defaults to None.\n        \"\"\"\n        self.stats = stats\n        self.window_sizes = window_sizes\n        self.features_names = features_names\n\n        # Validation and processing logic...\n        self._validate_params()\n\n    def _validate_params(self):\n        \"\"\"\n        Validate and process rolling features parameters.\n\n        Converts single values to lists, maps string statistics to functions,\n        and generates feature names if not provided.\n\n        Raises:\n            ValueError: If an unsupported statistic name is provided.\n        \"\"\"\n        if isinstance(self.window_sizes, int):\n            self.window_sizes = [self.window_sizes]\n\n        if isinstance(self.stats, str):\n            self.stats = [self.stats]\n\n        # Map strings to functions\n        valid_stats = {\n            \"mean\": _np_mean_jit,\n            \"std\": _np_std_jit,\n            \"min\": _np_min_jit,\n            \"max\": _np_max_jit,\n            \"sum\": _np_sum_jit,\n            \"median\": _np_median_jit,\n        }\n\n        self.stats_funcs = []\n        for s in self.stats:\n            if isinstance(s, str):\n                if s not in valid_stats:\n                    raise ValueError(\n                        f\"Stat '{s}' not supported. Supported: {list(valid_stats.keys())}\"\n                    )\n                self.stats_funcs.append(valid_stats[s])\n            else:\n                self.stats_funcs.append(s)\n\n        if self.features_names is None:\n            self.features_names = []\n            for ws in self.window_sizes:\n                for s in self.stats:\n                    s_name = s if isinstance(s, str) else s.__name__\n                    self.features_names.append(f\"roll_{s_name}_{ws}\")\n\n    def fit(self, X: Any, y: Any = None) -&gt; \"RollingFeatures\":\n        \"\"\"\n        Fit the rolling features transformer (no-op).\n\n        This transformer does not learn any parameters from the data.\n        Method exists for scikit-learn compatibility.\n\n        Args:\n            X: Time series data (not used for fitting).\n            y: Target values (ignored). Defaults to None.\n\n        Returns:\n            self: Returns the fitted transformer.\n        \"\"\"\n        return self\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Compute rolling window statistics from time series data.\n\n        For each statistic and window size combination, computes the rolling\n        statistic across the input time series. The output contains NaN values\n        for the initial positions where the window cannot be fully computed.\n\n        Args:\n            X: Time series data as 1D numpy array or array-like.\n\n        Returns:\n            np.ndarray: Array of shape (len(X), len(features_names)) containing\n                the computed rolling statistics. Each column corresponds to a\n                feature in features_names. Early positions contain NaN values\n                before the window is fully populated.\n        \"\"\"\n        # Assume X is 1D array\n        n_samples = len(X)\n        output = np.full((n_samples, len(self.features_names)), np.nan)\n\n        idx_feature = 0\n        for ws in self.window_sizes:\n            for func in self.stats_funcs:\n                # Naive rolling window loop - can be optimized or use pandas rolling\n                # Using pandas for simplicity and speed if X is convertible\n                series = pd.Series(X)\n                rolled = series.rolling(window=ws).apply(func, raw=True)\n                output[:, idx_feature] = rolled.values\n                idx_feature += 1\n\n        return output\n\n    def transform_batch(self, X: pd.Series) -&gt; pd.DataFrame:\n        \"\"\"\n        Compute rolling features from a pandas Series with index preservation.\n\n        Transforms a pandas Series into a DataFrame of rolling statistics while\n        preserving the original index. Useful for maintaining time alignment\n        with the input data.\n\n        Args:\n            X: Time series data as pandas Series. The index is preserved in output.\n\n        Returns:\n            pd.DataFrame: DataFrame with shape (len(X), len(features_names)) where\n                columns are feature names and index matches the input Series.\n                Contains NaN values at the beginning where windows are incomplete.\n\n        Note:\n            This method is preferred over transform() when working with time-indexed\n            data, as it preserves the temporal index and is compatible with\n            forecasting workflows.\n        \"\"\"\n        values = X.to_numpy()\n        transformed = self.transform(values)\n        return pd.DataFrame(transformed, index=X.index, columns=self.features_names)\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2.preprocessing._rolling.RollingFeatures.__init__","title":"<code>__init__(stats, window_sizes, features_names=None)</code>","text":"<p>Initialize the rolling features transformer.</p> <p>Parameters:</p> Name Type Description Default <code>stats</code> <code>str | List[str] | List[Any]</code> <p>Rolling statistics to compute. Can be a single string or list of statistics/functions.</p> required <code>window_sizes</code> <code>int | List[int]</code> <p>Window size(s) for rolling statistics.</p> required <code>features_names</code> <code>List[str] | None</code> <p>Custom names for output features. If None, auto-generated. Defaults to None.</p> <code>None</code> Source code in <code>src/spotforecast2/preprocessing/_rolling.py</code> <pre><code>def __init__(\n    self,\n    stats: str | List[str] | List[Any],\n    window_sizes: int | List[int],\n    features_names: List[str] | None = None,\n):\n    \"\"\"\n    Initialize the rolling features transformer.\n\n    Args:\n        stats: Rolling statistics to compute. Can be a single string or list\n            of statistics/functions.\n        window_sizes: Window size(s) for rolling statistics.\n        features_names: Custom names for output features. If None, auto-generated.\n            Defaults to None.\n    \"\"\"\n    self.stats = stats\n    self.window_sizes = window_sizes\n    self.features_names = features_names\n\n    # Validation and processing logic...\n    self._validate_params()\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2.preprocessing._rolling.RollingFeatures.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the rolling features transformer (no-op).</p> <p>This transformer does not learn any parameters from the data. Method exists for scikit-learn compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Any</code> <p>Time series data (not used for fitting).</p> required <code>y</code> <code>Any</code> <p>Target values (ignored). Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>RollingFeatures</code> <p>Returns the fitted transformer.</p> Source code in <code>src/spotforecast2/preprocessing/_rolling.py</code> <pre><code>def fit(self, X: Any, y: Any = None) -&gt; \"RollingFeatures\":\n    \"\"\"\n    Fit the rolling features transformer (no-op).\n\n    This transformer does not learn any parameters from the data.\n    Method exists for scikit-learn compatibility.\n\n    Args:\n        X: Time series data (not used for fitting).\n        y: Target values (ignored). Defaults to None.\n\n    Returns:\n        self: Returns the fitted transformer.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2.preprocessing._rolling.RollingFeatures.transform","title":"<code>transform(X)</code>","text":"<p>Compute rolling window statistics from time series data.</p> <p>For each statistic and window size combination, computes the rolling statistic across the input time series. The output contains NaN values for the initial positions where the window cannot be fully computed.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Time series data as 1D numpy array or array-like.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of shape (len(X), len(features_names)) containing the computed rolling statistics. Each column corresponds to a feature in features_names. Early positions contain NaN values before the window is fully populated.</p> Source code in <code>src/spotforecast2/preprocessing/_rolling.py</code> <pre><code>def transform(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute rolling window statistics from time series data.\n\n    For each statistic and window size combination, computes the rolling\n    statistic across the input time series. The output contains NaN values\n    for the initial positions where the window cannot be fully computed.\n\n    Args:\n        X: Time series data as 1D numpy array or array-like.\n\n    Returns:\n        np.ndarray: Array of shape (len(X), len(features_names)) containing\n            the computed rolling statistics. Each column corresponds to a\n            feature in features_names. Early positions contain NaN values\n            before the window is fully populated.\n    \"\"\"\n    # Assume X is 1D array\n    n_samples = len(X)\n    output = np.full((n_samples, len(self.features_names)), np.nan)\n\n    idx_feature = 0\n    for ws in self.window_sizes:\n        for func in self.stats_funcs:\n            # Naive rolling window loop - can be optimized or use pandas rolling\n            # Using pandas for simplicity and speed if X is convertible\n            series = pd.Series(X)\n            rolled = series.rolling(window=ws).apply(func, raw=True)\n            output[:, idx_feature] = rolled.values\n            idx_feature += 1\n\n    return output\n</code></pre>"},{"location":"api/preprocessing/#spotforecast2.preprocessing._rolling.RollingFeatures.transform_batch","title":"<code>transform_batch(X)</code>","text":"<p>Compute rolling features from a pandas Series with index preservation.</p> <p>Transforms a pandas Series into a DataFrame of rolling statistics while preserving the original index. Useful for maintaining time alignment with the input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Series</code> <p>Time series data as pandas Series. The index is preserved in output.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with shape (len(X), len(features_names)) where columns are feature names and index matches the input Series. Contains NaN values at the beginning where windows are incomplete.</p> Note <p>This method is preferred over transform() when working with time-indexed data, as it preserves the temporal index and is compatible with forecasting workflows.</p> Source code in <code>src/spotforecast2/preprocessing/_rolling.py</code> <pre><code>def transform_batch(self, X: pd.Series) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute rolling features from a pandas Series with index preservation.\n\n    Transforms a pandas Series into a DataFrame of rolling statistics while\n    preserving the original index. Useful for maintaining time alignment\n    with the input data.\n\n    Args:\n        X: Time series data as pandas Series. The index is preserved in output.\n\n    Returns:\n        pd.DataFrame: DataFrame with shape (len(X), len(features_names)) where\n            columns are feature names and index matches the input Series.\n            Contains NaN values at the beginning where windows are incomplete.\n\n    Note:\n        This method is preferred over transform() when working with time-indexed\n        data, as it preserves the temporal index and is compatible with\n        forecasting workflows.\n    \"\"\"\n    values = X.to_numpy()\n    transformed = self.transform(values)\n    return pd.DataFrame(transformed, index=X.index, columns=self.features_names)\n</code></pre>"},{"location":"api/stats/","title":"Stats Module","text":""},{"location":"api/stats/#autocorrelation","title":"Autocorrelation","text":""},{"location":"api/stats/#spotforecast2.stats.autocorrelation.calculate_lag_autocorrelation","title":"<code>spotforecast2.stats.autocorrelation.calculate_lag_autocorrelation(data, n_lags=50, last_n_samples=None, sort_by='partial_autocorrelation_abs', acf_kwargs={}, pacf_kwargs={})</code>","text":"<p>Calculate autocorrelation and partial autocorrelation for a time series.</p> <p>This is a wrapper around statsmodels.acf and statsmodels.pacf.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Series | DataFrame</code> <p>Time series to calculate autocorrelation. If a DataFrame is provided, it must have exactly one column.</p> required <code>n_lags</code> <code>int</code> <p>Number of lags to calculate autocorrelation. Default is 50.</p> <code>50</code> <code>last_n_samples</code> <code>int | None</code> <p>Number of most recent samples to use. If None, use the entire series. Note that partial correlations can only be computed for lags up to 50% of the sample size. For example, if the series has 10 samples, n_lags must be less than or equal to 5. This parameter is useful to speed up calculations when the series is very long. Default is None.</p> <code>None</code> <code>sort_by</code> <code>str</code> <p>Sort results by lag, partial_autocorrelation_abs, partial_autocorrelation, autocorrelation_abs or autocorrelation. Default is partial_autocorrelation_abs.</p> <code>'partial_autocorrelation_abs'</code> <code>acf_kwargs</code> <code>dict[str, object]</code> <p>Optional arguments to pass to statsmodels.tsa.stattools.acf. Default is {}.</p> <code>{}</code> <code>pacf_kwargs</code> <code>dict[str, object]</code> <p>Optional arguments to pass to statsmodels.tsa.stattools.pacf. Default is {}.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns: lag, partial_autocorrelation_abs, partial_autocorrelation, autocorrelation_abs, autocorrelation.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If data is not a pandas Series or DataFrame with a single column.</p> <code>ValueError</code> <p>If data is a DataFrame with more than one column.</p> <code>TypeError</code> <p>If n_lags is not a positive integer.</p> <code>TypeError</code> <p>If last_n_samples is not None and not a positive integer.</p> <code>ValueError</code> <p>If sort_by is not one of the valid options.</p> <p>Examples:</p> <p>Calculate autocorrelation for a simple Series:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast.stats.autocorrelation import calculate_lag_autocorrelation\n&gt;&gt;&gt;\n&gt;&gt;&gt; data = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; result = calculate_lag_autocorrelation(data=data, n_lags=4)\n&gt;&gt;&gt; result.head()\n   lag  partial_autocorrelation_abs  partial_autocorrelation  autocorrelation_abs  autocorrelation\n0    1                     0.999998                 0.999998             1.000000         1.000000\n1    2                     0.000002                -0.000002             0.645497         0.645497\n2    3                     0.000002                 0.000002             0.298549         0.298549\n3    4                     0.000001                -0.000001             0.068719         0.068719\n</code></pre> <p>Calculate autocorrelation using only the last 8 samples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2.stats.autocorrelation import calculate_lag_autocorrelation\n&gt;&gt;&gt;\n&gt;&gt;&gt; data = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; result = calculate_lag_autocorrelation(\n...     data=data,\n...     n_lags=3,\n...     last_n_samples=8\n... )\n&gt;&gt;&gt; result.shape\n(3, 5)\n</code></pre> <p>Calculate autocorrelation from a DataFrame with a single column:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast.stats.autocorrelation import calculate_lag_autocorrelation\n&gt;&gt;&gt;\n&gt;&gt;&gt; data = pd.DataFrame({'value': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})\n&gt;&gt;&gt; result = calculate_lag_autocorrelation(data=data, n_lags=4)\n&gt;&gt;&gt; result.shape\n(4, 5)\n</code></pre> <p>Sort results by autocorrelation in descending order:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast.stats.autocorrelation import calculate_lag_autocorrelation\n&gt;&gt;&gt;\n&gt;&gt;&gt; data = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; result = calculate_lag_autocorrelation(\n...     data=data,\n...     n_lags=4,\n...     sort_by='autocorrelation'\n... )\n&gt;&gt;&gt; result[['lag', 'autocorrelation']].head()\n   lag  autocorrelation\n0    1         1.000000\n1    2         0.645497\n2    3         0.298549\n3    4         0.068719\n</code></pre> Source code in <code>src/spotforecast2/stats/autocorrelation.py</code> <pre><code>def calculate_lag_autocorrelation(\n    data: pd.Series | pd.DataFrame,\n    n_lags: int = 50,\n    last_n_samples: int | None = None,\n    sort_by: str = \"partial_autocorrelation_abs\",\n    acf_kwargs: dict[str, object] = {},\n    pacf_kwargs: dict[str, object] = {},\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate autocorrelation and partial autocorrelation for a time series.\n\n    This is a wrapper around statsmodels.acf and statsmodels.pacf.\n\n    Args:\n        data: Time series to calculate autocorrelation. If a DataFrame is provided,\n            it must have exactly one column.\n        n_lags: Number of lags to calculate autocorrelation. Default is 50.\n        last_n_samples: Number of most recent samples to use. If None, use the entire\n            series. Note that partial correlations can only be computed for lags up to\n            50% of the sample size. For example, if the series has 10 samples,\n            n_lags must be less than or equal to 5. This parameter is useful\n            to speed up calculations when the series is very long. Default is None.\n        sort_by: Sort results by lag, partial_autocorrelation_abs,\n            partial_autocorrelation, autocorrelation_abs or autocorrelation.\n            Default is partial_autocorrelation_abs.\n        acf_kwargs: Optional arguments to pass to statsmodels.tsa.stattools.acf.\n            Default is {}.\n        pacf_kwargs: Optional arguments to pass to statsmodels.tsa.stattools.pacf.\n            Default is {}.\n\n    Returns:\n        DataFrame with columns: lag, partial_autocorrelation_abs,\n            partial_autocorrelation, autocorrelation_abs, autocorrelation.\n\n    Raises:\n        TypeError: If data is not a pandas Series or DataFrame with a single column.\n        ValueError: If data is a DataFrame with more than one column.\n        TypeError: If n_lags is not a positive integer.\n        TypeError: If last_n_samples is not None and not a positive integer.\n        ValueError: If sort_by is not one of the valid options.\n\n    Examples:\n        Calculate autocorrelation for a simple Series:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast.stats.autocorrelation import calculate_lag_autocorrelation\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; data = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; result = calculate_lag_autocorrelation(data=data, n_lags=4)\n        &gt;&gt;&gt; result.head()\n           lag  partial_autocorrelation_abs  partial_autocorrelation  autocorrelation_abs  autocorrelation\n        0    1                     0.999998                 0.999998             1.000000         1.000000\n        1    2                     0.000002                -0.000002             0.645497         0.645497\n        2    3                     0.000002                 0.000002             0.298549         0.298549\n        3    4                     0.000001                -0.000001             0.068719         0.068719\n\n        Calculate autocorrelation using only the last 8 samples:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2.stats.autocorrelation import calculate_lag_autocorrelation\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; data = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; result = calculate_lag_autocorrelation(\n        ...     data=data,\n        ...     n_lags=3,\n        ...     last_n_samples=8\n        ... )\n        &gt;&gt;&gt; result.shape\n        (3, 5)\n\n        Calculate autocorrelation from a DataFrame with a single column:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast.stats.autocorrelation import calculate_lag_autocorrelation\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; data = pd.DataFrame({'value': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})\n        &gt;&gt;&gt; result = calculate_lag_autocorrelation(data=data, n_lags=4)\n        &gt;&gt;&gt; result.shape\n        (4, 5)\n\n        Sort results by autocorrelation in descending order:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast.stats.autocorrelation import calculate_lag_autocorrelation\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; data = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; result = calculate_lag_autocorrelation(\n        ...     data=data,\n        ...     n_lags=4,\n        ...     sort_by='autocorrelation'\n        ... )\n        &gt;&gt;&gt; result[['lag', 'autocorrelation']].head()\n           lag  autocorrelation\n        0    1         1.000000\n        1    2         0.645497\n        2    3         0.298549\n        3    4         0.068719\n\n    \"\"\"\n    check_optional_dependency(\"statsmodels\")\n\n    if not isinstance(data, (pd.Series, pd.DataFrame)):\n        raise TypeError(\n            f\"`data` must be a pandas Series or a DataFrame with a single column. \"\n            f\"Got {type(data)}.\"\n        )\n    if isinstance(data, pd.DataFrame) and data.shape[1] != 1:\n        raise ValueError(\n            f\"If `data` is a DataFrame, it must have exactly one column. \"\n            f\"Got {data.shape[1]} columns.\"\n        )\n    if not isinstance(n_lags, int) or n_lags &lt;= 0:\n        raise TypeError(f\"`n_lags` must be a positive integer. Got {n_lags}.\")\n\n    if last_n_samples is not None:\n        if not isinstance(last_n_samples, int) or last_n_samples &lt;= 0:\n            raise TypeError(\n                f\"`last_n_samples` must be a positive integer. Got {last_n_samples}.\"\n            )\n        data = data.iloc[-last_n_samples:]\n\n    if sort_by not in [\n        \"lag\",\n        \"partial_autocorrelation_abs\",\n        \"partial_autocorrelation\",\n        \"autocorrelation_abs\",\n        \"autocorrelation\",\n    ]:\n        raise ValueError(\n            \"`sort_by` must be 'lag', 'partial_autocorrelation_abs', 'partial_autocorrelation', \"\n            \"'autocorrelation_abs' or 'autocorrelation'.\"\n        )\n\n    series = data.iloc[:, 0] if isinstance(data, pd.DataFrame) else data\n    if series.nunique() &lt;= 1:\n        acf_values = np.full(n_lags + 1, np.nan)\n        acf_values[0] = 1.0\n        pacf_values = np.zeros(n_lags + 1)\n        pacf_values[0] = 1.0\n    else:\n        pacf_values = pacf(data, nlags=n_lags, **pacf_kwargs)\n        acf_values = acf(data, nlags=n_lags, **acf_kwargs)\n\n    results = pd.DataFrame(\n        {\n            \"lag\": range(n_lags + 1),\n            \"partial_autocorrelation_abs\": np.abs(pacf_values),\n            \"partial_autocorrelation\": pacf_values,\n            \"autocorrelation_abs\": np.abs(acf_values),\n            \"autocorrelation\": acf_values,\n        }\n    ).iloc[1:]\n\n    if sort_by == \"lag\":\n        results = results.sort_values(by=sort_by, ascending=True).reset_index(drop=True)\n    else:\n        results = results.sort_values(by=sort_by, ascending=False).reset_index(\n            drop=True\n        )\n\n    return results\n</code></pre>"},{"location":"api/utils/","title":"Utilities Module","text":""},{"location":"api/utils/#validation","title":"Validation","text":""},{"location":"api/utils/#spotforecast2.utils.validation.check_y","title":"<code>spotforecast2.utils.validation.check_y(y, series_id='`y`')</code>","text":"<p>Validate that y is a pandas Series without missing values.</p> <p>This function ensures that the input time series meets the basic requirements for forecasting: it must be a pandas Series and must not contain any NaN values.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Any</code> <p>Time series values to validate.</p> required <code>series_id</code> <code>str</code> <p>Identifier of the series used in error messages. Defaults to \"<code>y</code>\".</p> <code>'`y`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If y is not a pandas Series.</p> <code>ValueError</code> <p>If y contains missing (NaN) values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_y\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid series\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n&gt;&gt;&gt; check_y(y)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not a Series\n&gt;&gt;&gt; try:\n...     check_y([1, 2, 3])\n... except TypeError as e:\n...     print(f\"Error: {e}\")\nError: `y` must be a pandas Series with a DatetimeIndex or a RangeIndex. Found &lt;class 'list'&gt;.\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: contains NaN\n&gt;&gt;&gt; y_with_nan = pd.Series([1, 2, np.nan, 4])\n&gt;&gt;&gt; try:\n...     check_y(y_with_nan)\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: `y` has missing values.\n</code></pre> Source code in <code>src/spotforecast2/utils/validation.py</code> <pre><code>def check_y(y: Any, series_id: str = \"`y`\") -&gt; None:\n    \"\"\"\n    Validate that y is a pandas Series without missing values.\n\n    This function ensures that the input time series meets the basic requirements\n    for forecasting: it must be a pandas Series and must not contain any NaN values.\n\n    Args:\n        y: Time series values to validate.\n        series_id: Identifier of the series used in error messages. Defaults to \"`y`\".\n\n    Raises:\n        TypeError: If y is not a pandas Series.\n        ValueError: If y contains missing (NaN) values.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_y\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid series\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; check_y(y)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not a Series\n        &gt;&gt;&gt; try:\n        ...     check_y([1, 2, 3])\n        ... except TypeError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `y` must be a pandas Series with a DatetimeIndex or a RangeIndex. Found &lt;class 'list'&gt;.\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: contains NaN\n        &gt;&gt;&gt; y_with_nan = pd.Series([1, 2, np.nan, 4])\n        &gt;&gt;&gt; try:\n        ...     check_y(y_with_nan)\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `y` has missing values.\n    \"\"\"\n    if not isinstance(y, pd.Series):\n        raise TypeError(\n            f\"{series_id} must be a pandas Series with a DatetimeIndex or a RangeIndex. \"\n            f\"Found {type(y)}.\"\n        )\n\n    if y.isna().to_numpy().any():\n        raise ValueError(f\"{series_id} has missing values.\")\n\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2.utils.validation.check_exog","title":"<code>spotforecast2.utils.validation.check_exog(exog, allow_nan=True, series_id='`exog`')</code>","text":"<p>Validate that exog is a pandas Series or DataFrame.</p> <p>This function ensures that exogenous variables meet basic requirements: - Must be a pandas Series or DataFrame - If Series, must have a name - Optionally warns if NaN values are present</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variable/s included as predictor/s.</p> required <code>allow_nan</code> <code>bool</code> <p>If True, allows NaN values but issues a warning. If False, raises no warning about NaN values. Defaults to True.</p> <code>True</code> <code>series_id</code> <code>str</code> <p>Identifier of the series used in error messages. Defaults to \"<code>exog</code>\".</p> <code>'`exog`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If exog is not a pandas Series or DataFrame.</p> <code>ValueError</code> <p>If exog is a Series without a name.</p> <p>Warns:</p> Type Description <code>MissingValuesWarning</code> <p>If allow_nan=True and exog contains NaN values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_exog\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid DataFrame\n&gt;&gt;&gt; exog_df = pd.DataFrame({\"temp\": [20, 21, 22], \"humidity\": [50, 55, 60]})\n&gt;&gt;&gt; check_exog(exog_df)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid Series with name\n&gt;&gt;&gt; exog_series = pd.Series([1, 2, 3], name=\"temperature\")\n&gt;&gt;&gt; check_exog(exog_series)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: Series without name\n&gt;&gt;&gt; exog_no_name = pd.Series([1, 2, 3])\n&gt;&gt;&gt; try:\n...     check_exog(exog_no_name)\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: When `exog` is a pandas Series, it must have a name.\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not a Series/DataFrame\n&gt;&gt;&gt; try:\n...     check_exog([1, 2, 3])\n... except TypeError as e:\n...     print(f\"Error: {e}\")\nError: `exog` must be a pandas Series or DataFrame. Got &lt;class 'list'&gt;.\n</code></pre> Source code in <code>src/spotforecast2/utils/validation.py</code> <pre><code>def check_exog(\n    exog: Union[pd.Series, pd.DataFrame],\n    allow_nan: bool = True,\n    series_id: str = \"`exog`\",\n) -&gt; None:\n    \"\"\"\n    Validate that exog is a pandas Series or DataFrame.\n\n    This function ensures that exogenous variables meet basic requirements:\n    - Must be a pandas Series or DataFrame\n    - If Series, must have a name\n    - Optionally warns if NaN values are present\n\n    Args:\n        exog: Exogenous variable/s included as predictor/s.\n        allow_nan: If True, allows NaN values but issues a warning. If False,\n            raises no warning about NaN values. Defaults to True.\n        series_id: Identifier of the series used in error messages. Defaults to \"`exog`\".\n\n    Raises:\n        TypeError: If exog is not a pandas Series or DataFrame.\n        ValueError: If exog is a Series without a name.\n\n    Warnings:\n        MissingValuesWarning: If allow_nan=True and exog contains NaN values.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_exog\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid DataFrame\n        &gt;&gt;&gt; exog_df = pd.DataFrame({\"temp\": [20, 21, 22], \"humidity\": [50, 55, 60]})\n        &gt;&gt;&gt; check_exog(exog_df)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid Series with name\n        &gt;&gt;&gt; exog_series = pd.Series([1, 2, 3], name=\"temperature\")\n        &gt;&gt;&gt; check_exog(exog_series)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: Series without name\n        &gt;&gt;&gt; exog_no_name = pd.Series([1, 2, 3])\n        &gt;&gt;&gt; try:\n        ...     check_exog(exog_no_name)\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: When `exog` is a pandas Series, it must have a name.\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not a Series/DataFrame\n        &gt;&gt;&gt; try:\n        ...     check_exog([1, 2, 3])\n        ... except TypeError as e:\n        ...     print(f\"Error: {e}\")\n        Error: `exog` must be a pandas Series or DataFrame. Got &lt;class 'list'&gt;.\n    \"\"\"\n    if not isinstance(exog, (pd.Series, pd.DataFrame)):\n        raise TypeError(\n            f\"{series_id} must be a pandas Series or DataFrame. Got {type(exog)}.\"\n        )\n\n    if isinstance(exog, pd.Series) and exog.name is None:\n        raise ValueError(f\"When {series_id} is a pandas Series, it must have a name.\")\n\n    if not allow_nan:\n        if exog.isna().to_numpy().any():\n            warnings.warn(\n                f\"{series_id} has missing values. Most machine learning models \"\n                f\"do not allow missing values. Fitting the forecaster may fail.\",\n                MissingValuesWarning,\n            )\n\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2.utils.validation.get_exog_dtypes","title":"<code>spotforecast2.utils.validation.get_exog_dtypes(exog)</code>","text":"<p>Extract and store the data types of exogenous variables.</p> <p>This function returns a dictionary mapping column names to their data types. For Series, uses the series name as the key. For DataFrames, uses all column names.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variable/s (Series or DataFrame).</p> required <p>Returns:</p> Type Description <code>Dict[str, type]</code> <p>Dictionary mapping variable names to their pandas dtypes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import get_exog_dtypes\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DataFrame with mixed types\n&gt;&gt;&gt; exog_df = pd.DataFrame({\n...     \"temp\": pd.Series([20.5, 21.3, 22.1], dtype='float64'),\n...     \"day\": pd.Series([1, 2, 3], dtype='int64'),\n...     \"is_weekend\": pd.Series([False, False, True], dtype='bool')\n... })\n&gt;&gt;&gt; dtypes = get_exog_dtypes(exog_df)\n&gt;&gt;&gt; dtypes['temp']\ndtype('float64')\n&gt;&gt;&gt; dtypes['day']\ndtype('int64')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series\n&gt;&gt;&gt; exog_series = pd.Series([1.0, 2.0, 3.0], name=\"temperature\", dtype='float64')\n&gt;&gt;&gt; dtypes = get_exog_dtypes(exog_series)\n&gt;&gt;&gt; dtypes\n{'temperature': dtype('float64')}\n</code></pre> Source code in <code>src/spotforecast2/utils/validation.py</code> <pre><code>def get_exog_dtypes(exog: Union[pd.Series, pd.DataFrame]) -&gt; Dict[str, type]:\n    \"\"\"\n    Extract and store the data types of exogenous variables.\n\n    This function returns a dictionary mapping column names to their data types.\n    For Series, uses the series name as the key. For DataFrames, uses all column names.\n\n    Args:\n        exog: Exogenous variable/s (Series or DataFrame).\n\n    Returns:\n        Dictionary mapping variable names to their pandas dtypes.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import get_exog_dtypes\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DataFrame with mixed types\n        &gt;&gt;&gt; exog_df = pd.DataFrame({\n        ...     \"temp\": pd.Series([20.5, 21.3, 22.1], dtype='float64'),\n        ...     \"day\": pd.Series([1, 2, 3], dtype='int64'),\n        ...     \"is_weekend\": pd.Series([False, False, True], dtype='bool')\n        ... })\n        &gt;&gt;&gt; dtypes = get_exog_dtypes(exog_df)\n        &gt;&gt;&gt; dtypes['temp']\n        dtype('float64')\n        &gt;&gt;&gt; dtypes['day']\n        dtype('int64')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series\n        &gt;&gt;&gt; exog_series = pd.Series([1.0, 2.0, 3.0], name=\"temperature\", dtype='float64')\n        &gt;&gt;&gt; dtypes = get_exog_dtypes(exog_series)\n        &gt;&gt;&gt; dtypes\n        {'temperature': dtype('float64')}\n    \"\"\"\n    if isinstance(exog, pd.Series):\n        exog_dtypes = {exog.name: exog.dtypes}\n    else:\n        exog_dtypes = exog.dtypes.to_dict()\n\n    return exog_dtypes\n</code></pre>"},{"location":"api/utils/#spotforecast2.utils.validation.check_exog_dtypes","title":"<code>spotforecast2.utils.validation.check_exog_dtypes(exog, call_check_exog=True, series_id='`exog`')</code>","text":"<p>Check that exogenous variables have valid data types (int, float, category).</p> <p>This function validates that the exogenous variables (Series or DataFrame) contain only supported data types: integer, float, or category. It issues a warning if other types (like object/string) are found, as these may cause issues with some machine learning estimators.</p> <p>It also strictly enforces that categorical columns must have integer categories.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Union[Series, DataFrame]</code> <p>Exogenous variables to check.</p> required <code>call_check_exog</code> <code>bool</code> <p>If True, calls check_exog() first to ensure basic validity. Defaults to True.</p> <code>True</code> <code>series_id</code> <code>str</code> <p>Identifier used in warning/error messages. Defaults to \"<code>exog</code>\".</p> <code>'`exog`'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If categorical columns contain non-integer categories.</p> <p>Warns:</p> Type Description <code>DataTypeWarning</code> <p>If columns with unsupported data types (not int, float, category) are found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.validation import check_exog_dtypes\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid types (float, int)\n&gt;&gt;&gt; df_valid = pd.DataFrame({\n...     \"a\": [1.0, 2.0, 3.0],\n...     \"b\": [1, 2, 3]\n... })\n&gt;&gt;&gt; check_exog_dtypes(df_valid)  # No warning\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid type (object/string)\n&gt;&gt;&gt; df_invalid = pd.DataFrame({\n...     \"a\": [1, 2, 3],\n...     \"b\": [\"x\", \"y\", \"z\"]\n... })\n&gt;&gt;&gt; check_exog_dtypes(df_invalid)\n... # Issues DataTypeWarning about column 'b'\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid categorical (with integer categories)\n&gt;&gt;&gt; df_cat = pd.DataFrame({\"a\": [1, 2, 1]})\n&gt;&gt;&gt; df_cat[\"a\"] = df_cat[\"a\"].astype(\"category\")\n&gt;&gt;&gt; check_exog_dtypes(df_cat)  # No warning\n</code></pre> Source code in <code>src/spotforecast2/utils/validation.py</code> <pre><code>def check_exog_dtypes(\n    exog: Union[pd.Series, pd.DataFrame],\n    call_check_exog: bool = True,\n    series_id: str = \"`exog`\",\n) -&gt; None:\n    \"\"\"\n    Check that exogenous variables have valid data types (int, float, category).\n\n    This function validates that the exogenous variables (Series or DataFrame)\n    contain only supported data types: integer, float, or category. It issues a\n    warning if other types (like object/string) are found, as these may cause\n    issues with some machine learning estimators.\n\n    It also strictly enforces that categorical columns must have integer categories.\n\n    Args:\n        exog: Exogenous variables to check.\n        call_check_exog: If True, calls check_exog() first to ensure basic validity.\n            Defaults to True.\n        series_id: Identifier used in warning/error messages. Defaults to \"`exog`\".\n\n    Raises:\n        TypeError: If categorical columns contain non-integer categories.\n\n    Warnings:\n        DataTypeWarning: If columns with unsupported data types (not int, float, category)\n            are found.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_exog_dtypes\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid types (float, int)\n        &gt;&gt;&gt; df_valid = pd.DataFrame({\n        ...     \"a\": [1.0, 2.0, 3.0],\n        ...     \"b\": [1, 2, 3]\n        ... })\n        &gt;&gt;&gt; check_exog_dtypes(df_valid)  # No warning\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid type (object/string)\n        &gt;&gt;&gt; df_invalid = pd.DataFrame({\n        ...     \"a\": [1, 2, 3],\n        ...     \"b\": [\"x\", \"y\", \"z\"]\n        ... })\n        &gt;&gt;&gt; check_exog_dtypes(df_invalid)\n        ... # Issues DataTypeWarning about column 'b'\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid categorical (with integer categories)\n        &gt;&gt;&gt; df_cat = pd.DataFrame({\"a\": [1, 2, 1]})\n        &gt;&gt;&gt; df_cat[\"a\"] = df_cat[\"a\"].astype(\"category\")\n        &gt;&gt;&gt; check_exog_dtypes(df_cat)  # No warning\n    \"\"\"\n    if call_check_exog:\n        check_exog(exog=exog, allow_nan=False, series_id=series_id)\n\n    valid_dtypes = (\"int\", \"Int\", \"float\", \"Float\", \"uint\")\n\n    if isinstance(exog, pd.DataFrame):\n        unique_dtypes = set(exog.dtypes)\n        has_invalid_dtype = False\n        for dtype in unique_dtypes:\n            if isinstance(dtype, pd.CategoricalDtype):\n                try:\n                    is_integer = np.issubdtype(dtype.categories.dtype, np.integer)\n                except TypeError:\n                    # Pandas StringDtype and other non-numpy dtypes will raise TypeError\n                    is_integer = False\n\n                if not is_integer:\n                    raise TypeError(\n                        \"Categorical dtypes in exog must contain only integer values. \"\n                    )\n            elif not dtype.name.startswith(valid_dtypes):\n                has_invalid_dtype = True\n\n        if has_invalid_dtype:\n            warnings.warn(\n                f\"{series_id} may contain only `int`, `float` or `category` dtypes. \"\n                f\"Most machine learning models do not allow other types of values. \"\n                f\"Fitting the forecaster may fail.\",\n                DataTypeWarning,\n            )\n\n    else:\n        dtype_name = str(exog.dtypes)\n        if not (dtype_name.startswith(valid_dtypes) or dtype_name == \"category\"):\n            warnings.warn(\n                f\"{series_id} may contain only `int`, `float` or `category` dtypes. Most \"\n                f\"machine learning models do not allow other types of values. \"\n                f\"Fitting the forecaster may fail.\",\n                DataTypeWarning,\n            )\n\n        if isinstance(exog.dtype, pd.CategoricalDtype):\n            if not np.issubdtype(exog.cat.categories.dtype, np.integer):\n                raise TypeError(\n                    \"Categorical dtypes in exog must contain only integer values. \"\n                )\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2.utils.validation.check_interval","title":"<code>spotforecast2.utils.validation.check_interval(interval=None, ensure_symmetric_intervals=False, quantiles=None, alpha=None, alpha_literal='alpha')</code>","text":"<p>Validate that a confidence interval specification is valid.</p> <p>This function checks that interval values are properly formatted and within valid ranges for confidence interval prediction.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>Union[List[float], Tuple[float], None]</code> <p>Confidence interval percentiles (0-100 inclusive). Should be [lower_bound, upper_bound]. Example: [2.5, 97.5] for 95% interval.</p> <code>None</code> <code>ensure_symmetric_intervals</code> <code>bool</code> <p>If True, ensure intervals are symmetric (lower + upper = 100).</p> <code>False</code> <code>quantiles</code> <code>Union[List[float], Tuple[float], None]</code> <p>Sequence of quantiles (0-1 inclusive). Currently not validated, reserved for future use.</p> <code>None</code> <code>alpha</code> <code>Optional[float]</code> <p>Confidence level (1-alpha). Currently not validated, reserved for future use.</p> <code>None</code> <code>alpha_literal</code> <code>Optional[str]</code> <p>Name used in error messages for alpha parameter.</p> <code>'alpha'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If interval is not a list or tuple.</p> <code>ValueError</code> <p>If interval doesn't have exactly 2 values, values out of range (0-100), lower &gt;= upper, or intervals not symmetric when required.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2.utils.validation import check_interval\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid 95% confidence interval\n&gt;&gt;&gt; check_interval(interval=[2.5, 97.5])  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Valid symmetric interval\n&gt;&gt;&gt; check_interval(interval=[2.5, 97.5], ensure_symmetric_intervals=True)  # No error\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: not symmetric\n&gt;&gt;&gt; try:\n...     check_interval(interval=[5, 90], ensure_symmetric_intervals=True)\n... except ValueError as e:\n...     print(\"Error: Interval not symmetric\")\nError: Interval not symmetric\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: wrong number of values\n&gt;&gt;&gt; try:\n...     check_interval(interval=[2.5, 50, 97.5])\n... except ValueError as e:\n...     print(\"Error: Must have exactly 2 values\")\nError: Must have exactly 2 values\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: out of range\n&gt;&gt;&gt; try:\n...     check_interval(interval=[-5, 105])\n... except ValueError as e:\n...     print(\"Error: Values out of range\")\nError: Values out of range\n</code></pre> Source code in <code>src/spotforecast2/utils/validation.py</code> <pre><code>def check_interval(\n    interval: Union[List[float], Tuple[float], None] = None,\n    ensure_symmetric_intervals: bool = False,\n    quantiles: Union[List[float], Tuple[float], None] = None,\n    alpha: Optional[float] = None,\n    alpha_literal: Optional[str] = \"alpha\",\n) -&gt; None:\n    \"\"\"\n    Validate that a confidence interval specification is valid.\n\n    This function checks that interval values are properly formatted and within\n    valid ranges for confidence interval prediction.\n\n    Args:\n        interval: Confidence interval percentiles (0-100 inclusive).\n            Should be [lower_bound, upper_bound]. Example: [2.5, 97.5] for 95% interval.\n        ensure_symmetric_intervals: If True, ensure intervals are symmetric\n            (lower + upper = 100).\n        quantiles: Sequence of quantiles (0-1 inclusive). Currently not validated,\n            reserved for future use.\n        alpha: Confidence level (1-alpha). Currently not validated, reserved for future use.\n        alpha_literal: Name used in error messages for alpha parameter.\n\n    Raises:\n        TypeError: If interval is not a list or tuple.\n        ValueError: If interval doesn't have exactly 2 values, values out of range (0-100),\n            lower &gt;= upper, or intervals not symmetric when required.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2.utils.validation import check_interval\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid 95% confidence interval\n        &gt;&gt;&gt; check_interval(interval=[2.5, 97.5])  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Valid symmetric interval\n        &gt;&gt;&gt; check_interval(interval=[2.5, 97.5], ensure_symmetric_intervals=True)  # No error\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: not symmetric\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[5, 90], ensure_symmetric_intervals=True)\n        ... except ValueError as e:\n        ...     print(\"Error: Interval not symmetric\")\n        Error: Interval not symmetric\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: wrong number of values\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[2.5, 50, 97.5])\n        ... except ValueError as e:\n        ...     print(\"Error: Must have exactly 2 values\")\n        Error: Must have exactly 2 values\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: out of range\n        &gt;&gt;&gt; try:\n        ...     check_interval(interval=[-5, 105])\n        ... except ValueError as e:\n        ...     print(\"Error: Values out of range\")\n        Error: Values out of range\n    \"\"\"\n    if interval is not None:\n        if not isinstance(interval, (list, tuple)):\n            raise TypeError(\n                \"`interval` must be a `list` or `tuple`. For example, interval of 95% \"\n                \"should be as `interval = [2.5, 97.5]`.\"\n            )\n\n        if len(interval) != 2:\n            raise ValueError(\n                \"`interval` must contain exactly 2 values, respectively the \"\n                \"lower and upper interval bounds. For example, interval of 95% \"\n                \"should be as `interval = [2.5, 97.5]`.\"\n            )\n\n        if (interval[0] &lt; 0.0) or (interval[0] &gt;= 100.0):\n            raise ValueError(\n                f\"Lower interval bound ({interval[0]}) must be &gt;= 0 and &lt; 100.\"\n            )\n\n        if (interval[1] &lt;= 0.0) or (interval[1] &gt; 100.0):\n            raise ValueError(\n                f\"Upper interval bound ({interval[1]}) must be &gt; 0 and &lt;= 100.\"\n            )\n\n        if interval[0] &gt;= interval[1]:\n            raise ValueError(\n                f\"Lower interval bound ({interval[0]}) must be less than the \"\n                f\"upper interval bound ({interval[1]}).\"\n            )\n\n        if ensure_symmetric_intervals and interval[0] + interval[1] != 100:\n            raise ValueError(\n                f\"Interval must be symmetric, the sum of the lower, ({interval[0]}), \"\n                f\"and upper, ({interval[1]}), interval bounds must be equal to \"\n                f\"100. Got {interval[0] + interval[1]}.\"\n            )\n\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2.utils.check_predict_input","title":"<code>spotforecast2.utils.check_predict_input(forecaster_name, steps, is_fitted, exog_in_, index_type_, index_freq_, window_size, last_window, last_window_exog=None, exog=None, exog_names_in_=None, interval=None, alpha=None, max_step=None, levels=None, levels_forecaster=None, series_names_in_=None, encoding=None)</code>","text":"<p>Check all inputs of predict method. This is a helper function to validate that inputs used in predict method match attributes of a forecaster already trained.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>str Forecaster name.</p> required <code>steps</code> <code>Union[int, List[int]]</code> <p>int, list Number of future steps predicted.</p> required <code>is_fitted</code> <code>bool</code> <p>bool Tag to identify if the estimator has been fitted (trained).</p> required <code>exog_in_</code> <code>bool</code> <p>bool If the forecaster has been trained using exogenous variable/s.</p> required <code>index_type_</code> <code>type</code> <p>type Type of index of the input used in training.</p> required <code>index_freq_</code> <code>str</code> <p>str Frequency of Index of the input used in training.</p> required <code>window_size</code> <code>int</code> <p>int Size of the window needed to create the predictors. It is equal to <code>max_lag</code>.</p> required <code>last_window</code> <code>Optional[Union[Series, DataFrame]]</code> <p>pandas Series, pandas DataFrame, None Values of the series used to create the predictors (lags) need in the first iteration of prediction (t + 1).</p> required <code>last_window_exog</code> <code>Optional[Union[Series, DataFrame]]</code> <p>pandas Series, pandas DataFrame, default None Values of the exogenous variables aligned with <code>last_window</code> in ForecasterStats predictions.</p> <code>None</code> <code>exog</code> <code>Optional[Union[Series, DataFrame, Dict[str, Union[Series, DataFrame]]]]</code> <p>pandas Series, pandas DataFrame, dict, default None Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>exog_names_in_</code> <code>Optional[List[str]]</code> <p>list, default None Names of the exogenous variables used during training.</p> <code>None</code> <code>interval</code> <code>Optional[List[float]]</code> <p>list, tuple, default None Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. For example, interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>None</code> <code>alpha</code> <code>Optional[float]</code> <p>float, default None The confidence intervals used in ForecasterStats are (1 - alpha) %.</p> <code>None</code> <code>max_step</code> <code>Optional[int]</code> <p>int, default None Maximum number of steps allowed (<code>ForecasterDirect</code> and <code>ForecasterDirectMultiVariate</code>).</p> <code>None</code> <code>levels</code> <code>Optional[Union[str, List[str]]]</code> <p>str, list, default None Time series to be predicted (<code>ForecasterRecursiveMultiSeries</code> and `ForecasterRnn).</p> <code>None</code> <code>levels_forecaster</code> <code>Optional[Union[str, List[str]]]</code> <p>str, list, default None Time series used as output data of a multiseries problem in a RNN problem (<code>ForecasterRnn</code>).</p> <code>None</code> <code>series_names_in_</code> <code>Optional[List[str]]</code> <p>list, default None Names of the columns used during fit (<code>ForecasterRecursiveMultiSeries</code>, <code>ForecasterDirectMultiVariate</code> and <code>ForecasterRnn</code>).</p> <code>None</code> <code>encoding</code> <code>Optional[str]</code> <p>str, default None Encoding used to identify the different series (<code>ForecasterRecursiveMultiSeries</code>).</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spotforecast2_safe/utils/validation.py</code> <pre><code>def check_predict_input(\n    forecaster_name: str,\n    steps: Union[int, List[int]],\n    is_fitted: bool,\n    exog_in_: bool,\n    index_type_: type,\n    index_freq_: str,\n    window_size: int,\n    last_window: Optional[Union[pd.Series, pd.DataFrame]],\n    last_window_exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    exog: Optional[\n        Union[pd.Series, pd.DataFrame, Dict[str, Union[pd.Series, pd.DataFrame]]]\n    ] = None,\n    exog_names_in_: Optional[List[str]] = None,\n    interval: Optional[List[float]] = None,\n    alpha: Optional[float] = None,\n    max_step: Optional[int] = None,\n    levels: Optional[Union[str, List[str]]] = None,\n    levels_forecaster: Optional[Union[str, List[str]]] = None,\n    series_names_in_: Optional[List[str]] = None,\n    encoding: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Check all inputs of predict method. This is a helper function to validate\n    that inputs used in predict method match attributes of a forecaster already\n    trained.\n\n    Args:\n        forecaster_name: str\n            Forecaster name.\n        steps: int, list\n            Number of future steps predicted.\n        is_fitted: bool\n            Tag to identify if the estimator has been fitted (trained).\n        exog_in_: bool\n            If the forecaster has been trained using exogenous variable/s.\n        index_type_: type\n            Type of index of the input used in training.\n        index_freq_: str\n            Frequency of Index of the input used in training.\n        window_size: int\n            Size of the window needed to create the predictors. It is equal to\n            `max_lag`.\n        last_window: pandas Series, pandas DataFrame, None\n            Values of the series used to create the predictors (lags) need in the\n            first iteration of prediction (t + 1).\n        last_window_exog: pandas Series, pandas DataFrame, default None\n            Values of the exogenous variables aligned with `last_window` in\n            ForecasterStats predictions.\n        exog: pandas Series, pandas DataFrame, dict, default None\n            Exogenous variable/s included as predictor/s.\n        exog_names_in_: list, default None\n            Names of the exogenous variables used during training.\n        interval: list, tuple, default None\n            Confidence of the prediction interval estimated. Sequence of percentiles\n            to compute, which must be between 0 and 100 inclusive. For example,\n            interval of 95% should be as `interval = [2.5, 97.5]`.\n        alpha: float, default None\n            The confidence intervals used in ForecasterStats are (1 - alpha) %.\n        max_step: int, default None\n            Maximum number of steps allowed (`ForecasterDirect` and\n            `ForecasterDirectMultiVariate`).\n        levels: str, list, default None\n            Time series to be predicted (`ForecasterRecursiveMultiSeries`\n            and `ForecasterRnn).\n        levels_forecaster: str, list, default None\n            Time series used as output data of a multiseries problem in a RNN problem\n            (`ForecasterRnn`).\n        series_names_in_: list, default None\n            Names of the columns used during fit (`ForecasterRecursiveMultiSeries`,\n            `ForecasterDirectMultiVariate` and `ForecasterRnn`).\n        encoding: str, default None\n            Encoding used to identify the different series (`ForecasterRecursiveMultiSeries`).\n\n    Returns:\n        None\n    \"\"\"\n\n    if not is_fitted:\n        raise RuntimeError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `predict`.\"\n        )\n\n    if isinstance(steps, (int, np.integer)) and steps &lt; 1:\n        raise ValueError(\n            f\"`steps` must be an integer greater than or equal to 1. Got {steps}.\"\n        )\n\n    if isinstance(steps, list) and min(steps) &lt; 1:\n        raise ValueError(\n            f\"`steps` must be a list of integers greater than or equal to 1. Got {steps}.\"\n        )\n\n    if max_step is not None:\n        if isinstance(steps, (int, np.integer)):\n            if steps &gt; max_step:\n                raise ValueError(\n                    f\"The maximum step that can be predicted is {max_step}. \"\n                    f\"Got {steps}.\"\n                )\n        elif isinstance(steps, list):\n            if max(steps) &gt; max_step:\n                raise ValueError(\n                    f\"The maximum step that can be predicted is {max_step}. \"\n                    f\"Got {max(steps)}.\"\n                )\n\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n\n    if exog_in_ and exog is None:\n        raise ValueError(\n            \"Forecaster trained with exogenous variable/s. \"\n            \"Same variable/s must be provided when predicting.\"\n        )\n\n    if not exog_in_ and exog is not None:\n        raise ValueError(\n            \"Forecaster trained without exogenous variable/s. \"\n            \"`exog` must be `None` when predicting.\"\n        )\n\n    if exog is not None:\n        # If exog is a dictionary, it is assumed that it contains the exogenous\n        # variables for each series.\n        if isinstance(exog, dict):\n            # Check that all series have the exogenous variables\n            if levels is None and series_names_in_ is not None:\n                levels = series_names_in_\n\n            if isinstance(levels, str):\n                levels = [levels]\n\n            if levels is not None:\n                for level in levels:\n                    if level not in exog:\n                        raise ValueError(\n                            f\"Exogenous variables for series '{level}' are missing.\"\n                        )\n                    check_exog(\n                        exog=exog[level],\n                        allow_nan=False,\n                        series_id=f\"`exog` for series '{level}'\",\n                    )\n                    check_exog_dtypes(\n                        exog=exog[level],\n                        call_check_exog=False,\n                        series_id=f\"`exog` for series '{level}'\",\n                    )\n\n                    # Check that exogenous variables are the same as used in training\n                    # Get the name of columns\n                    if isinstance(exog[level], pd.Series):\n                        exog_names = [exog[level].name]\n                    else:\n                        exog_names = exog[level].columns.tolist()\n\n                    if len(set(exog_names) - set(exog_names_in_)) &gt; 0:\n                        raise ValueError(\n                            f\"Exogenous variables must be: {exog_names_in_}. \"\n                            f\"Got {exog_names} for series '{level}'.\"\n                        )\n        else:\n            check_exog(exog=exog, allow_nan=False)\n            check_exog_dtypes(exog=exog, call_check_exog=False)\n\n            # Check that exogenous variables are the same as used in training\n            # Get the name of columns\n            if isinstance(exog, pd.Series):\n                exog_names = [exog.name]\n            else:\n                exog_names = exog.columns.tolist()\n\n            if len(set(exog_names) - set(exog_names_in_)) &gt; 0:\n                raise ValueError(\n                    f\"Exogenous variables must be: {exog_names_in_}. Got {exog_names}.\"\n                )\n\n    # Check last_window\n    if last_window is not None:\n        if isinstance(last_window, pd.DataFrame):\n            if last_window.isna().to_numpy().any():\n                raise ValueError(\"`last_window` has missing values.\")\n        else:\n            check_y(last_window, series_id=\"`last_window`\")\n\n    return\n</code></pre>"},{"location":"api/utils/#spotforecast2.utils.validation.MissingValuesWarning","title":"<code>spotforecast2.utils.validation.MissingValuesWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for missing values in data.</p> <p>Used to indicate that there are missing values in the data. This warning occurs when the input data contains missing values, or the training matrix generates missing values. Most machine learning models do not accept missing values, so the Forecaster's <code>fit' and</code>predict' methods may fail.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Missing values detected in input data.\",\n...     MissingValuesWarning\n... )\n</code></pre> Source code in <code>spotforecast2_safe/exceptions.py</code> <pre><code>class MissingValuesWarning(UserWarning):\n    \"\"\"Warning for missing values in data.\n\n    Used to indicate that there are missing values in the data. This\n    warning occurs when the input data contains missing values, or the training\n    matrix generates missing values. Most machine learning models do not accept\n    missing values, so the Forecaster's `fit' and `predict' methods may fail.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Missing values detected in input data.\",\n        ...     MissingValuesWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=MissingValuesWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/utils/#spotforecast2.utils.validation.DataTypeWarning","title":"<code>spotforecast2.utils.validation.DataTypeWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning for incompatible data types in exogenous data.</p> <p>Used to notify there are dtypes in the exogenous data that are not 'int', 'float', 'bool' or 'category'. Most machine learning models do not accept other data types, therefore the forecaster <code>fit</code> and <code>predict</code> may fail.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; warnings.warn(\n...     \"Exogenous data contains unsupported dtypes.\",\n...     DataTypeWarning\n... )\n</code></pre> Source code in <code>spotforecast2_safe/exceptions.py</code> <pre><code>class DataTypeWarning(UserWarning):\n    \"\"\"Warning for incompatible data types in exogenous data.\n\n    Used to notify there are dtypes in the exogenous data that are not\n    'int', 'float', 'bool' or 'category'. Most machine learning models do not\n    accept other data types, therefore the forecaster `fit` and `predict` may fail.\n\n    Examples:\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.warn(\n        ...     \"Exogenous data contains unsupported dtypes.\",\n        ...     DataTypeWarning\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        extra_message = (\n            \"You can suppress this warning using: \"\n            \"warnings.simplefilter('ignore', category=DataTypeWarning)\"\n        )\n        return self.message + \"\\\\n\" + extra_message\n</code></pre>"},{"location":"api/utils/#data-transform","title":"Data Transform","text":""},{"location":"api/utils/#spotforecast2.utils.data_transform.input_to_frame","title":"<code>spotforecast2.utils.data_transform.input_to_frame(data, input_name)</code>","text":"<p>Convert input data to a pandas DataFrame.</p> <p>This function ensures consistent DataFrame format for internal processing. If data is already a DataFrame, it's returned as-is. If it's a Series, it's converted to a single-column DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Series, DataFrame]</code> <p>Input data as pandas Series or DataFrame.</p> required <code>input_name</code> <code>str</code> <p>Name of the input data type. Accepted values are: - 'y': Target time series - 'last_window': Last window for prediction - 'exog': Exogenous variables</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame version of the input data. For Series input, uses the series</p> <code>DataFrame</code> <p>name if available, otherwise uses a default name based on input_name.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2.utils.data_transform import input_to_frame\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series with name\n&gt;&gt;&gt; y = pd.Series([1, 2, 3], name=\"sales\")\n&gt;&gt;&gt; df = input_to_frame(y, input_name=\"y\")\n&gt;&gt;&gt; df.columns.tolist()\n['sales']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Series without name (uses default)\n&gt;&gt;&gt; y_no_name = pd.Series([1, 2, 3])\n&gt;&gt;&gt; df = input_to_frame(y_no_name, input_name=\"y\")\n&gt;&gt;&gt; df.columns.tolist()\n['y']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DataFrame (returned as-is)\n&gt;&gt;&gt; df_input = pd.DataFrame({\"temp\": [20, 21], \"humidity\": [50, 55]})\n&gt;&gt;&gt; df_output = input_to_frame(df_input, input_name=\"exog\")\n&gt;&gt;&gt; df_output.columns.tolist()\n['temp', 'humidity']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Exog series without name\n&gt;&gt;&gt; exog = pd.Series([10, 20, 30])\n&gt;&gt;&gt; df_exog = input_to_frame(exog, input_name=\"exog\")\n&gt;&gt;&gt; df_exog.columns.tolist()\n['exog']\n</code></pre> Source code in <code>src/spotforecast2/utils/data_transform.py</code> <pre><code>def input_to_frame(\n    data: Union[pd.Series, pd.DataFrame], input_name: str\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert input data to a pandas DataFrame.\n\n    This function ensures consistent DataFrame format for internal processing.\n    If data is already a DataFrame, it's returned as-is. If it's a Series,\n    it's converted to a single-column DataFrame.\n\n    Args:\n        data: Input data as pandas Series or DataFrame.\n        input_name: Name of the input data type. Accepted values are:\n            - 'y': Target time series\n            - 'last_window': Last window for prediction\n            - 'exog': Exogenous variables\n\n    Returns:\n        DataFrame version of the input data. For Series input, uses the series\n        name if available, otherwise uses a default name based on input_name.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2.utils.data_transform import input_to_frame\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series with name\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3], name=\"sales\")\n        &gt;&gt;&gt; df = input_to_frame(y, input_name=\"y\")\n        &gt;&gt;&gt; df.columns.tolist()\n        ['sales']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Series without name (uses default)\n        &gt;&gt;&gt; y_no_name = pd.Series([1, 2, 3])\n        &gt;&gt;&gt; df = input_to_frame(y_no_name, input_name=\"y\")\n        &gt;&gt;&gt; df.columns.tolist()\n        ['y']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DataFrame (returned as-is)\n        &gt;&gt;&gt; df_input = pd.DataFrame({\"temp\": [20, 21], \"humidity\": [50, 55]})\n        &gt;&gt;&gt; df_output = input_to_frame(df_input, input_name=\"exog\")\n        &gt;&gt;&gt; df_output.columns.tolist()\n        ['temp', 'humidity']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Exog series without name\n        &gt;&gt;&gt; exog = pd.Series([10, 20, 30])\n        &gt;&gt;&gt; df_exog = input_to_frame(exog, input_name=\"exog\")\n        &gt;&gt;&gt; df_exog.columns.tolist()\n        ['exog']\n    \"\"\"\n    output_col_name = {\"y\": \"y\", \"last_window\": \"y\", \"exog\": \"exog\"}\n\n    if isinstance(data, pd.Series):\n        data = data.to_frame(\n            name=data.name if data.name is not None else output_col_name[input_name]\n        )\n\n    return data\n</code></pre>"},{"location":"api/utils/#spotforecast2.utils.data_transform.expand_index","title":"<code>spotforecast2.utils.data_transform.expand_index(index, steps)</code>","text":"<p>Create a new index extending from the end of the original index.</p> <p>This function generates future indices for forecasting by extending the time series index by a specified number of steps. Handles both DatetimeIndex and RangeIndex appropriately.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Union[Index, None]</code> <p>Original pandas Index (DatetimeIndex or RangeIndex). If None, creates a RangeIndex starting from 0.</p> required <code>steps</code> <code>int</code> <p>Number of future steps to generate.</p> required <p>Returns:</p> Type Description <code>Index</code> <p>New pandas Index with <code>steps</code> future periods.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If steps is not an integer, or if index is neither DatetimeIndex nor RangeIndex.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotforecast2.utils.data_transform import expand_index\n&gt;&gt;&gt;\n&gt;&gt;&gt; # DatetimeIndex\n&gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=5, freq=\"D\")\n&gt;&gt;&gt; new_index = expand_index(dates, 3)\n&gt;&gt;&gt; new_index\nDatetimeIndex(['2023-01-06', '2023-01-07', '2023-01-08'], dtype='datetime64[ns]', freq='D')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # RangeIndex\n&gt;&gt;&gt; range_idx = pd.RangeIndex(start=0, stop=10)\n&gt;&gt;&gt; new_index = expand_index(range_idx, 5)\n&gt;&gt;&gt; new_index\nRangeIndex(start=10, stop=15, step=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # None index (creates new RangeIndex)\n&gt;&gt;&gt; new_index = expand_index(None, 3)\n&gt;&gt;&gt; new_index\nRangeIndex(start=0, stop=3, step=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: steps not an integer\n&gt;&gt;&gt; try:\n...     expand_index(dates, 3.5)\n... except TypeError as e:\n...     print(\"Error: steps must be an integer\")\nError: steps must be an integer\n</code></pre> Source code in <code>src/spotforecast2/utils/data_transform.py</code> <pre><code>def expand_index(index: Union[pd.Index, None], steps: int) -&gt; pd.Index:\n    \"\"\"\n    Create a new index extending from the end of the original index.\n\n    This function generates future indices for forecasting by extending the time\n    series index by a specified number of steps. Handles both DatetimeIndex and\n    RangeIndex appropriately.\n\n    Args:\n        index: Original pandas Index (DatetimeIndex or RangeIndex). If None,\n            creates a RangeIndex starting from 0.\n        steps: Number of future steps to generate.\n\n    Returns:\n        New pandas Index with `steps` future periods.\n\n    Raises:\n        TypeError: If steps is not an integer, or if index is neither DatetimeIndex\n            nor RangeIndex.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotforecast2.utils.data_transform import expand_index\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # DatetimeIndex\n        &gt;&gt;&gt; dates = pd.date_range(\"2023-01-01\", periods=5, freq=\"D\")\n        &gt;&gt;&gt; new_index = expand_index(dates, 3)\n        &gt;&gt;&gt; new_index\n        DatetimeIndex(['2023-01-06', '2023-01-07', '2023-01-08'], dtype='datetime64[ns]', freq='D')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # RangeIndex\n        &gt;&gt;&gt; range_idx = pd.RangeIndex(start=0, stop=10)\n        &gt;&gt;&gt; new_index = expand_index(range_idx, 5)\n        &gt;&gt;&gt; new_index\n        RangeIndex(start=10, stop=15, step=1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # None index (creates new RangeIndex)\n        &gt;&gt;&gt; new_index = expand_index(None, 3)\n        &gt;&gt;&gt; new_index\n        RangeIndex(start=0, stop=3, step=1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: steps not an integer\n        &gt;&gt;&gt; try:\n        ...     expand_index(dates, 3.5)\n        ... except TypeError as e:\n        ...     print(\"Error: steps must be an integer\")\n        Error: steps must be an integer\n    \"\"\"\n    if not isinstance(steps, (int, np.integer)):\n        raise TypeError(f\"`steps` must be an integer. Got {type(steps)}.\")\n\n    # Convert numpy integer to Python int if needed\n    if isinstance(steps, np.integer):\n        steps = int(steps)\n\n    if isinstance(index, pd.Index):\n        if isinstance(index, pd.DatetimeIndex):\n            new_index = pd.date_range(\n                start=index[-1] + index.freq, periods=steps, freq=index.freq\n            )\n        elif isinstance(index, pd.RangeIndex):\n            new_index = pd.RangeIndex(start=index[-1] + 1, stop=index[-1] + 1 + steps)\n        else:\n            raise TypeError(\n                \"Argument `index` must be a pandas DatetimeIndex or RangeIndex.\"\n            )\n    else:\n        new_index = pd.RangeIndex(start=0, stop=steps)\n\n    return new_index\n</code></pre>"},{"location":"api/utils/#spotforecast2.utils.transform_dataframe","title":"<code>spotforecast2.utils.transform_dataframe(df, transformer, fit=False, inverse_transform=False)</code>","text":"<p>Transform raw values of pandas DataFrame with a scikit-learn alike transformer, preprocessor or ColumnTransformer.</p> <p>The transformer used must have the following methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to be transformed.</p> required <code>transformer</code> <code>object</code> <p>Scikit-learn alike transformer, preprocessor, or ColumnTransformer. Must implement fit, transform, fit_transform and inverse_transform.</p> required <code>fit</code> <code>bool</code> <p>Train the transformer before applying it. Defaults to False.</p> <code>False</code> <code>inverse_transform</code> <code>bool</code> <p>Transform back the data to the original representation. This is not available when using transformers of class scikit-learn ColumnTransformers. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Transformed DataFrame.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If df is not a pandas DataFrame.</p> <code>ValueError</code> <p>If inverse_transform is requested for ColumnTransformer.</p> Source code in <code>spotforecast2_safe/utils/data_transform.py</code> <pre><code>def transform_dataframe(\n    df: pd.DataFrame,\n    transformer: object,\n    fit: bool = False,\n    inverse_transform: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform raw values of pandas DataFrame with a scikit-learn alike\n    transformer, preprocessor or ColumnTransformer.\n\n    The transformer used must have the following methods: fit, transform,\n    fit_transform and inverse_transform. ColumnTransformers are not allowed\n    since they do not have inverse_transform method.\n\n    Args:\n        df: DataFrame to be transformed.\n        transformer: Scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n            Must implement fit, transform, fit_transform and inverse_transform.\n        fit: Train the transformer before applying it. Defaults to False.\n        inverse_transform: Transform back the data to the original representation.\n            This is not available when using transformers of class\n            scikit-learn ColumnTransformers. Defaults to False.\n\n    Returns:\n        Transformed DataFrame.\n\n    Raises:\n        TypeError: If df is not a pandas DataFrame.\n        ValueError: If inverse_transform is requested for ColumnTransformer.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(f\"`df` argument must be a pandas DataFrame. Got {type(df)}\")\n\n    if transformer is None:\n        return df\n\n    # Check for ColumnTransformer by class name to avoid importing sklearn\n    is_column_transformer = type(\n        transformer\n    ).__name__ == \"ColumnTransformer\" or hasattr(transformer, \"transformers\")\n\n    if inverse_transform and is_column_transformer:\n        raise ValueError(\n            \"`inverse_transform` is not available when using ColumnTransformers.\"\n        )\n\n    if not inverse_transform:\n        if fit:\n            values_transformed = transformer.fit_transform(df)\n        else:\n            values_transformed = transformer.transform(df)\n    else:\n        values_transformed = transformer.inverse_transform(df)\n\n    if hasattr(values_transformed, \"toarray\"):\n        # If the returned values are in sparse matrix format, it is converted to dense\n        values_transformed = values_transformed.toarray()\n\n    if isinstance(values_transformed, pd.DataFrame):\n        df_transformed = values_transformed\n    else:\n        df_transformed = pd.DataFrame(\n            values_transformed, index=df.index, columns=df.columns\n        )\n\n    return df_transformed\n</code></pre>"},{"location":"api/utils/#forecaster-config","title":"Forecaster Config","text":""},{"location":"api/utils/#spotforecast2.utils.forecaster_config.initialize_lags","title":"<code>spotforecast2.utils.forecaster_config.initialize_lags(forecaster_name, lags)</code>","text":"<p>Validate and normalize lag specification for forecasting.</p> <p>This function converts various lag specifications (int, list, tuple, range, ndarray) into a standardized format: sorted numpy array, lag names, and maximum lag value.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Name of the forecaster class for error messages.</p> required <code>lags</code> <code>Any</code> <p>Lag specification in one of several formats: - int: Creates lags from 1 to lags (e.g., 5 \u2192 [1,2,3,4,5]) - list/tuple/range: Converted to numpy array - numpy.ndarray: Validated and used directly - None: Returns (None, None, None)</p> required <p>Returns:</p> Type Description <code>Optional[ndarray]</code> <p>Tuple containing:</p> <code>Optional[List[str]]</code> <ul> <li>lags: Sorted numpy array of lag values (or None)</li> </ul> <code>Optional[int]</code> <ul> <li>lags_names: List of lag names like ['lag_1', 'lag_2', ...] (or None)</li> </ul> <code>Tuple[Optional[ndarray], Optional[List[str]], Optional[int]]</code> <ul> <li>max_lag: Maximum lag value (or None)</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If lags &lt; 1, empty array, or not 1-dimensional.</p> <code>TypeError</code> <p>If lags is not an integer, not in the right format for the forecaster, or array contains non-integer values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_lags\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Integer input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", 3)\n&gt;&gt;&gt; lags\narray([1, 2, 3])\n&gt;&gt;&gt; names\n['lag_1', 'lag_2', 'lag_3']\n&gt;&gt;&gt; max_lag\n3\n&gt;&gt;&gt;\n&gt;&gt;&gt; # List input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", [1, 3, 5])\n&gt;&gt;&gt; lags\narray([1, 3, 5])\n&gt;&gt;&gt; names\n['lag_1', 'lag_3', 'lag_5']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Range input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", range(1, 4))\n&gt;&gt;&gt; lags\narray([1, 2, 3])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # None input\n&gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", None)\n&gt;&gt;&gt; lags is None\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: lags &lt; 1\n&gt;&gt;&gt; try:\n...     initialize_lags(\"ForecasterRecursive\", 0)\n... except ValueError as e:\n...     print(\"Error: Minimum value of lags allowed is 1\")\nError: Minimum value of lags allowed is 1\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid: negative lags\n&gt;&gt;&gt; try:\n...     initialize_lags(\"ForecasterRecursive\", [1, -2, 3])\n... except ValueError as e:\n...     print(\"Error: Minimum value of lags allowed is 1\")\nError: Minimum value of lags allowed is 1\n</code></pre> Source code in <code>src/spotforecast2/utils/forecaster_config.py</code> <pre><code>def initialize_lags(\n    forecaster_name: str, lags: Any\n) -&gt; Tuple[Optional[np.ndarray], Optional[List[str]], Optional[int]]:\n    \"\"\"\n    Validate and normalize lag specification for forecasting.\n\n    This function converts various lag specifications (int, list, tuple, range, ndarray)\n    into a standardized format: sorted numpy array, lag names, and maximum lag value.\n\n    Args:\n        forecaster_name: Name of the forecaster class for error messages.\n        lags: Lag specification in one of several formats:\n            - int: Creates lags from 1 to lags (e.g., 5 \u2192 [1,2,3,4,5])\n            - list/tuple/range: Converted to numpy array\n            - numpy.ndarray: Validated and used directly\n            - None: Returns (None, None, None)\n\n    Returns:\n        Tuple containing:\n        - lags: Sorted numpy array of lag values (or None)\n        - lags_names: List of lag names like ['lag_1', 'lag_2', ...] (or None)\n        - max_lag: Maximum lag value (or None)\n\n    Raises:\n        ValueError: If lags &lt; 1, empty array, or not 1-dimensional.\n        TypeError: If lags is not an integer, not in the right format for the forecaster,\n            or array contains non-integer values.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_lags\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Integer input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", 3)\n        &gt;&gt;&gt; lags\n        array([1, 2, 3])\n        &gt;&gt;&gt; names\n        ['lag_1', 'lag_2', 'lag_3']\n        &gt;&gt;&gt; max_lag\n        3\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # List input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", [1, 3, 5])\n        &gt;&gt;&gt; lags\n        array([1, 3, 5])\n        &gt;&gt;&gt; names\n        ['lag_1', 'lag_3', 'lag_5']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Range input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", range(1, 4))\n        &gt;&gt;&gt; lags\n        array([1, 2, 3])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # None input\n        &gt;&gt;&gt; lags, names, max_lag = initialize_lags(\"ForecasterRecursive\", None)\n        &gt;&gt;&gt; lags is None\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: lags &lt; 1\n        &gt;&gt;&gt; try:\n        ...     initialize_lags(\"ForecasterRecursive\", 0)\n        ... except ValueError as e:\n        ...     print(\"Error: Minimum value of lags allowed is 1\")\n        Error: Minimum value of lags allowed is 1\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid: negative lags\n        &gt;&gt;&gt; try:\n        ...     initialize_lags(\"ForecasterRecursive\", [1, -2, 3])\n        ... except ValueError as e:\n        ...     print(\"Error: Minimum value of lags allowed is 1\")\n        Error: Minimum value of lags allowed is 1\n    \"\"\"\n    lags_names = None\n    max_lag = None\n\n    if lags is not None:\n        if isinstance(lags, int):\n            if lags &lt; 1:\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n            lags = np.arange(1, lags + 1)\n\n        if isinstance(lags, (list, tuple, range)):\n            lags = np.array(lags)\n\n        if isinstance(lags, np.ndarray):\n            if lags.size == 0:\n                return None, None, None\n            if lags.ndim != 1:\n                raise ValueError(\"`lags` must be a 1-dimensional array.\")\n            if not np.issubdtype(lags.dtype, np.integer):\n                raise TypeError(\"All values in `lags` must be integers.\")\n            if np.any(lags &lt; 1):\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n        else:\n            if forecaster_name == \"ForecasterDirectMultiVariate\":\n                raise TypeError(\n                    f\"`lags` argument must be a dict, int, 1d numpy ndarray, range, \"\n                    f\"tuple or list. Got {type(lags)}.\"\n                )\n            else:\n                raise TypeError(\n                    f\"`lags` argument must be an int, 1d numpy ndarray, range, \"\n                    f\"tuple or list. Got {type(lags)}.\"\n                )\n\n        lags = np.sort(lags)\n        lags_names = [f\"lag_{i}\" for i in lags]\n        max_lag = int(max(lags))\n\n    return lags, lags_names, max_lag\n</code></pre>"},{"location":"api/utils/#spotforecast2.utils.forecaster_config.initialize_weights","title":"<code>spotforecast2.utils.forecaster_config.initialize_weights(forecaster_name, estimator, weight_func, series_weights)</code>","text":"<p>Validate and initialize weight function configuration for forecasting.</p> <p>This function validates weight_func and series_weights, extracts source code from weight functions for serialization, and checks if the estimator supports sample weights in its fit method.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Name of the forecaster class.</p> required <code>estimator</code> <code>Any</code> <p>Scikit-learn compatible estimator or pipeline.</p> required <code>weight_func</code> <code>Any</code> <p>Weight function specification: - Callable: Single weight function - dict: Dictionary of weight functions (for MultiSeries forecasters) - None: No weighting</p> required <code>series_weights</code> <code>Any</code> <p>Dictionary of series-level weights (for MultiSeries forecasters). - dict: Maps series names to weight values - None: No series weighting</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Tuple containing:</p> <code>Optional[Union[str, dict]]</code> <ul> <li>weight_func: Validated weight function (or None if invalid)</li> </ul> <code>Any</code> <ul> <li>source_code_weight_func: Source code of weight function(s) for serialization (or None)</li> </ul> <code>Tuple[Any, Optional[Union[str, dict]], Any]</code> <ul> <li>series_weights: Validated series weights (or None if invalid)</li> </ul> <p>Raises:</p> Type Description <code>TypeError</code> <p>If weight_func is not Callable/dict (depending on forecaster type), or if series_weights is not a dict.</p> <p>Warns:</p> Type Description <code>IgnoredArgumentWarning</code> <p>If estimator doesn't support sample_weight.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_weights\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Simple weight function\n&gt;&gt;&gt; def custom_weights(index):\n...     return np.ones(len(index))\n&gt;&gt;&gt;\n&gt;&gt;&gt; estimator = Ridge()\n&gt;&gt;&gt; wf, source, sw = initialize_weights(\n...     \"ForecasterRecursive\", estimator, custom_weights, None\n... )\n&gt;&gt;&gt; wf is not None\nTrue\n&gt;&gt;&gt; isinstance(source, str)\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # No weight function\n&gt;&gt;&gt; wf, source, sw = initialize_weights(\n...     \"ForecasterRecursive\", estimator, None, None\n... )\n&gt;&gt;&gt; wf is None\nTrue\n&gt;&gt;&gt; source is None\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Invalid type for non-MultiSeries forecaster\n&gt;&gt;&gt; try:\n...     initialize_weights(\"ForecasterRecursive\", estimator, \"invalid\", None)\n... except TypeError as e:\n...     print(\"Error: weight_func must be Callable\")\nError: weight_func must be Callable\n</code></pre> Source code in <code>src/spotforecast2/utils/forecaster_config.py</code> <pre><code>def initialize_weights(\n    forecaster_name: str, estimator: Any, weight_func: Any, series_weights: Any\n) -&gt; Tuple[Any, Optional[Union[str, dict]], Any]:\n    \"\"\"\n    Validate and initialize weight function configuration for forecasting.\n\n    This function validates weight_func and series_weights, extracts source code\n    from weight functions for serialization, and checks if the estimator supports\n    sample weights in its fit method.\n\n    Args:\n        forecaster_name: Name of the forecaster class.\n        estimator: Scikit-learn compatible estimator or pipeline.\n        weight_func: Weight function specification:\n            - Callable: Single weight function\n            - dict: Dictionary of weight functions (for MultiSeries forecasters)\n            - None: No weighting\n        series_weights: Dictionary of series-level weights (for MultiSeries forecasters).\n            - dict: Maps series names to weight values\n            - None: No series weighting\n\n    Returns:\n        Tuple containing:\n        - weight_func: Validated weight function (or None if invalid)\n        - source_code_weight_func: Source code of weight function(s) for serialization (or None)\n        - series_weights: Validated series weights (or None if invalid)\n\n    Raises:\n        TypeError: If weight_func is not Callable/dict (depending on forecaster type),\n            or if series_weights is not a dict.\n\n    Warnings:\n        IgnoredArgumentWarning: If estimator doesn't support sample_weight.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; from spotforecast2.utils.forecaster_config import initialize_weights\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Simple weight function\n        &gt;&gt;&gt; def custom_weights(index):\n        ...     return np.ones(len(index))\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; estimator = Ridge()\n        &gt;&gt;&gt; wf, source, sw = initialize_weights(\n        ...     \"ForecasterRecursive\", estimator, custom_weights, None\n        ... )\n        &gt;&gt;&gt; wf is not None\n        True\n        &gt;&gt;&gt; isinstance(source, str)\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # No weight function\n        &gt;&gt;&gt; wf, source, sw = initialize_weights(\n        ...     \"ForecasterRecursive\", estimator, None, None\n        ... )\n        &gt;&gt;&gt; wf is None\n        True\n        &gt;&gt;&gt; source is None\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Invalid type for non-MultiSeries forecaster\n        &gt;&gt;&gt; try:\n        ...     initialize_weights(\"ForecasterRecursive\", estimator, \"invalid\", None)\n        ... except TypeError as e:\n        ...     print(\"Error: weight_func must be Callable\")\n        Error: weight_func must be Callable\n    \"\"\"\n    import inspect\n    import warnings\n    from collections.abc import Callable\n\n    # Import IgnoredArgumentWarning if available, otherwise define locally\n    try:\n        from spotforecast2.exceptions import IgnoredArgumentWarning\n    except ImportError:\n\n        class IgnoredArgumentWarning(UserWarning):\n            \"\"\"Warning for ignored arguments.\"\"\"\n\n            pass\n\n    source_code_weight_func = None\n\n    if weight_func is not None:\n        if forecaster_name in [\"ForecasterRecursiveMultiSeries\"]:\n            if not isinstance(weight_func, (Callable, dict)):\n                raise TypeError(\n                    f\"Argument `weight_func` must be a Callable or a dict of \"\n                    f\"Callables. Got {type(weight_func)}.\"\n                )\n        elif not isinstance(weight_func, Callable):\n            raise TypeError(\n                f\"Argument `weight_func` must be a Callable. Got {type(weight_func)}.\"\n            )\n\n        if isinstance(weight_func, dict):\n            source_code_weight_func = {}\n            for key in weight_func:\n                try:\n                    source_code_weight_func[key] = inspect.getsource(weight_func[key])\n                except (OSError, TypeError):\n                    # OSError: source not available, TypeError: callable class instance\n                    source_code_weight_func[key] = (\n                        f\"&lt;source unavailable: {weight_func[key]!r}&gt;\"\n                    )\n        else:\n            try:\n                source_code_weight_func = inspect.getsource(weight_func)\n            except (OSError, TypeError):\n                # OSError: source not available (e.g., built-in, lambda in REPL)\n                # TypeError: callable class instance (e.g., WeightFunction)\n                # In these cases, we can't get source but the object can still be pickled\n                source_code_weight_func = f\"&lt;source unavailable: {weight_func!r}&gt;\"\n\n        if \"sample_weight\" not in inspect.signature(estimator.fit).parameters:\n            warnings.warn(\n                f\"Argument `weight_func` is ignored since estimator {estimator} \"\n                f\"does not accept `sample_weight` in its `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n            weight_func = None\n            source_code_weight_func = None\n\n    if series_weights is not None:\n        if not isinstance(series_weights, dict):\n            raise TypeError(\n                f\"Argument `series_weights` must be a dict of floats or ints.\"\n                f\"Got {type(series_weights)}.\"\n            )\n        if \"sample_weight\" not in inspect.signature(estimator.fit).parameters:\n            warnings.warn(\n                f\"Argument `series_weights` is ignored since estimator {estimator} \"\n                f\"does not accept `sample_weight` in its `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n            series_weights = None\n\n    return weight_func, source_code_weight_func, series_weights\n</code></pre>"},{"location":"api/utils/#spotforecast2.utils.forecaster_config.check_select_fit_kwargs","title":"<code>spotforecast2.utils.forecaster_config.check_select_fit_kwargs(estimator, fit_kwargs=None)</code>","text":"<p>Check if <code>fit_kwargs</code> is a dict and select only keys used by estimator's <code>fit</code>.</p> <p>This function validates that fit_kwargs is a dictionary, warns about unused arguments, removes 'sample_weight' (which should be handled via weight_func), and returns a dictionary containing only the arguments accepted by the estimator's fit method.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>Any</code> <p>Scikit-learn compatible estimator.</p> required <code>fit_kwargs</code> <code>Optional[dict]</code> <p>Dictionary of arguments to pass to the estimator's fit method.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with only the arguments accepted by the estimator's fit method.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If fit_kwargs is not a dict.</p> <p>Warns:</p> Type Description <code>IgnoredArgumentWarning</code> <p>If fit_kwargs contains keys not used by fit method, or if 'sample_weight' is present (it gets removed).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; from spotforecast2.utils.forecaster_config import check_select_fit_kwargs\n&gt;&gt;&gt;\n&gt;&gt;&gt; estimator = Ridge()\n&gt;&gt;&gt; # Valid argument for Ridge.fit\n&gt;&gt;&gt; kwargs = {\"sample_weight\": [1, 1], \"invalid_arg\": 10}\n&gt;&gt;&gt; # sample_weight is removed (should be passed via weight_func in forecaster)\n&gt;&gt;&gt; # invalid_arg is ignored\n&gt;&gt;&gt; filtered = check_select_fit_kwargs(estimator, kwargs)\n&gt;&gt;&gt; filtered\n{}\n</code></pre> Source code in <code>src/spotforecast2/utils/forecaster_config.py</code> <pre><code>def check_select_fit_kwargs(estimator: Any, fit_kwargs: Optional[dict] = None) -&gt; dict:\n    \"\"\"\n    Check if `fit_kwargs` is a dict and select only keys used by estimator's `fit`.\n\n    This function validates that fit_kwargs is a dictionary, warns about unused arguments,\n    removes 'sample_weight' (which should be handled via weight_func), and returns\n    a dictionary containing only the arguments accepted by the estimator's fit method.\n\n    Args:\n        estimator: Scikit-learn compatible estimator.\n        fit_kwargs: Dictionary of arguments to pass to the estimator's fit method.\n\n    Returns:\n        Dictionary with only the arguments accepted by the estimator's fit method.\n\n    Raises:\n        TypeError: If fit_kwargs is not a dict.\n\n    Warnings:\n        IgnoredArgumentWarning: If fit_kwargs contains keys not used by fit method,\n            or if 'sample_weight' is present (it gets removed).\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.linear_model import Ridge\n        &gt;&gt;&gt; from spotforecast2.utils.forecaster_config import check_select_fit_kwargs\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; estimator = Ridge()\n        &gt;&gt;&gt; # Valid argument for Ridge.fit\n        &gt;&gt;&gt; kwargs = {\"sample_weight\": [1, 1], \"invalid_arg\": 10}\n        &gt;&gt;&gt; # sample_weight is removed (should be passed via weight_func in forecaster)\n        &gt;&gt;&gt; # invalid_arg is ignored\n        &gt;&gt;&gt; filtered = check_select_fit_kwargs(estimator, kwargs)\n        &gt;&gt;&gt; filtered\n        {}\n    \"\"\"\n    import inspect\n    import warnings\n\n    # Import IgnoredArgumentWarning if available, otherwise define locally\n    try:\n        from spotforecast2.exceptions import IgnoredArgumentWarning\n    except ImportError:\n\n        class IgnoredArgumentWarning(UserWarning):\n            \"\"\"Warning for ignored arguments.\"\"\"\n\n            pass\n\n    if fit_kwargs is None:\n        fit_kwargs = {}\n    else:\n        if not isinstance(fit_kwargs, dict):\n            raise TypeError(\n                f\"Argument `fit_kwargs` must be a dict. Got {type(fit_kwargs)}.\"\n            )\n\n        # Get parameters accepted by estimator.fit\n        fit_params = inspect.signature(estimator.fit).parameters\n\n        # Identify unused keys\n        non_used_keys = [k for k in fit_kwargs.keys() if k not in fit_params]\n        if non_used_keys:\n            warnings.warn(\n                f\"Argument/s {non_used_keys} ignored since they are not used by the \"\n                f\"estimator's `fit` method.\",\n                IgnoredArgumentWarning,\n            )\n\n        # Handle sample_weight specially\n        if \"sample_weight\" in fit_kwargs.keys():\n            warnings.warn(\n                \"The `sample_weight` argument is ignored. Use `weight_func` to pass \"\n                \"a function that defines the individual weights for each sample \"\n                \"based on its index.\",\n                IgnoredArgumentWarning,\n            )\n            del fit_kwargs[\"sample_weight\"]\n\n        # Select only the keyword arguments allowed by the estimator's `fit` method.\n        # Note: We need to re-check keys because sample_weight might have been deleted but it might be in fit_params\n        # If it was deleted, it is no longer in fit_kwargs, so this comprehension is safe\n        fit_kwargs = {k: v for k, v in fit_kwargs.items() if k in fit_params}\n\n    return fit_kwargs\n</code></pre>"},{"location":"api/utils/#generate-holiday","title":"Generate Holiday","text":""},{"location":"api/utils/#spotforecast2.utils.generate_holiday.create_holiday_df","title":"<code>spotforecast2.utils.generate_holiday.create_holiday_df(start, end, tz='UTC', freq='h', country_code='DE', state='NW')</code>","text":"<p>Create a DataFrame with datetime index and a binary holiday indicator column.</p> <p>Expands daily holidays to all timestamps in the desired frequency.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>Union[str, Timestamp]</code> <p>Start date/datetime.</p> required <code>end</code> <code>Union[str, Timestamp]</code> <p>End date/datetime.</p> required <code>tz</code> <code>str</code> <p>Timezone to use if not inferred from start/end.</p> <code>'UTC'</code> <code>freq</code> <code>str</code> <p>Frequency of the resulting DataFrame.</p> <code>'h'</code> <code>country_code</code> <code>str</code> <p>Country code for holidays (e.g. \"DE\", \"US\").</p> <code>'DE'</code> <code>state</code> <code>str</code> <p>State code for holidays (e.g. \"NW\", \"CA\").</p> <code>'NW'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with index covering [start, end] at <code>freq</code>,           and a 'holiday' column (1 if holiday, 0 otherwise).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = create_holiday_df(\"2023-12-24\", \"2023-12-26\", freq=\"D\")\n&gt;&gt;&gt; df[\"holiday\"].tolist()\n[0, 1, 1]\n</code></pre> Source code in <code>spotforecast2_safe/utils/generate_holiday.py</code> <pre><code>def create_holiday_df(\n    start: Union[str, pd.Timestamp],\n    end: Union[str, pd.Timestamp],\n    tz: str = \"UTC\",\n    freq: str = \"h\",\n    country_code: str = \"DE\",\n    state: str = \"NW\",\n) -&gt; pd.DataFrame:\n    \"\"\"Create a DataFrame with datetime index and a binary holiday indicator column.\n\n    Expands daily holidays to all timestamps in the desired frequency.\n\n    Args:\n        start: Start date/datetime.\n        end: End date/datetime.\n        tz: Timezone to use if not inferred from start/end.\n        freq: Frequency of the resulting DataFrame.\n        country_code: Country code for holidays (e.g. \"DE\", \"US\").\n        state: State code for holidays (e.g. \"NW\", \"CA\").\n\n    Returns:\n        pd.DataFrame: DataFrame with index covering [start, end] at `freq`,\n                      and a 'holiday' column (1 if holiday, 0 otherwise).\n\n    Examples:\n        &gt;&gt;&gt; df = create_holiday_df(\"2023-12-24\", \"2023-12-26\", freq=\"D\")\n        &gt;&gt;&gt; df[\"holiday\"].tolist()\n        [0, 1, 1]\n    \"\"\"\n    # If start/end are Timestamps with timezones, use that timezone instead of\n    # the default. This avoids conflicts when timezone-aware Timestamps are\n    # passed with a different tz parameter\n    inferred_tz = None\n    if isinstance(start, pd.Timestamp) and start.tz is not None:\n        inferred_tz = str(start.tz)\n    elif isinstance(end, pd.Timestamp) and end.tz is not None:\n        inferred_tz = str(end.tz)\n\n    # Use inferred timezone if available, otherwise use the provided tz parameter\n    effective_tz = inferred_tz if inferred_tz is not None else tz\n\n    # When creating date_range with timezone-aware Timestamps, don't pass tz parameter\n    # to avoid conflicts - pandas will infer it from the Timestamps\n    if inferred_tz is not None:\n        full_index = pd.date_range(start=start, end=end, freq=freq)\n        daily_index = pd.date_range(start=start, end=end, freq=\"D\")\n    else:\n        full_index = pd.date_range(start=start, end=end, freq=freq, tz=effective_tz)\n        daily_index = pd.date_range(start=start, end=end, freq=\"D\", tz=effective_tz)\n\n    # Get holidays for the country/state\n    country_holidays = holidays.country_holidays(country_code, subdiv=state)\n\n    # Check each day if it is a holiday\n    # We use the date part for lookup\n    is_holiday = [1 if date.date() in country_holidays else 0 for date in daily_index]\n\n    df_holiday = pd.DataFrame({\"holiday\": is_holiday}, index=daily_index)\n\n    # Reindex to full frequency and forward fill\n    df_full = df_holiday.reindex(full_index, method=\"ffill\").fillna(0).astype(int)\n\n    return df_full\n</code></pre>"},{"location":"api/weather/","title":"Weather Module","text":""},{"location":"api/weather/#spotforecast2.weather.weather_client","title":"<code>spotforecast2.weather.weather_client</code>","text":"<p>Weather data fetching and processing using Open-Meteo API.</p>"},{"location":"api/weather/#spotforecast2.weather.weather_client.WeatherClient","title":"<code>WeatherClient</code>","text":"<p>Client for fetching weather data from Open-Meteo API.</p> <p>Handles the low-level API interactions, parameter building, and response parsing.</p> Source code in <code>spotforecast2_safe/weather/weather_client.py</code> <pre><code>class WeatherClient:\n    \"\"\"Client for fetching weather data from Open-Meteo API.\n\n    Handles the low-level API interactions, parameter building, and response parsing.\n    \"\"\"\n\n    ARCHIVE_BASE_URL = \"https://archive-api.open-meteo.com/v1/archive\"\n    FORECAST_BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n\n    HOURLY_PARAMS = [\n        \"temperature_2m\",\n        \"relative_humidity_2m\",\n        \"precipitation\",\n        \"rain\",\n        \"snowfall\",\n        \"weather_code\",\n        \"pressure_msl\",\n        \"surface_pressure\",\n        \"cloud_cover\",\n        \"cloud_cover_low\",\n        \"cloud_cover_mid\",\n        \"cloud_cover_high\",\n        \"wind_speed_10m\",\n        \"wind_direction_10m\",\n        \"wind_gusts_10m\",\n    ]\n\n    def __init__(self, latitude: float, longitude: float):\n        \"\"\"Initialize WeatherClient.\n\n        Args:\n            latitude: Latitude of the location.\n            longitude: Longitude of the location.\n        \"\"\"\n        self.latitude = latitude\n        self.longitude = longitude\n        self.logger = logging.getLogger(__name__)\n        self._session = self._create_session()\n\n    def _create_session(self) -&gt; requests.Session:\n        \"\"\"Create a requests session with retry logic.\"\"\"\n        session = requests.Session()\n        retry_strategy = Retry(\n            total=3,\n            backoff_factor=1,\n            status_forcelist=[429, 500, 502, 503, 504],\n        )\n        adapter = HTTPAdapter(max_retries=retry_strategy)\n        session.mount(\"https://\", adapter)\n        session.mount(\"http://\", adapter)\n        return session\n\n    def _fetch(self, url: str, params: Dict[str, Any]) -&gt; pd.DataFrame:\n        \"\"\"Execute API request and return parsed DataFrame.\"\"\"\n        try:\n            response = self._session.get(url, params=params, timeout=30)\n            response.raise_for_status()\n            data = response.json()\n        except requests.exceptions.RequestException as e:\n            self.logger.error(f\"API request failed: {e}\")\n            raise\n\n        if \"error\" in data and data[\"error\"]:\n            raise ValueError(\n                f\"Open-Meteo API error: {data.get('reason', 'Unknown error')}\"\n            )\n\n        hourly_data = data.get(\"hourly\", {})\n        if not hourly_data:\n            raise ValueError(\"No hourly data returned from API\")\n\n        # Parse to DataFrame\n        times = pd.to_datetime(hourly_data[\"time\"])\n        df_dict = {\"datetime\": times}\n        for param in self.HOURLY_PARAMS:\n            if param in hourly_data:\n                df_dict[param] = hourly_data[param]\n\n        df = pd.DataFrame(df_dict)\n        df.set_index(\"datetime\", inplace=True)\n        return df\n\n    def fetch_archive(\n        self, start: pd.Timestamp, end: pd.Timestamp, timezone: str = \"UTC\"\n    ) -&gt; pd.DataFrame:\n        \"\"\"Fetch historical data from Archive API.\"\"\"\n        params = {\n            \"latitude\": self.latitude,\n            \"longitude\": self.longitude,\n            \"hourly\": \",\".join(self.HOURLY_PARAMS),\n            \"timezone\": timezone,\n            \"start_date\": start.strftime(\"%Y-%m-%d\"),\n            \"end_date\": end.strftime(\"%Y-%m-%d\"),\n        }\n        return self._fetch(self.ARCHIVE_BASE_URL, params)\n\n    def fetch_forecast(self, days_ahead: int, timezone: str = \"UTC\") -&gt; pd.DataFrame:\n        \"\"\"Fetch forecast data from Forecast API.\"\"\"\n        params = {\n            \"latitude\": self.latitude,\n            \"longitude\": self.longitude,\n            \"hourly\": \",\".join(self.HOURLY_PARAMS),\n            \"timezone\": timezone,\n            \"forecast_days\": days_ahead,\n        }\n        return self._fetch(self.FORECAST_BASE_URL, params)\n</code></pre>"},{"location":"api/weather/#spotforecast2.weather.weather_client.WeatherClient.__init__","title":"<code>__init__(latitude, longitude)</code>","text":"<p>Initialize WeatherClient.</p> <p>Parameters:</p> Name Type Description Default <code>latitude</code> <code>float</code> <p>Latitude of the location.</p> required <code>longitude</code> <code>float</code> <p>Longitude of the location.</p> required Source code in <code>spotforecast2_safe/weather/weather_client.py</code> <pre><code>def __init__(self, latitude: float, longitude: float):\n    \"\"\"Initialize WeatherClient.\n\n    Args:\n        latitude: Latitude of the location.\n        longitude: Longitude of the location.\n    \"\"\"\n    self.latitude = latitude\n    self.longitude = longitude\n    self.logger = logging.getLogger(__name__)\n    self._session = self._create_session()\n</code></pre>"},{"location":"api/weather/#spotforecast2.weather.weather_client.WeatherClient.fetch_archive","title":"<code>fetch_archive(start, end, timezone='UTC')</code>","text":"<p>Fetch historical data from Archive API.</p> Source code in <code>spotforecast2_safe/weather/weather_client.py</code> <pre><code>def fetch_archive(\n    self, start: pd.Timestamp, end: pd.Timestamp, timezone: str = \"UTC\"\n) -&gt; pd.DataFrame:\n    \"\"\"Fetch historical data from Archive API.\"\"\"\n    params = {\n        \"latitude\": self.latitude,\n        \"longitude\": self.longitude,\n        \"hourly\": \",\".join(self.HOURLY_PARAMS),\n        \"timezone\": timezone,\n        \"start_date\": start.strftime(\"%Y-%m-%d\"),\n        \"end_date\": end.strftime(\"%Y-%m-%d\"),\n    }\n    return self._fetch(self.ARCHIVE_BASE_URL, params)\n</code></pre>"},{"location":"api/weather/#spotforecast2.weather.weather_client.WeatherClient.fetch_forecast","title":"<code>fetch_forecast(days_ahead, timezone='UTC')</code>","text":"<p>Fetch forecast data from Forecast API.</p> Source code in <code>spotforecast2_safe/weather/weather_client.py</code> <pre><code>def fetch_forecast(self, days_ahead: int, timezone: str = \"UTC\") -&gt; pd.DataFrame:\n    \"\"\"Fetch forecast data from Forecast API.\"\"\"\n    params = {\n        \"latitude\": self.latitude,\n        \"longitude\": self.longitude,\n        \"hourly\": \",\".join(self.HOURLY_PARAMS),\n        \"timezone\": timezone,\n        \"forecast_days\": days_ahead,\n    }\n    return self._fetch(self.FORECAST_BASE_URL, params)\n</code></pre>"},{"location":"api/weather/#spotforecast2.weather.weather_client.WeatherService","title":"<code>WeatherService</code>","text":"<p>               Bases: <code>WeatherClient</code></p> <p>High-level service for weather data generation.</p> <p>Extends WeatherClient with caching, hybrid fetching (archive+forecast), and fallback strategies.</p> Source code in <code>spotforecast2_safe/weather/weather_client.py</code> <pre><code>class WeatherService(WeatherClient):\n    \"\"\"High-level service for weather data generation.\n\n    Extends WeatherClient with caching, hybrid fetching (archive+forecast),\n    and fallback strategies.\n    \"\"\"\n\n    def __init__(\n        self,\n        latitude: float,\n        longitude: float,\n        cache_path: Optional[Path] = None,\n        use_forecast: bool = True,\n    ):\n        super().__init__(latitude, longitude)\n        self.cache_path = cache_path\n        self.use_forecast = use_forecast\n\n    def get_dataframe(\n        self,\n        start: Union[str, pd.Timestamp],\n        end: Union[str, pd.Timestamp],\n        timezone: str = \"UTC\",\n        freq: str = \"h\",\n        fallback_on_failure: bool = True,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Get weather DataFrame for a specified range using best available methods.\n\n        Refactored from spotpredict.create_weather_df.\n        \"\"\"\n        start_ts = pd.Timestamp(start)\n        end_ts = pd.Timestamp(end)\n\n        # Localize if naive\n        if start_ts.tz is None:\n            start_ts = start_ts.tz_localize(timezone)\n        if end_ts.tz is None:\n            end_ts = end_ts.tz_localize(timezone)\n\n        # Convert to UTC for consistency\n        start_utc = start_ts.tz_convert(\"UTC\")\n        end_utc = end_ts.tz_convert(\"UTC\")\n\n        # 1. Try Cache\n        cached_df = self._load_cache()\n        if cached_df is not None:\n            if cached_df.index.min() &lt;= start_utc and cached_df.index.max() &gt;= end_utc:\n                self.logger.info(\"Using full cached data.\")\n                return self._finalize_df(\n                    cached_df.loc[start_utc:end_utc], freq, timezone\n                )\n\n        # 2. Hybrid Fetch (filling gaps if cache exists, or fetching all)\n        # (The original logic did partial fills, but full fetch is safer and\n        # simpler for now unless specifically improved).\n        # Actually, strict refactor implies keeping logic. Let's keep it simple:\n        # fetch what's needed.\n\n        try:\n            df = self._fetch_hybrid(start_ts, end_ts, timezone)\n        except Exception as e:\n            self.logger.warning(f\"Fetch failed: {e}\")\n            if fallback_on_failure and cached_df is not None and len(cached_df) &gt;= 24:\n                df = self._create_fallback(start_utc, end_utc, cached_df, timezone)\n            else:\n                raise\n\n        # 3. Merge with cache and save\n        if cached_df is not None:\n            df = pd.concat([cached_df, df])\n            df = df[~df.index.duplicated(keep=\"last\")].sort_index()  # Keep new data\n\n        if self.cache_path:\n            self._save_cache(df)\n\n        # 4. Return slice\n        return self._finalize_df(df.loc[start_utc:end_utc], freq, timezone)\n\n    def _fetch_hybrid(\n        self, start: pd.Timestamp, end: pd.Timestamp, timezone: str\n    ) -&gt; pd.DataFrame:\n        \"\"\"Fetch from Archive and/or Forecast based on date.\"\"\"\n        now = pd.Timestamp.now(tz=start.tz)\n        archive_cutoff = now - pd.Timedelta(days=5)\n\n        dfs = []\n\n        # Archive part\n        if start &lt; archive_cutoff:\n            arch_end = min(end, archive_cutoff)\n            try:\n                dfs.append(self.fetch_archive(start, arch_end, timezone))\n            except Exception as e:\n                self.logger.warning(f\"Archive fetch warning: {e}\")\n\n        # Forecast part\n        if end &gt; now and self.use_forecast:\n            days = (end - now).days + 2\n            days = min(max(1, days), 16)\n            try:\n                df_fore = self.fetch_forecast(days, timezone)\n                # Filter forecast to needed range to avoid overlap issues\n                dfs.append(df_fore)\n            except Exception as e:\n                self.logger.warning(f\"Forecast fetch warning: {e}\")\n\n        if not dfs:\n            raise ValueError(\"Could not fetch data from Archive or Forecast.\")\n\n        full_df = pd.concat(dfs)\n        full_df = full_df[~full_df.index.duplicated(keep=\"first\")].sort_index()\n\n        # Ensure UTC index\n        if full_df.index.tz is None:\n            full_df.index = full_df.index.tz_localize(timezone)\n        full_df.index = full_df.index.tz_convert(\"UTC\")\n\n        return full_df\n\n    def _create_fallback(\n        self,\n        start: pd.Timestamp,\n        end: pd.Timestamp,\n        source_df: pd.DataFrame,\n        timezone: str,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Repeat last 24h of data.\"\"\"\n        last_24 = source_df.tail(24)\n        hours = int((end - start).total_seconds() / 3600) + 1\n        repeats = (hours // 24) + 1\n\n        new_data = pd.concat([last_24] * repeats, ignore_index=True)\n        new_data = new_data.iloc[:hours]\n\n        idx = pd.date_range(start, periods=hours, freq=\"h\", tz=\"UTC\")\n        new_data.index = idx\n        return new_data\n\n    def _load_cache(self) -&gt; Optional[pd.DataFrame]:\n        if not self.cache_path or not self.cache_path.exists():\n            return None\n        try:\n            df = pd.read_parquet(self.cache_path)\n            if df.index.tz is None:\n                df.index = df.index.tz_localize(\"UTC\")\n            return df\n        except Exception:\n            return None\n\n    def _save_cache(self, df: pd.DataFrame):\n        if self.cache_path:\n            self.cache_path.parent.mkdir(parents=True, exist_ok=True)\n            df.to_parquet(self.cache_path)\n\n    def _finalize_df(self, df: pd.DataFrame, freq: str, timezone: str) -&gt; pd.DataFrame:\n        \"\"\"Resample and localize.\"\"\"\n        # Resample\n        if freq != \"h\":  # Assuming API returns hourly\n            df = df.resample(freq).ffill()  # Forward fill for weather is reasonable\n\n        # Fill gaps\n        df = df.ffill().bfill()\n\n        # Convert to requested timezone if needed (though we keep internal UTC mostly)\n        # User requested specific tz output usually?\n        # Original code returned normalized DF. Let's ensure frequency matches exactly.\n\n        return df\n</code></pre>"},{"location":"api/weather/#spotforecast2.weather.weather_client.WeatherService.get_dataframe","title":"<code>get_dataframe(start, end, timezone='UTC', freq='h', fallback_on_failure=True)</code>","text":"<p>Get weather DataFrame for a specified range using best available methods.</p> <p>Refactored from spotpredict.create_weather_df.</p> Source code in <code>spotforecast2_safe/weather/weather_client.py</code> <pre><code>def get_dataframe(\n    self,\n    start: Union[str, pd.Timestamp],\n    end: Union[str, pd.Timestamp],\n    timezone: str = \"UTC\",\n    freq: str = \"h\",\n    fallback_on_failure: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"Get weather DataFrame for a specified range using best available methods.\n\n    Refactored from spotpredict.create_weather_df.\n    \"\"\"\n    start_ts = pd.Timestamp(start)\n    end_ts = pd.Timestamp(end)\n\n    # Localize if naive\n    if start_ts.tz is None:\n        start_ts = start_ts.tz_localize(timezone)\n    if end_ts.tz is None:\n        end_ts = end_ts.tz_localize(timezone)\n\n    # Convert to UTC for consistency\n    start_utc = start_ts.tz_convert(\"UTC\")\n    end_utc = end_ts.tz_convert(\"UTC\")\n\n    # 1. Try Cache\n    cached_df = self._load_cache()\n    if cached_df is not None:\n        if cached_df.index.min() &lt;= start_utc and cached_df.index.max() &gt;= end_utc:\n            self.logger.info(\"Using full cached data.\")\n            return self._finalize_df(\n                cached_df.loc[start_utc:end_utc], freq, timezone\n            )\n\n    # 2. Hybrid Fetch (filling gaps if cache exists, or fetching all)\n    # (The original logic did partial fills, but full fetch is safer and\n    # simpler for now unless specifically improved).\n    # Actually, strict refactor implies keeping logic. Let's keep it simple:\n    # fetch what's needed.\n\n    try:\n        df = self._fetch_hybrid(start_ts, end_ts, timezone)\n    except Exception as e:\n        self.logger.warning(f\"Fetch failed: {e}\")\n        if fallback_on_failure and cached_df is not None and len(cached_df) &gt;= 24:\n            df = self._create_fallback(start_utc, end_utc, cached_df, timezone)\n        else:\n            raise\n\n    # 3. Merge with cache and save\n    if cached_df is not None:\n        df = pd.concat([cached_df, df])\n        df = df[~df.index.duplicated(keep=\"last\")].sort_index()  # Keep new data\n\n    if self.cache_path:\n        self._save_cache(df)\n\n    # 4. Return slice\n    return self._finalize_df(df.loc[start_utc:end_utc], freq, timezone)\n</code></pre>"},{"location":"api/manager/configurator/config_entsoe/","title":"Configuration Module","text":""},{"location":"api/manager/configurator/config_entsoe/#configentsoe","title":"ConfigEntsoe","text":""},{"location":"api/manager/configurator/config_entsoe/#spotforecast2.manager.configurator.config_entsoe.ConfigEntsoe","title":"<code>spotforecast2.manager.configurator.config_entsoe.ConfigEntsoe</code>","text":"<p>Configuration for the ENTSO-E forecasting pipeline.</p> <p>This class manages all configuration parameters for the ENTSO-E task, including API settings, training/prediction intervals, and feature engineering specifications. All parameters can be customized during initialization or used with sensible defaults.</p> <p>Parameters:</p> Name Type Description Default <code>api_country_code</code> <code>str</code> <p>ISO country code for ENTSO-E API queries.</p> <code>'DE'</code> <code>periods</code> <code>Optional[List[Period]]</code> <p>List of Period objects defining cyclical feature encodings.</p> <code>None</code> <code>lags_consider</code> <code>Optional[List[int]]</code> <p>List of lag values to consider for feature selection.</p> <code>None</code> <code>train_size</code> <code>Optional[Timedelta]</code> <p>Time window for training data.</p> <code>None</code> <code>end_train_default</code> <code>str</code> <p>Default end date for training period (ISO format with timezone).</p> <code>'2025-12-31 00:00+00:00'</code> <code>delta_val</code> <code>Optional[Timedelta]</code> <p>Validation window size.</p> <code>None</code> <code>predict_size</code> <code>int</code> <p>Number of hours to predict ahead.</p> <code>24</code> <code>refit_size</code> <code>int</code> <p>Number of days between model refits.</p> <code>7</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility.</p> <code>314159</code> <code>n_hyperparameters_trials</code> <code>int</code> <p>Number of trials for hyperparameter optimization.</p> <code>20</code> <p>Attributes:</p> Name Type Description <code>API_COUNTRY_CODE</code> <p>ISO country code for API queries.</p> <code>periods</code> <p>Cyclical feature encoding specifications.</p> <code>lags_consider</code> <p>Lag values for autoregressive features.</p> <code>train_size</code> <p>Training data window.</p> <code>end_train_default</code> <p>Default training end date.</p> <code>delta_val</code> <p>Validation window.</p> <code>predict_size</code> <p>Prediction horizon in hours.</p> <code>refit_size</code> <p>Refit interval in days.</p> <code>random_state</code> <p>Random seed.</p> <code>n_hyperparameters_trials</code> <p>Hyperparameter tuning trials.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotforecast2 import Config\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use default configuration\n&gt;&gt;&gt; config = Config()\n&gt;&gt;&gt; config.API_COUNTRY_CODE\n'DE'\n&gt;&gt;&gt; config.predict_size\n24\n&gt;&gt;&gt; config.random_state\n314159\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create custom configuration\n&gt;&gt;&gt; custom_config = Config(\n...     api_country_code='FR',\n...     predict_size=48,\n...     random_state=42\n... )\n&gt;&gt;&gt; custom_config.API_COUNTRY_CODE\n'FR'\n&gt;&gt;&gt; custom_config.predict_size\n48\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Verify training window\n&gt;&gt;&gt; config.train_size == pd.Timedelta(days=3 * 365)\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check default periods\n&gt;&gt;&gt; len(config.periods)\n5\n&gt;&gt;&gt; config.periods[0].name\n'daily'\n</code></pre> Source code in <code>src/spotforecast2/manager/configurator/config_entsoe.py</code> <pre><code>class ConfigEntsoe:\n    \"\"\"Configuration for the ENTSO-E forecasting pipeline.\n\n    This class manages all configuration parameters for the ENTSO-E task,\n    including API settings, training/prediction intervals, and feature\n    engineering specifications. All parameters can be customized during\n    initialization or used with sensible defaults.\n\n    Args:\n        api_country_code: ISO country code for ENTSO-E API queries.\n        periods: List of Period objects defining cyclical feature encodings.\n        lags_consider: List of lag values to consider for feature selection.\n        train_size: Time window for training data.\n        end_train_default: Default end date for training period (ISO format with timezone).\n        delta_val: Validation window size.\n        predict_size: Number of hours to predict ahead.\n        refit_size: Number of days between model refits.\n        random_state: Random seed for reproducibility.\n        n_hyperparameters_trials: Number of trials for hyperparameter optimization.\n\n    Attributes:\n        API_COUNTRY_CODE: ISO country code for API queries.\n        periods: Cyclical feature encoding specifications.\n        lags_consider: Lag values for autoregressive features.\n        train_size: Training data window.\n        end_train_default: Default training end date.\n        delta_val: Validation window.\n        predict_size: Prediction horizon in hours.\n        refit_size: Refit interval in days.\n        random_state: Random seed.\n        n_hyperparameters_trials: Hyperparameter tuning trials.\n\n    Examples:\n        &gt;&gt;&gt; from spotforecast2 import Config\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Use default configuration\n        &gt;&gt;&gt; config = Config()\n        &gt;&gt;&gt; config.API_COUNTRY_CODE\n        'DE'\n        &gt;&gt;&gt; config.predict_size\n        24\n        &gt;&gt;&gt; config.random_state\n        314159\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create custom configuration\n        &gt;&gt;&gt; custom_config = Config(\n        ...     api_country_code='FR',\n        ...     predict_size=48,\n        ...     random_state=42\n        ... )\n        &gt;&gt;&gt; custom_config.API_COUNTRY_CODE\n        'FR'\n        &gt;&gt;&gt; custom_config.predict_size\n        48\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Verify training window\n        &gt;&gt;&gt; config.train_size == pd.Timedelta(days=3 * 365)\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check default periods\n        &gt;&gt;&gt; len(config.periods)\n        5\n        &gt;&gt;&gt; config.periods[0].name\n        'daily'\n    \"\"\"\n\n    def __init__(\n        self,\n        api_country_code: str = \"DE\",\n        periods: Optional[List[Period]] = None,\n        lags_consider: Optional[List[int]] = None,\n        train_size: Optional[pd.Timedelta] = None,\n        end_train_default: str = \"2025-12-31 00:00+00:00\",\n        delta_val: Optional[pd.Timedelta] = None,\n        predict_size: int = 24,\n        refit_size: int = 7,\n        random_state: int = 314159,\n        n_hyperparameters_trials: int = 20,\n    ):\n        \"\"\"Initialize ConfigEntsoe with specified or default parameters.\"\"\"\n        self.API_COUNTRY_CODE = api_country_code\n\n        # Default periods use deliberate n_periods choices:\n        # - daily: n_periods=12 for 24 hours (2:1 ratio) provides 2-hour resolution,\n        #   balancing detail vs overfitting while reducing dimensionality by 50%\n        # - weekly/monthly/quarterly: n_periods matches range_size (1:1 ratio)\n        # - yearly: n_periods=12 for 365 days (30:1 ratio) provides strong smoothing\n        # See docs/PERIOD_CONFIGURATION_RATIONALE.md for detailed analysis\n        self.periods = (\n            periods\n            if periods is not None\n            else [\n                Period(name=\"daily\", n_periods=12, column=\"hour\", input_range=(1, 24)),\n                Period(\n                    name=\"weekly\", n_periods=7, column=\"dayofweek\", input_range=(0, 6)\n                ),\n                Period(\n                    name=\"monthly\", n_periods=12, column=\"month\", input_range=(1, 12)\n                ),\n                Period(\n                    name=\"quarterly\", n_periods=4, column=\"quarter\", input_range=(1, 4)\n                ),\n                Period(\n                    name=\"yearly\",\n                    n_periods=12,\n                    column=\"dayofyear\",\n                    input_range=(1, 365),\n                ),\n            ]\n        )\n        self.lags_consider = (\n            lags_consider if lags_consider is not None else list(range(1, 24))\n        )\n        self.train_size = (\n            train_size if train_size is not None else pd.Timedelta(days=3 * 365)\n        )\n        self.end_train_default = end_train_default\n        self.delta_val = (\n            delta_val if delta_val is not None else pd.Timedelta(hours=24 * 7 * 10)\n        )\n        self.predict_size = predict_size\n        self.refit_size = refit_size\n        self.random_state = random_state\n        self.n_hyperparameters_trials = n_hyperparameters_trials\n</code></pre>"},{"location":"api/manager/configurator/config_entsoe/#spotforecast2.manager.configurator.config_entsoe.ConfigEntsoe.API_COUNTRY_CODE","title":"<code>API_COUNTRY_CODE = api_country_code</code>  <code>instance-attribute</code>","text":""},{"location":"api/manager/configurator/config_entsoe/#spotforecast2.manager.configurator.config_entsoe.ConfigEntsoe.periods","title":"<code>periods = periods if periods is not None else [Period(name='daily', n_periods=12, column='hour', input_range=(1, 24)), Period(name='weekly', n_periods=7, column='dayofweek', input_range=(0, 6)), Period(name='monthly', n_periods=12, column='month', input_range=(1, 12)), Period(name='quarterly', n_periods=4, column='quarter', input_range=(1, 4)), Period(name='yearly', n_periods=12, column='dayofyear', input_range=(1, 365))]</code>  <code>instance-attribute</code>","text":""},{"location":"api/manager/configurator/config_entsoe/#spotforecast2.manager.configurator.config_entsoe.ConfigEntsoe.lags_consider","title":"<code>lags_consider = lags_consider if lags_consider is not None else list(range(1, 24))</code>  <code>instance-attribute</code>","text":""},{"location":"api/manager/configurator/config_entsoe/#spotforecast2.manager.configurator.config_entsoe.ConfigEntsoe.train_size","title":"<code>train_size = train_size if train_size is not None else pd.Timedelta(days=(3 * 365))</code>  <code>instance-attribute</code>","text":""},{"location":"api/manager/configurator/config_entsoe/#spotforecast2.manager.configurator.config_entsoe.ConfigEntsoe.end_train_default","title":"<code>end_train_default = end_train_default</code>  <code>instance-attribute</code>","text":""},{"location":"api/manager/configurator/config_entsoe/#spotforecast2.manager.configurator.config_entsoe.ConfigEntsoe.delta_val","title":"<code>delta_val = delta_val if delta_val is not None else pd.Timedelta(hours=(24 * 7 * 10))</code>  <code>instance-attribute</code>","text":""},{"location":"api/manager/configurator/config_entsoe/#spotforecast2.manager.configurator.config_entsoe.ConfigEntsoe.predict_size","title":"<code>predict_size = predict_size</code>  <code>instance-attribute</code>","text":""},{"location":"api/manager/configurator/config_entsoe/#spotforecast2.manager.configurator.config_entsoe.ConfigEntsoe.refit_size","title":"<code>refit_size = refit_size</code>  <code>instance-attribute</code>","text":""},{"location":"api/manager/configurator/config_entsoe/#spotforecast2.manager.configurator.config_entsoe.ConfigEntsoe.random_state","title":"<code>random_state = random_state</code>  <code>instance-attribute</code>","text":""},{"location":"api/manager/configurator/config_entsoe/#spotforecast2.manager.configurator.config_entsoe.ConfigEntsoe.n_hyperparameters_trials","title":"<code>n_hyperparameters_trials = n_hyperparameters_trials</code>  <code>instance-attribute</code>","text":""},{"location":"api/manager/configurator/config_entsoe/#spotforecast2.manager.configurator.config_entsoe.ConfigEntsoe.__init__","title":"<code>__init__(api_country_code='DE', periods=None, lags_consider=None, train_size=None, end_train_default='2025-12-31 00:00+00:00', delta_val=None, predict_size=24, refit_size=7, random_state=314159, n_hyperparameters_trials=20)</code>","text":"<p>Initialize ConfigEntsoe with specified or default parameters.</p> Source code in <code>src/spotforecast2/manager/configurator/config_entsoe.py</code> <pre><code>def __init__(\n    self,\n    api_country_code: str = \"DE\",\n    periods: Optional[List[Period]] = None,\n    lags_consider: Optional[List[int]] = None,\n    train_size: Optional[pd.Timedelta] = None,\n    end_train_default: str = \"2025-12-31 00:00+00:00\",\n    delta_val: Optional[pd.Timedelta] = None,\n    predict_size: int = 24,\n    refit_size: int = 7,\n    random_state: int = 314159,\n    n_hyperparameters_trials: int = 20,\n):\n    \"\"\"Initialize ConfigEntsoe with specified or default parameters.\"\"\"\n    self.API_COUNTRY_CODE = api_country_code\n\n    # Default periods use deliberate n_periods choices:\n    # - daily: n_periods=12 for 24 hours (2:1 ratio) provides 2-hour resolution,\n    #   balancing detail vs overfitting while reducing dimensionality by 50%\n    # - weekly/monthly/quarterly: n_periods matches range_size (1:1 ratio)\n    # - yearly: n_periods=12 for 365 days (30:1 ratio) provides strong smoothing\n    # See docs/PERIOD_CONFIGURATION_RATIONALE.md for detailed analysis\n    self.periods = (\n        periods\n        if periods is not None\n        else [\n            Period(name=\"daily\", n_periods=12, column=\"hour\", input_range=(1, 24)),\n            Period(\n                name=\"weekly\", n_periods=7, column=\"dayofweek\", input_range=(0, 6)\n            ),\n            Period(\n                name=\"monthly\", n_periods=12, column=\"month\", input_range=(1, 12)\n            ),\n            Period(\n                name=\"quarterly\", n_periods=4, column=\"quarter\", input_range=(1, 4)\n            ),\n            Period(\n                name=\"yearly\",\n                n_periods=12,\n                column=\"dayofyear\",\n                input_range=(1, 365),\n            ),\n        ]\n    )\n    self.lags_consider = (\n        lags_consider if lags_consider is not None else list(range(1, 24))\n    )\n    self.train_size = (\n        train_size if train_size is not None else pd.Timedelta(days=3 * 365)\n    )\n    self.end_train_default = end_train_default\n    self.delta_val = (\n        delta_val if delta_val is not None else pd.Timedelta(hours=24 * 7 * 10)\n    )\n    self.predict_size = predict_size\n    self.refit_size = refit_size\n    self.random_state = random_state\n    self.n_hyperparameters_trials = n_hyperparameters_trials\n</code></pre>"},{"location":"api/manager/configurator/config_entsoe/#period","title":"Period","text":""},{"location":"api/manager/configurator/config_entsoe/#spotforecast2.manager.configurator.config_entsoe.Period","title":"<code>spotforecast2.manager.configurator.config_entsoe.Period</code>  <code>dataclass</code>","text":"<p>Information required to encode a period using RBF.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Label used for column naming.</p> required <code>n_periods</code> <code>int</code> <p>Number of periodic basis functions.</p> required <code>column</code> <code>str</code> <p>Column name used for encoding.</p> required <code>input_range</code> <code>Tuple[int, int]</code> <p>Inclusive input range for the column.</p> required Source code in <code>src/spotforecast2/manager/configurator/config_entsoe.py</code> <pre><code>@dataclass(frozen=True)\nclass Period:\n    \"\"\"Information required to encode a period using RBF.\n\n    Args:\n        name: Label used for column naming.\n        n_periods: Number of periodic basis functions.\n        column: Column name used for encoding.\n        input_range: Inclusive input range for the column.\n    \"\"\"\n\n    name: str\n    n_periods: int\n    column: str\n    input_range: Tuple[int, int]\n</code></pre>"},{"location":"preprocessing/outliers/","title":"Outlier Detection and Visualization","text":"<p>This module provides comprehensive tools for detecting and visualizing outliers in time series data using the Isolation Forest algorithm.</p>"},{"location":"preprocessing/outliers/#overview","title":"Overview","text":"<p>The outlier detection module includes three main functions:</p> <ul> <li><code>get_outliers()</code> - Detect outliers using Isolation Forest</li> <li><code>visualize_outliers_hist()</code> - Visualize outliers with static histograms</li> <li><code>visualize_outliers_plotly_scatter()</code> - Visualize outliers with interactive Plotly scatter plots</li> </ul> <p>These functions work together to provide a complete workflow for outlier analysis in time series data.</p>"},{"location":"preprocessing/outliers/#installation","title":"Installation","text":"<p>The outlier visualization functions require <code>matplotlib</code> for histograms and <code>plotly</code> for interactive scatter plots.</p> <p>Using pip: <pre><code>pip install matplotlib plotly\n</code></pre></p> <p>Using uv: <pre><code>uv pip install matplotlib plotly\n</code></pre></p>"},{"location":"preprocessing/outliers/#quick-start","title":"Quick Start","text":""},{"location":"preprocessing/outliers/#basic-outlier-detection","title":"Basic Outlier Detection","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom spotforecast2_safe.preprocessing.outlier import get_outliers\n\n# Create sample data with outliers\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'temperature': np.concatenate([\n        np.random.normal(20, 5, 100),\n        [50, 60, 70]  # outliers\n    ]),\n    'humidity': np.concatenate([\n        np.random.normal(60, 10, 100),\n        [95, 98, 99]  # outliers\n    ])\n})\n\n# Detect outliers\noutliers = get_outliers(data, contamination=0.03)\n\nfor col, outlier_vals in outliers.items():\n    print(f\"{col}: {len(outlier_vals)} outliers detected\")\n</code></pre>"},{"location":"preprocessing/outliers/#histogram-visualization","title":"Histogram Visualization","text":"<pre><code>from spotforecast2.preprocessing.outlier_plots import visualize_outliers_hist\n\n# Create sample data\nnp.random.seed(42)\ndata_original = pd.DataFrame({\n    'temperature': np.concatenate([\n        np.random.normal(20, 5, 100),\n        [50, 60, 70]  # outliers\n    ])\n})\n\ndata_cleaned = data_original.copy()\n\n# Visualize outliers\nvisualize_outliers_hist(\n    data_cleaned,\n    data_original,\n    contamination=0.03,\n    figsize=(12, 5),\n    alpha=0.7\n)\n</code></pre>"},{"location":"preprocessing/outliers/#interactive-plotly-visualization","title":"Interactive Plotly Visualization","text":"<pre><code>from spotforecast2.preprocessing.outlier_plots import visualize_outliers_plotly_scatter\n\n# Create time series data\ndates = pd.date_range('2024-01-01', periods=103, freq='h')\ndata_original = pd.DataFrame({\n    'temperature': np.concatenate([\n        np.random.normal(20, 5, 100),\n        [50, 60, 70]  # outliers\n    ])\n}, index=dates)\n\ndata_cleaned = data_original.copy()\n\n# Visualize outliers with interactive plot\nvisualize_outliers_plotly_scatter(\n    data_cleaned,\n    data_original,\n    contamination=0.03,\n    template='plotly_white'\n)\n</code></pre>"},{"location":"preprocessing/outliers/#api-reference","title":"API Reference","text":""},{"location":"preprocessing/outliers/#get_outliers","title":"get_outliers()","text":"<p>Detect outliers in each column using Isolation Forest.</p> <p>Signature: <pre><code>def get_outliers(\n    data: pd.DataFrame,\n    data_original: Optional[pd.DataFrame] = None,\n    contamination: float = 0.01,\n    random_state: int = 1234,\n) -&gt; Dict[str, pd.Series]\n</code></pre></p> <p>Parameters:</p> Parameter Type Default Description <code>data</code> DataFrame Required The input DataFrame to check for outliers <code>data_original</code> DataFrame None Optional original DataFrame before outlier detection. If provided, helps identify which values became NaN due to outlier detection <code>contamination</code> float 0.01 The estimated proportion of outliers in the dataset (between 0 and 1) <code>random_state</code> int 1234 Random seed for reproducibility <p>Returns:</p> <p>A dictionary mapping column names to pandas Series of outlier values.</p> <p>Raises:</p> <ul> <li><code>ValueError</code> - If data is empty or contains no columns</li> </ul> <p>Example:</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom spotforecast2_safe.preprocessing.outlier import get_outliers\n\n# Create sample data\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'A': np.concatenate([np.random.normal(0, 1, 100), [10, 11, 12]]),\n    'B': np.concatenate([np.random.normal(5, 2, 100), [100, 110, 120]])\n})\n\n# Detect outliers\noutliers = get_outliers(data, contamination=0.03)\nfor col, outlier_vals in outliers.items():\n    print(f\"{col}: {len(outlier_vals)} outliers detected\")\n</code></pre>"},{"location":"preprocessing/outliers/#visualize_outliers_hist","title":"visualize_outliers_hist()","text":"<p>Visualize outliers using stacked histograms.</p> <p>Signature: <pre><code>def visualize_outliers_hist(\n    data: pd.DataFrame,\n    data_original: pd.DataFrame,\n    columns: Optional[list[str]] = None,\n    contamination: float = 0.01,\n    random_state: int = 1234,\n    figsize: tuple[int, int] = (10, 5),\n    bins: int = 50,\n    **kwargs: Any,\n) -&gt; None\n</code></pre></p> <p>Parameters:</p> Parameter Type Default Description <code>data</code> DataFrame Required The DataFrame with cleaned data (outliers may be NaN) <code>data_original</code> DataFrame Required The original DataFrame before outlier detection <code>columns</code> list[str] None List of column names to visualize. If None, all columns are used <code>contamination</code> float 0.01 The estimated proportion of outliers in the dataset <code>random_state</code> int 1234 Random seed for reproducibility <code>figsize</code> tuple[int, int] (10, 5) Figure size as (width, height) <code>bins</code> int 50 Number of histogram bins <code>**kwargs</code> Any - Additional keyword arguments passed to plt.hist() (e.g., color, alpha, edgecolor) <p>Returns:</p> <p>None. Displays matplotlib figures.</p> <p>Raises:</p> <ul> <li><code>ValueError</code> - If data or data_original is empty, or if specified columns don't exist</li> </ul> <p>Example:</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom spotforecast2.preprocessing.outlier_plots import visualize_outliers_hist\n\n# Create sample data\nnp.random.seed(42)\ndata_original = pd.DataFrame({\n    'temperature': np.concatenate([\n        np.random.normal(20, 5, 100),\n        [50, 60, 70]  # outliers\n    ]),\n    'humidity': np.concatenate([\n        np.random.normal(60, 10, 100),\n        [95, 98, 99]  # outliers\n    ])\n})\n\ndata_cleaned = data_original.copy()\n\n# Visualize outliers\nvisualize_outliers_hist(\n    data_cleaned,\n    data_original,\n    contamination=0.03,\n    figsize=(12, 5),\n    alpha=0.7,\n    edgecolor='black'\n)\n</code></pre>"},{"location":"preprocessing/outliers/#visualize_outliers_plotly_scatter","title":"visualize_outliers_plotly_scatter()","text":"<p>Visualize outliers using interactive Plotly scatter plots.</p> <p>Signature: <pre><code>def visualize_outliers_plotly_scatter(\n    data: pd.DataFrame,\n    data_original: pd.DataFrame,\n    columns: Optional[list[str]] = None,\n    contamination: float = 0.01,\n    random_state: int = 1234,\n    **kwargs: Any,\n) -&gt; None\n</code></pre></p> <p>Parameters:</p> Parameter Type Default Description <code>data</code> DataFrame Required The DataFrame with cleaned data (outliers may be NaN) <code>data_original</code> DataFrame Required The original DataFrame before outlier detection <code>columns</code> list[str] None List of column names to visualize. If None, all columns are used <code>contamination</code> float 0.01 The estimated proportion of outliers in the dataset <code>random_state</code> int 1234 Random seed for reproducibility <code>**kwargs</code> Any - Additional keyword arguments passed to go.Figure.update_layout() (e.g., template, height) <p>Returns:</p> <p>None. Displays Plotly figures.</p> <p>Raises:</p> <ul> <li><code>ValueError</code> - If data or data_original is empty, or if specified columns don't exist</li> <li><code>ImportError</code> - If plotly is not installed</li> </ul> <p>Example:</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom spotforecast2.preprocessing.outlier_plots import visualize_outliers_plotly_scatter\n\n# Create time series data\nnp.random.seed(42)\ndates = pd.date_range('2024-01-01', periods=103, freq='h')\ndata_original = pd.DataFrame({\n    'temperature': np.concatenate([\n        np.random.normal(20, 5, 100),\n        [50, 60, 70]  # outliers\n    ]),\n    'humidity': np.concatenate([\n        np.random.normal(60, 10, 100),\n        [95, 98, 99]  # outliers\n    ])\n}, index=dates)\n\ndata_cleaned = data_original.copy()\n\n# Visualize outliers\nvisualize_outliers_plotly_scatter(\n    data_cleaned,\n    data_original,\n    contamination=0.03,\n    template='plotly_white',\n    height=600\n)\n</code></pre>"},{"location":"preprocessing/outliers/#complete-workflow-example","title":"Complete Workflow Example","text":"<p>Here's a complete example showing the typical workflow for outlier detection and visualization:</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom spotforecast2_safe.preprocessing.outlier import get_outliers\nfrom spotforecast2.preprocessing.outlier_plots import (\n    visualize_outliers_hist,\n    visualize_outliers_plotly_scatter)\n\n# Create realistic time series data with outliers\nnp.random.seed(42)\ndates = pd.date_range('2024-01-01', periods=200, freq='h')\ndata_original = pd.DataFrame({\n    'temperature': np.concatenate([\n        np.random.normal(20, 5, 197),\n        [50, 60, 70]  # outliers\n    ]),\n    'humidity': np.concatenate([\n        np.random.normal(60, 10, 197),\n        [95, 98, 99]  # outliers\n    ]),\n    'pressure': np.concatenate([\n        np.random.normal(1013, 10, 197),\n        [800, 1200, 950]  # outliers\n    ])\n}, index=dates)\n\n# Make a copy for cleaning\ndata_cleaned = data_original.copy()\n\n# Step 1: Detect outliers\nprint(\"=== Outlier Detection ===\")\noutliers = get_outliers(\n    data_original,\n    contamination=0.015\n)\n\nfor col, outlier_vals in outliers.items():\n    pct = (len(outlier_vals) / len(data_original)) * 100\n    print(f\"{col}: {len(outlier_vals)} outliers ({pct:.2f}%)\")\n\n# Step 2: Visualize with histograms\nprint(\"\\n=== Histogram Visualization ===\")\nvisualize_outliers_hist(\n    data_cleaned,\n    data_original,\n    contamination=0.015,\n    figsize=(14, 4),\n    alpha=0.7\n)\n\n# Step 3: Visualize with Plotly (interactive)\nprint(\"\\n=== Interactive Plotly Visualization ===\")\nvisualize_outliers_plotly_scatter(\n    data_cleaned,\n    data_original,\n    contamination=0.015,\n    template='plotly_white'\n)\n\n# Step 4: Selective column visualization\nprint(\"\\n=== Selective Column Analysis ===\")\nvisualize_outliers_hist(\n    data_cleaned,\n    data_original,\n    columns=['temperature', 'humidity'],\n    contamination=0.015\n)\n</code></pre>"},{"location":"preprocessing/outliers/#parameters-and-configuration","title":"Parameters and Configuration","text":""},{"location":"preprocessing/outliers/#contamination-parameter","title":"contamination parameter","text":"<p>The <code>contamination</code> parameter controls the expected proportion of outliers in the dataset:</p> <ul> <li>0.01 (1%) - Conservative, detects severe outliers only</li> <li>0.02 (2%) - Moderate, typical for most applications</li> <li>0.05 (5%) - Liberal, detects more potential anomalies</li> </ul> <p>Choose based on your domain knowledge and data characteristics.</p>"},{"location":"preprocessing/outliers/#random_state-parameter","title":"random_state parameter","text":"<p>The <code>random_state</code> parameter ensures reproducibility:</p> <pre><code># Same random_state produces consistent results\noutliers1 = get_outliers(data, random_state=42)\noutliers2 = get_outliers(data, random_state=42)\n# outliers1 == outliers2\n</code></pre>"},{"location":"preprocessing/outliers/#matplotlib-histogram-options","title":"Matplotlib histogram options","text":"<p>When using <code>visualize_outliers_hist()</code>, you can pass additional matplotlib histogram options:</p> <pre><code>visualize_outliers_hist(\n    data_cleaned,\n    data_original,\n    bins=100,           # More granular bins\n    alpha=0.5,          # Transparency\n    edgecolor='black',  # Border around bars\n    linewidth=0.5       # Border thickness\n)\n</code></pre>"},{"location":"preprocessing/outliers/#plotly-layout-options","title":"Plotly layout options","text":"<p>When using <code>visualize_outliers_plotly_scatter()</code>, you can customize the Plotly figure:</p> <pre><code>visualize_outliers_plotly_scatter(\n    data_cleaned,\n    data_original,\n    template='plotly_dark',    # Dark theme\n    height=700,                 # Figure height\n    width=1200                  # Figure width\n)\n</code></pre>"},{"location":"preprocessing/outliers/#algorithm-details","title":"Algorithm Details","text":""},{"location":"preprocessing/outliers/#isolation-forest","title":"Isolation Forest","text":"<p>The underlying algorithm uses scikit-learn's <code>IsolationForest</code>, which:</p> <ol> <li>Randomly selects features and split values</li> <li>Isolates anomalies by exploiting their rarity</li> <li>Assigns anomaly scores based on path lengths</li> <li>Marks points with scores exceeding the contamination threshold as outliers</li> </ol> <p>Key characteristics:</p> <ul> <li>No distance computation needed (efficient for high dimensions)</li> <li>Scales well with number of features</li> <li>Robust to varying scales</li> <li>No hyperparameter tuning required beyond contamination</li> </ul>"},{"location":"preprocessing/outliers/#best-practices","title":"Best Practices","text":""},{"location":"preprocessing/outliers/#1-preprocessing","title":"1. Preprocessing","text":"<p>Clean your data before outlier detection:</p> <pre><code># Remove missing values\ndata_clean = data.dropna()\n\n# Standardize if needed\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndata_scaled = pd.DataFrame(\n    scaler.fit_transform(data),\n    index=data.index,\n    columns=data.columns\n)\n</code></pre>"},{"location":"preprocessing/outliers/#2-contamination-estimation","title":"2. Contamination Estimation","text":"<p>Estimate contamination based on domain knowledge:</p> <pre><code># For known outlier percentage\ncontamination = n_outliers / len(data)\n\n# For exploratory analysis, try multiple values\nfor cont in [0.01, 0.02, 0.05]:\n    outliers = get_outliers(data, contamination=cont)\n    print(f\"Contamination {cont}: {len(outliers)} outliers\")\n</code></pre>"},{"location":"preprocessing/outliers/#3-visual-inspection","title":"3. Visual Inspection","text":"<p>Always visualize results:</p> <pre><code># Histogram for distribution analysis\nvisualize_outliers_hist(data_cleaned, data_original)\n\n# Time series plot for temporal patterns\nvisualize_outliers_plotly_scatter(data_cleaned, data_original)\n</code></pre>"},{"location":"preprocessing/outliers/#4-validation","title":"4. Validation","text":"<p>Verify outliers make sense in context:</p> <pre><code>outliers = get_outliers(data, contamination=0.02)\n\nfor col, vals in outliers.items():\n    print(f\"\\n{col}:\")\n    print(f\"  Regular range: {data[col].min():.2f} - {data[col].max():.2f}\")\n    print(f\"  Outlier values: {sorted(vals.unique())}\")\n    print(f\"  Outlier indices: {list(vals.index)}\")\n</code></pre>"},{"location":"preprocessing/outliers/#testing","title":"Testing","text":"<p>All examples in this guide are validated by <code>tests/test_docs_outliers_examples.py</code> with 43 comprehensive pytest cases covering:</p> <ul> <li>Basic outlier detection functionality</li> <li>Contamination parameter variations (0.01, 0.02, 0.05)</li> <li>Random state reproducibility</li> <li>Data integrity and value validation</li> <li>Complete workflow integration</li> <li>Edge cases (small/large datasets, extreme values, NaN handling)</li> <li>Timeseries data with DatetimeIndex</li> <li>API examples and return types</li> <li>Safety-critical behavior validation</li> </ul> <p>Run the tests:</p> <pre><code># Run outliers documentation tests\nuv run pytest tests/test_docs_outliers_examples.py -v\n\n# Quick check\nuv run pytest tests/test_docs_outliers_examples.py --tb=no -q\n</code></pre>"},{"location":"preprocessing/outliers/#troubleshooting","title":"Troubleshooting","text":""},{"location":"preprocessing/outliers/#issue-no-outliers-detected","title":"Issue: No outliers detected","text":"<p>Solution: Increase the <code>contamination</code> parameter:</p> <pre><code># Try higher contamination\noutliers = get_outliers(data, contamination=0.05)\n</code></pre>"},{"location":"preprocessing/outliers/#issue-too-many-false-positives","title":"Issue: Too many false positives","text":"<p>Solution: Decrease the <code>contamination</code> parameter:</p> <pre><code># Be more conservative\noutliers = get_outliers(data, contamination=0.01)\n</code></pre>"},{"location":"preprocessing/outliers/#issue-importerror-for-plotly","title":"Issue: ImportError for plotly","text":"<p>Solution: Install plotly:</p> <pre><code>pip install plotly\n</code></pre> <p>Or use histogram visualization instead:</p> <pre><code>visualize_outliers_hist(data_cleaned, data_original)\n</code></pre>"},{"location":"preprocessing/outliers/#see-also","title":"See Also","text":"<ul> <li>Isolation Forest Algorithm</li> </ul>"},{"location":"preprocessing/time_series_visualization/","title":"Time Series Visualization","text":"<p>This module provides interactive time series visualization using Plotly, with support for multiple datasets and flexible customization options.</p>"},{"location":"preprocessing/time_series_visualization/#overview","title":"Overview","text":"<p>The time series visualization module includes two main functions:</p> <ul> <li><code>visualize_ts_plotly()</code> - Visualize multiple time series datasets with Plotly</li> <li><code>visualize_ts_comparison()</code> - Compare datasets with optional statistical overlays</li> </ul> <p>These functions provide a flexible, interactive way to explore time series data with support for train/validation/test splits or any custom dataset groupings.</p>"},{"location":"preprocessing/time_series_visualization/#installation","title":"Installation","text":"<p>The time series visualization functions require <code>plotly</code>:</p> <p>Using pip: <pre><code>pip install plotly\n</code></pre></p> <p>Using uv: <pre><code>uv pip install plotly\n</code></pre></p>"},{"location":"preprocessing/time_series_visualization/#quick-start","title":"Quick Start","text":""},{"location":"preprocessing/time_series_visualization/#basic-visualization","title":"Basic Visualization","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom spotforecast2.preprocessing.time_series_visualization import visualize_ts_plotly\n\n# Create sample datasets\nnp.random.seed(42)\ndates_train = pd.date_range('2024-01-01', periods=100, freq='h')\ndates_val = pd.date_range('2024-05-11', periods=50, freq='h')\ndates_test = pd.date_range('2024-07-01', periods=30, freq='h')\n\ndata_train = pd.DataFrame({\n    'temperature': np.random.normal(20, 5, 100),\n    'humidity': np.random.normal(60, 10, 100)\n}, index=dates_train)\n\ndata_val = pd.DataFrame({\n    'temperature': np.random.normal(22, 5, 50),\n    'humidity': np.random.normal(55, 10, 50)\n}, index=dates_val)\n\ndata_test = pd.DataFrame({\n    'temperature': np.random.normal(25, 5, 30),\n    'humidity': np.random.normal(50, 10, 30)\n}, index=dates_test)\n\n# Visualize all datasets\ndataframes = {\n    'Train': data_train,\n    'Validation': data_val,\n    'Test': data_test\n}\n\nvisualize_ts_plotly(dataframes)\n</code></pre>"},{"location":"preprocessing/time_series_visualization/#single-dataset-visualization","title":"Single Dataset Visualization","text":"<pre><code># Visualize a single dataset\ndataframes = {'Data': data_train}\nvisualize_ts_plotly(dataframes, columns=['temperature'])\n</code></pre>"},{"location":"preprocessing/time_series_visualization/#custom-styling","title":"Custom Styling","text":"<pre><code># Customize colors and template\nvisualize_ts_plotly(\n    dataframes,\n    template='plotly_dark',\n    colors={\n        'Train': 'blue',\n        'Validation': 'green',\n        'Test': 'red'\n    },\n    figsize=(1400, 600)\n)\n</code></pre>"},{"location":"preprocessing/time_series_visualization/#api-reference","title":"API Reference","text":""},{"location":"preprocessing/time_series_visualization/#visualize_ts_plotly","title":"visualize_ts_plotly()","text":"<p>Visualize multiple time series datasets interactively with Plotly.</p> <p>Signature: <pre><code>def visualize_ts_plotly(\n    dataframes: Dict[str, pd.DataFrame],\n    columns: Optional[List[str]] = None,\n    title_suffix: str = \"\",\n    figsize: tuple[int, int] = (1000, 500),\n    template: str = \"plotly_white\",\n    colors: Optional[Dict[str, str]] = None,\n    **kwargs: Any,\n) -&gt; None\n</code></pre></p> <p>Parameters:</p> Parameter Type Default Description <code>dataframes</code> Dict[str, DataFrame] Required Dictionary mapping dataset names to DataFrames with datetime index <code>columns</code> list[str] None Columns to visualize. If None, all columns are used <code>title_suffix</code> str \"\" Suffix to append to column names in titles (e.g., \"[\u00b0C]\") <code>figsize</code> tuple[int, int] (1000, 500) Figure size as (width, height) in pixels <code>template</code> str \"plotly_white\" Plotly template name (\"plotly_white\", \"plotly_dark\", \"ggplot2\", etc.) <code>colors</code> Dict[str, str] None Dictionary mapping dataset names to colors. If None, uses default colors <code>**kwargs</code> Any - Additional arguments passed to go.Scatter() (e.g., fill='tozeroy') <p>Returns:</p> <p>None. Displays Plotly figures.</p> <p>Raises:</p> <ul> <li><code>ValueError</code> - If dataframes dict is empty, contains empty DataFrames, or if specified columns don't exist</li> <li><code>ImportError</code> - If plotly is not installed</li> <li><code>TypeError</code> - If dataframes parameter is not a dictionary</li> </ul> <p>Example:</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom spotforecast2.preprocessing.time_series_visualization import visualize_ts_plotly\n\n# Create sample data\nnp.random.seed(42)\ndates = pd.date_range('2024-01-01', periods=100, freq='h')\ndf = pd.DataFrame({\n    'temperature': np.random.normal(20, 5, 100),\n    'humidity': np.random.normal(60, 10, 100)\n}, index=dates)\n\n# Visualize single dataset\nvisualize_ts_plotly({'Data': df})\n</code></pre>"},{"location":"preprocessing/time_series_visualization/#visualize_ts_comparison","title":"visualize_ts_comparison()","text":"<p>Compare multiple datasets with optional statistical overlays.</p> <p>Signature: <pre><code>def visualize_ts_comparison(\n    dataframes: Dict[str, pd.DataFrame],\n    columns: Optional[List[str]] = None,\n    title_suffix: str = \"\",\n    figsize: tuple[int, int] = (1000, 500),\n    template: str = \"plotly_white\",\n    colors: Optional[Dict[str, str]] = None,\n    show_mean: bool = False,\n    **kwargs: Any,\n) -&gt; None\n</code></pre></p> <p>Parameters:</p> Parameter Type Default Description <code>dataframes</code> Dict[str, DataFrame] Required Dictionary mapping dataset names to DataFrames <code>columns</code> list[str] None Columns to visualize. If None, all columns are used <code>title_suffix</code> str \"\" Suffix to append to titles <code>figsize</code> tuple[int, int] (1000, 500) Figure size as (width, height) in pixels <code>template</code> str \"plotly_white\" Plotly template <code>colors</code> Dict[str, str] None Dictionary mapping dataset names to colors <code>show_mean</code> bool False If True, overlay the mean of all datasets <code>**kwargs</code> Any - Additional arguments for go.Scatter() <p>Returns:</p> <p>None. Displays Plotly figures.</p> <p>Raises:</p> <ul> <li><code>ValueError</code> - If dataframes dict is empty</li> <li><code>ImportError</code> - If plotly is not installed</li> </ul> <p>Example:</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom spotforecast2.preprocessing.time_series_visualization import visualize_ts_comparison\n\n# Create sample data\nnp.random.seed(42)\ndates1 = pd.date_range('2024-01-01', periods=100, freq='h')\ndates2 = pd.date_range('2024-05-11', periods=100, freq='h')\n\ndf1 = pd.DataFrame({\n    'value': np.random.normal(20, 5, 100)\n}, index=dates1)\n\ndf2 = pd.DataFrame({\n    'value': np.random.normal(22, 5, 100)\n}, index=dates2)\n\n# Compare with mean overlay\nvisualize_ts_comparison(\n    {'Dataset1': df1, 'Dataset2': df2},\n    show_mean=True\n)\n</code></pre>"},{"location":"preprocessing/time_series_visualization/#complete-workflow-examples","title":"Complete Workflow Examples","text":""},{"location":"preprocessing/time_series_visualization/#trainvalidationtest-split-visualization","title":"Train/Validation/Test Split Visualization","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom spotforecast2.preprocessing.time_series_visualization import visualize_ts_plotly\n\n# Create time series data\nnp.random.seed(42)\nfull_data = pd.DataFrame({\n    'temperature': np.sin(np.linspace(0, 10, 300)) + np.random.normal(0, 0.1, 300),\n    'humidity': np.cos(np.linspace(0, 10, 300)) * 100 + np.random.normal(50, 5, 300)\n}, index=pd.date_range('2024-01-01', periods=300, freq='h'))\n\n# Split data\nsplit1 = int(0.6 * len(full_data))\nsplit2 = int(0.8 * len(full_data))\n\ndata_train = full_data.iloc[:split1]\ndata_val = full_data.iloc[split1:split2]\ndata_test = full_data.iloc[split2:]\n\n# Visualize\ndataframes = {\n    'Train': data_train,\n    'Validation': data_val,\n    'Test': data_test\n}\n\nvisualize_ts_plotly(\n    dataframes,\n    template='plotly_white',\n    figsize=(1200, 600)\n)\n</code></pre>"},{"location":"preprocessing/time_series_visualization/#multiple-datasets-comparison","title":"Multiple Datasets Comparison","text":"<pre><code>from spotforecast2.preprocessing.time_series_visualization import visualize_ts_comparison\n\n# Create datasets from different time periods\ndates1 = pd.date_range('2024-01-01', periods=100, freq='h')\ndates2 = pd.date_range('2024-04-01', periods=100, freq='h')\ndates3 = pd.date_range('2024-07-01', periods=100, freq='h')\n\ndf1 = pd.DataFrame({\n    'temperature': np.random.normal(15, 3, 100)\n}, index=dates1)\n\ndf2 = pd.DataFrame({\n    'temperature': np.random.normal(22, 3, 100)\n}, index=dates2)\n\ndf3 = pd.DataFrame({\n    'temperature': np.random.normal(25, 3, 100)\n}, index=dates3)\n\n# Compare with mean\nvisualize_ts_comparison(\n    {\n        'Winter': df1,\n        'Spring': df2,\n        'Summer': df3\n    },\n    show_mean=True,\n    colors={\n        'Winter': 'blue',\n        'Spring': 'green',\n        'Summer': 'red'\n    }\n)\n</code></pre>"},{"location":"preprocessing/time_series_visualization/#dynamic-dataset-handling","title":"Dynamic Dataset Handling","text":"<pre><code># Function works with any number of datasets\ndataframes = {}\n\nfor i in range(5):\n    dates = pd.date_range(f'2024-{i+1:02d}-01', periods=50, freq='h')\n    dataframes[f'Month_{i+1}'] = pd.DataFrame({\n        'sales': np.random.gamma(2, 2, 50) * 1000\n    }, index=dates)\n\nvisualize_ts_plotly(\n    dataframes,\n    title_suffix='[USD]',\n    figsize=(1400, 600)\n)\n</code></pre>"},{"location":"preprocessing/time_series_visualization/#parameters-and-configuration","title":"Parameters and Configuration","text":""},{"location":"preprocessing/time_series_visualization/#figsize-parameter","title":"figsize Parameter","text":"<p>Figure size as (width, height) in pixels:</p> <pre><code># Small figure\nvisualize_ts_plotly(dataframes, figsize=(800, 400))\n\n# Large figure for detailed inspection\nvisualize_ts_plotly(dataframes, figsize=(1600, 800))\n</code></pre>"},{"location":"preprocessing/time_series_visualization/#template-options","title":"Template Options","text":"<p>Plotly provides several built-in templates:</p> <pre><code># Light theme (default)\nvisualize_ts_plotly(dataframes, template='plotly_white')\n\n# Dark theme\nvisualize_ts_plotly(dataframes, template='plotly_dark')\n\n# Minimal theme\nvisualize_ts_plotly(dataframes, template='plotly')\n\n# Other themes\nvisualize_ts_plotly(dataframes, template='ggplot2')\nvisualize_ts_plotly(dataframes, template='seaborn')\n</code></pre>"},{"location":"preprocessing/time_series_visualization/#color-customization","title":"Color Customization","text":"<p>Define custom colors for each dataset:</p> <pre><code>colors = {\n    'Train': '#1f77b4',      # Blue\n    'Validation': '#ff7f0e', # Orange\n    'Test': '#2ca02c'        # Green\n}\n\nvisualize_ts_plotly(dataframes, colors=colors)\n</code></pre>"},{"location":"preprocessing/time_series_visualization/#advanced-scatter-customization","title":"Advanced Scatter Customization","text":"<p>Pass additional options to Plotly Scatter:</p> <pre><code>visualize_ts_plotly(\n    dataframes,\n    fill='tozeroy',           # Fill area under line\n    line=dict(width=2),       # Line width\n    opacity=0.8               # Transparency\n)\n</code></pre>"},{"location":"preprocessing/time_series_visualization/#best-practices","title":"Best Practices","text":""},{"location":"preprocessing/time_series_visualization/#1-use-datetime-index","title":"1. Use Datetime Index","text":"<p>Always use pandas datetime index for proper time axis handling:</p> <pre><code># Good\ndf = pd.DataFrame(data, index=pd.date_range('2024-01-01', periods=len(data), freq='h'))\n\n# Avoid\ndf = pd.DataFrame(data)  # Will use default integer index\n</code></pre>"},{"location":"preprocessing/time_series_visualization/#2-consistent-data-shapes","title":"2. Consistent Data Shapes","text":"<p>Ensure all DataFrames have consistent columns for comparison:</p> <pre><code># Verify columns match\ncolumns = set(df1.columns) &amp; set(df2.columns) &amp; set(df3.columns)\nif not columns:\n    raise ValueError(\"DataFrames have no common columns\")\n</code></pre>"},{"location":"preprocessing/time_series_visualization/#3-handle-large-datasets","title":"3. Handle Large Datasets","text":"<p>For large time series, consider subsampling:</p> <pre><code># Subsample every 10th point\ndf_sub = df[::10]\nvisualize_ts_plotly({'Data': df_sub})\n</code></pre>"},{"location":"preprocessing/time_series_visualization/#4-meaningful-dataset-names","title":"4. Meaningful Dataset Names","text":"<p>Use descriptive names for datasets:</p> <pre><code># Good\ndataframes = {\n    'Training (2023)': data_train,\n    'Validation (Jan 2024)': data_val,\n    'Testing (Feb 2024)': data_test\n}\n\n# Avoid\ndataframes = {\n    'A': data_train,\n    'B': data_val,\n    'C': data_test\n}\n</code></pre>"},{"location":"preprocessing/time_series_visualization/#troubleshooting","title":"Troubleshooting","text":""},{"location":"preprocessing/time_series_visualization/#issue-overlapping-datasets","title":"Issue: Overlapping Datasets","text":"<p>If datasets overlap in time, use separate figures:</p> <pre><code># Visualize one column at a time\nfor col in dataframes[list(dataframes.keys())[0]].columns:\n    visualize_ts_plotly(dataframes, columns=[col])\n</code></pre>"},{"location":"preprocessing/time_series_visualization/#issue-memory-issues-with-large-datasets","title":"Issue: Memory Issues with Large Datasets","text":"<p>Downsample before visualization:</p> <pre><code># Downsample to hourly\ndf_downsampled = df.resample('1H').mean()\nvisualize_ts_plotly({'Data': df_downsampled})\n</code></pre>"},{"location":"preprocessing/time_series_visualization/#issue-missing-data-in-visualization","title":"Issue: Missing Data in Visualization","text":"<p>Handle missing values before visualization:</p> <pre><code># Forward fill missing values\ndf_filled = df.fillna(method='ffill')\nvisualize_ts_plotly({'Data': df_filled})\n</code></pre>"},{"location":"preprocessing/time_series_visualization/#testing","title":"Testing","text":"<p>This module includes comprehensive pytest tests validating all documentation examples and API functionality. Tests are located in <code>tests/test_docs_time_series_visualization_examples.py</code>.</p>"},{"location":"preprocessing/time_series_visualization/#running-tests","title":"Running Tests","text":"<p>Run all time series visualization tests:</p> <pre><code>uv run pytest tests/test_docs_time_series_visualization_examples.py -v\n</code></pre> <p>Run specific test class:</p> <pre><code>uv run pytest tests/test_docs_time_series_visualization_examples.py::TestVisualizeTimeSeriesPlotlyBasic -v\n</code></pre>"},{"location":"preprocessing/time_series_visualization/#test-coverage","title":"Test Coverage","text":"<p>The test suite includes 50 comprehensive tests covering:</p> <ul> <li>Basic Visualization (9 tests): Single/multiple dataset visualization, column selection, custom parameters</li> <li>Comparison Functionality (6 tests): Dataset comparison, statistical overlays, customization</li> <li>Complete Workflows (3 tests): Train/val/test split visualization, multi-dataset comparison, dynamic datasets</li> <li>Parameters &amp; Configuration (8 tests): figsize options, template variations, color customization</li> <li>Best Practices (4 tests): Datetime index handling, consistent shapes, subsampling for large datasets</li> <li>Edge Cases (7 tests): Single value, constant values, NaN handling, negative/large values, many columns</li> <li>API Examples (5 tests): Quick start examples, API function validation</li> <li>Data Integrity (3 tests): Index preservation, data value preservation, dataset independence</li> <li>Safety-Critical (5 tests): Error handling, empty input validation, determinism</li> </ul>"},{"location":"preprocessing/time_series_visualization/#test-validation-command","title":"Test Validation Command","text":"<p>Verify all time series visualization tests pass:</p> <pre><code>uv run pytest tests/test_docs_time_series_visualization_examples.py --tb=short -q\n</code></pre> <p>Expected output: <code>50 passed</code></p>"},{"location":"preprocessing/time_series_visualization/#documentation-examples-tested","title":"Documentation Examples Tested","text":"<p>All code examples in this documentation have been validated with pytest: - Quick start examples (all variants) - Complete workflow examples (train/val/test split, comparison, dynamic) - Parameter configuration examples (figsize, templates, colors) - Best practices examples (datetime index, consistent shapes, large datasets) - Troubleshooting examples (overlapping datasets, memory issues, missing data)</p>"},{"location":"preprocessing/time_series_visualization/#see-also","title":"See Also","text":"<ul> <li>Outlier Detection and Visualization</li> <li>Plotly Documentation</li> </ul>"},{"location":"preprocessing/time_series_visualization/#references","title":"References","text":"<ul> <li>Plotly Dash and Plotly.py documentation: https://plotly.com/python/</li> <li>Pandas datetime index: https://pandas.pydata.org/docs/user_guide/timeseries.html</li> </ul>"},{"location":"processing/model_persistence/","title":"Model Persistence Guide","text":""},{"location":"processing/model_persistence/#overview","title":"Overview","text":"<p>This guide explains how to use the model persistence feature in spotforecast2, which provides scikit-learn-style caching of trained forecasters to disk.</p> <p>Key Feature: Model persistence is fully enabled with support for sample weight functions, providing significant speedup for repeated predictions!</p>"},{"location":"processing/model_persistence/#installation-setup","title":"Installation &amp; Setup","text":"<p>No additional installation needed! The implementation uses joblib (already in requirements) and the built-in <code>WeightFunction</code> class.</p>"},{"location":"processing/model_persistence/#quick-start","title":"Quick Start","text":""},{"location":"processing/model_persistence/#first-run-training-and-caching","title":"First Run - Training and Caching","text":"<pre><code>from spotforecast2_safe.processing.n2n_predict_with_covariates import n2n_predict_with_covariates\n\n# Models are trained and cached automatically\npredictions, metadata, forecasters = n2n_predict_with_covariates(\n    forecast_horizon=24,\n    verbose=True  # Shows: \"Training X forecasters...\" and \"Saving X trained forecasters...\"\n)\n</code></pre>"},{"location":"processing/model_persistence/#second-run-loading-from-cache","title":"Second Run - Loading from Cache","text":"<pre><code># Models are loaded from cache (much faster!)\npredictions, metadata, forecasters = n2n_predict_with_covariates(\n    forecast_horizon=24,\n    verbose=True  # Shows: \"All X forecasters loaded from cache\"\n)\n</code></pre>"},{"location":"processing/model_persistence/#force-retraining","title":"Force Retraining","text":"<pre><code># Force retraining - ignore cache, retrain all models\npredictions, metadata, forecasters = n2n_predict_with_covariates(\n    forecast_horizon=24,\n    force_train=True,  # Ignore cache, retrain all\n    verbose=True\n)\n</code></pre>"},{"location":"processing/model_persistence/#custom-cache-location","title":"Custom Cache Location","text":"<pre><code># Use custom directory for models\npredictions, metadata, forecasters = n2n_predict_with_covariates(\n    forecast_horizon=24,\n    model_dir=\"/path/to/models\",  # Default: None (uses ~/spotforecast2_cache/forecasters)\n    verbose=True\n)\n</code></pre>"},{"location":"processing/model_persistence/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>force_train</code> bool True Force retraining, ignore cache <code>model_dir</code> str/Path None Cache directory location. If None, uses <code>get_cache_home()/forecasters</code>"},{"location":"processing/model_persistence/#performance","title":"Performance","text":"<p>Default Cache Directory: - Location: <code>~/spotforecast2_cache/forecasters/</code> - Environment Variable: <code>SPOTFORECAST2_CACHE</code> (overrides default directory) - Models are stored in the format: <code>model_dir/forecaster_{target_name}.joblib</code></p>"},{"location":"processing/model_persistence/#verbose-output-examples","title":"Verbose Output Examples","text":""},{"location":"processing/model_persistence/#all-models-loaded-from-cache","title":"All Models Loaded from Cache","text":"<pre><code>[8/9] Loading or training recursive forecasters with exogenous variables...\n  Attempting to load cached models...\n  \u2713 Loaded forecaster for power from ./forecaster_models/forecaster_power.joblib\n  \u2713 Loaded forecaster for energy from ./forecaster_models/forecaster_energy.joblib\n  ...\n  \u2713 All 10 forecasters loaded from cache\n</code></pre>"},{"location":"processing/model_persistence/#partial-cache-loading-and-training","title":"Partial Cache - Loading and Training","text":"<pre><code>[8/9] Loading or training recursive forecasters with exogenous variables...\n  Attempting to load cached models...\n  \u2713 Loaded forecaster for power from ./forecaster_models/forecaster_power.joblib\n  \u2713 Loaded 1 forecasters, will train 1 new ones\n  Training forecaster for energy...\n    \u2713 Forecaster trained for energy\n  Saving 1 trained forecasters to disk...\n  \u2713 Saved forecaster for energy to ./forecaster_models/forecaster_energy.joblib\n  \u2713 Total forecasters available: 2\n</code></pre>"},{"location":"processing/model_persistence/#force-retraining_1","title":"Force Retraining","text":"<pre><code>[8/9] Loading or training recursive forecasters with exogenous variables...\n  Force retraining all 2 forecasters...\n  Training forecaster for power...\n    \u2713 Forecaster trained for power\n  Training forecaster for energy...\n    \u2713 Forecaster trained for energy\n  Saving 2 trained forecasters to disk...\n  \u2713 Saved forecaster for power to ./forecaster_models/forecaster_power.joblib\n  \u2713 Saved forecaster for energy to ./forecaster_models/forecaster_energy.joblib\n  \u2713 Total forecasters available: 2\n</code></pre>"},{"location":"processing/model_persistence/#key-implementation-details","title":"Key Implementation Details","text":""},{"location":"processing/model_persistence/#weightfunction-class","title":"WeightFunction Class","text":"<p>The <code>WeightFunction</code> class enables model persistence with sample weights:</p> <pre><code>from spotforecast2.preprocessing import WeightFunction\n\n# Create picklable weight function\nweights_series = pd.Series([1.0, 0.9, 0.8], index=[0, 1, 2])\nweight_func = WeightFunction(weights_series)\n\n# Use with forecaster - automatically persisted to disk!\nforecaster = ForecasterRecursive(\n    estimator=estimator,\n    weight_func=weight_func\n)\n</code></pre> <p>Calling WeightFunction:</p> <pre><code>import pandas as pd\nfrom spotforecast2.preprocessing import WeightFunction\n\nweights = pd.Series([1.0, 0.9, 0.8], index=[0, 1, 2])\nweight_func = WeightFunction(weights)\n\n# For single index value\nweight = weight_func(0)  # Returns: 1.0\n\n# For multiple index values\nweights = weight_func(pd.Index([0, 1, 2]))  # Returns: array([1.0, 0.9, 0.8])\n</code></pre> <p>Benefits: - \u2705 Fully picklable (works with joblib) - \u2705 No external dependencies - \u2705 No closure limitations - \u2705 Follows sklearn conventions</p> <p>This approach ensures all trained models with sample weights can be persisted to disk without any external dependencies.</p>"},{"location":"processing/model_persistence/#smart-caching-strategy","title":"Smart Caching Strategy","text":"<p>The system implements intelligent selective retraining:</p> <ol> <li>Cache Lookup (if <code>force_train=False</code>)</li> <li>Check if model cache directory exists</li> <li>Attempt to load all target models from disk</li> <li> <p>Identify which targets are missing</p> </li> <li> <p>Selective Training</p> </li> <li>Train only missing models (not cached)</li> <li>Keep loaded models in memory</li> <li> <p>Saves significant computation time</p> </li> <li> <p>Auto-Save</p> </li> <li>Newly trained models automatically saved to disk</li> <li>Maintains cache consistency</li> <li> <p>No manual save required</p> </li> <li> <p>Force Retraining (if <code>force_train=True</code>)</p> </li> <li>Clears cache directory</li> <li>Trains all models from scratch</li> <li>Useful for model updates or validation</li> </ol>"},{"location":"processing/model_persistence/#working-with-models","title":"Working with Models","text":""},{"location":"processing/model_persistence/#helper-functions-advanced-usage","title":"Helper Functions (Advanced Usage)","text":"<p>For advanced use cases, you can directly use the persistence helper functions:</p> <pre><code>from spotforecast2_safe.processing.n2n_predict_with_covariates import (\n    _ensure_model_dir,\n    _get_model_filepath,\n    _save_forecasters,\n    _load_forecasters,\n    _model_directory_exists\n)\n\n# Create/ensure model directory exists\nmodel_dir = _ensure_model_dir(\"./my_models\")\n\n# Get path for a specific model\npath = _get_model_filepath(model_dir, \"power\")\n# Returns: my_models/forecaster_power.joblib\n\n# Load cached models\nforecasters, missing = _load_forecasters(\n    [\"power\", \"energy\", \"temperature\"],\n    model_dir,\n    verbose=True\n)\n# Returns: (loaded_forecasters_dict, missing_targets_list)\n\n# Save models to disk\nsaved_paths = _save_forecasters(\n    {\"power\": forecaster_obj, \"energy\": forecaster_obj},\n    model_dir,\n    verbose=True\n)\n\n# Check if cache directory exists\nif _model_directory_exists(model_dir):\n    print(\"Cache directory found\")\n</code></pre>"},{"location":"processing/model_persistence/#programmatic-configuration","title":"Programmatic Configuration","text":"<pre><code>import os\nfrom spotforecast2_safe.data import get_cache_home\n\n# Get default cache location\ncache_dir = get_cache_home()\n\n# Or set environment variable\nos.environ['SPOTFORECAST2_CACHE'] = '/custom/cache/path'\ncache_dir = get_cache_home()  # Now uses custom path\n\n# Use in forecasting\npredictions, metadata, forecasters = n2n_predict_with_covariates(\n    forecast_horizon=24,\n    model_dir=str(cache_dir / \"forecasters\"),\n    verbose=True\n)\n</code></pre>"},{"location":"processing/model_persistence/#implementation-files","title":"Implementation Files","text":"<p>Core Implementation: - <code>src/spotforecast2/processing/n2n_predict_with_covariates.py</code> - Main forecasting function with persistence - <code>src/spotforecast2/preprocessing/imputation.py</code> - WeightFunction class - <code>src/spotforecast2/utils/forecaster_config.py</code> - Weight function initialization</p> <p>Test Files: - <code>tests/test_model_persistence.py</code> (35 unit tests) - <code>tests/test_n2n_persistence_integration.py</code> (12 integration tests) - <code>tests/test_weight_function_pickle.py</code> (6 pickling tests) - <code>tests/test_cache_home.py</code> (14 cache home tests)</p>"},{"location":"processing/model_persistence/#testing","title":"Testing","text":"<pre><code># Run persistence tests\nuv run pytest tests/test_model_persistence.py -v\n\n# Run documentation example tests\nuv run pytest tests/test_docs_model_persistence_examples.py -v\n\n# Run integration tests\nuv run pytest tests/test_n2n_persistence_integration.py -v\n\n# Run weight function pickling tests\nuv run pytest tests/test_weight_function_pickle.py -v\n\n# Run all persistence-related tests\nuv run pytest tests/test_model_persistence.py tests/test_docs_model_persistence_examples.py tests/test_n2n_persistence_integration.py tests/test_weight_function_pickle.py -v\n\n# Quick check (all tests should pass)\nuv run pytest tests/test_model_persistence.py tests/test_docs_model_persistence_examples.py tests/test_n2n_persistence_integration.py tests/test_weight_function_pickle.py --tb=no -q\n</code></pre> <p>Documentation validation: All examples in this guide are validated by <code>tests/test_docs_model_persistence_examples.py</code> with 43 comprehensive pytest cases.</p>"},{"location":"processing/model_persistence/#troubleshooting","title":"Troubleshooting","text":""},{"location":"processing/model_persistence/#q-models-not-loading","title":"Q: Models not loading?","text":"<p>A: Check that the <code>model_dir</code> path is correct and accessible: <pre><code># Verify models exist in the directory\nls ./forecaster_models/\n\n# Check file permissions\nls -la ./forecaster_models/\n</code></pre></p> <p>Use <code>force_train=True</code> to rebuild the cache if needed: <pre><code>predictions, metadata, forecasters = n2n_predict_with_covariates(\n    forecast_horizon=24,\n    force_train=True,  # Rebuild cache\n    model_dir=\"./forecaster_models\",\n    verbose=True\n)\n</code></pre></p>"},{"location":"processing/model_persistence/#q-slow-on-first-run","title":"Q: Slow on first run?","text":"<p>A: Training takes 5-10 minutes depending on data size and number of models. This is normal - models are then cached for fast reuse. Subsequent runs will be 1-2 seconds.</p>"},{"location":"processing/model_persistence/#q-want-to-clear-cache","title":"Q: Want to clear cache?","text":"<p>A: Delete the model directory: <pre><code>rm -rf ./forecaster_models/\n</code></pre></p> <p>Or set <code>force_train=True</code> to rebuild: <pre><code>predictions, metadata, forecasters = n2n_predict_with_covariates(\n    forecast_horizon=24,\n    force_train=True,\n    model_dir=\"./forecaster_models\",\n    verbose=True\n)\n</code></pre></p>"},{"location":"processing/model_persistence/#q-models-taking-up-too-much-space","title":"Q: Models taking up too much space?","text":"<p>A: Each model is ~1-5 MB compressed with joblib. You can: - Delete <code>model_dir</code> to free space: <code>rm -rf ./forecaster_models/</code> - Use a different location: Set <code>model_dir</code> to a location with more space - Set <code>force_train=True</code> to rebuild only if needed</p>"},{"location":"processing/model_persistence/#q-how-do-i-use-custom-estimators-with-persistence","title":"Q: How do I use custom estimators with persistence?","text":"<p>A: Custom estimators work with persistence as long as they're pickle-compatible (most scikit-learn compatible estimators are):</p> <pre><code>from lightgbm import LGBMRegressor\n\ncustom_estimator = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=7,\n    random_state=42\n)\n\npredictions, metadata, forecasters = n2n_predict_with_covariates(\n    forecast_horizon=24,\n    estimator=custom_estimator,\n    force_train=False,  # Use cache if available\n    model_dir=\"./models\",\n    verbose=True\n)\n</code></pre>"},{"location":"processing/model_persistence/#technical-details","title":"Technical Details","text":""},{"location":"processing/model_persistence/#implementation","title":"Implementation","text":"<p>The model persistence feature uses joblib for serialization, following scikit-learn conventions: - Format: Binary compressed files with <code>.joblib</code> extension - Compression: joblib compress=3 (good balance of speed and size) - Location: Configurable directory (default: <code>./forecaster_models/</code>) - Naming: <code>forecaster_{target_name}.joblib</code></p>"},{"location":"processing/model_persistence/#weight-function-pickling","title":"Weight Function Pickling","text":"<p>The implementation uses a <code>WeightFunction</code> class to ensure sample weights can be pickled. This solves a common problem where local functions with closures cannot be serialized:</p> <pre><code>from spotforecast2.preprocessing import WeightFunction\nimport pandas as pd\n\n# Weights created from missing data analysis\nweights = pd.Series([1.0, 0.9, 0.8], index=[0, 1, 2])\n\n# Wrap in WeightFunction (picklable, unlike local functions!)\nweight_func = WeightFunction(weights)\n\n# Can be pickled and saved to disk\nimport pickle\npickled = pickle.dumps(weight_func)\n\n# Use with ForecasterRecursive\nforecaster = ForecasterRecursive(\n    estimator=estimator,\n    lags=24,\n    weight_func=weight_func  # Fully picklable!\n)\n</code></pre> <p>This approach ensures all trained models with sample weights can be persisted to disk without any external dependencies.</p>"},{"location":"processing/model_persistence/#smart-caching-strategy_1","title":"Smart Caching Strategy","text":"<p>The system implements intelligent selective retraining:</p> <ol> <li>Cache Lookup (if <code>force_train=False</code>)</li> <li>Check if model cache directory exists</li> <li>Attempt to load all target models from disk</li> <li> <p>Identify which targets are missing</p> </li> <li> <p>Selective Training</p> </li> <li>Train only missing models (not cached)</li> <li>Keep loaded models in memory</li> <li> <p>Saves significant computation time</p> </li> <li> <p>Auto-Save</p> </li> <li>Newly trained models automatically saved to disk</li> <li>Maintains cache consistency</li> <li> <p>No manual save required</p> </li> <li> <p>Force Retraining (if <code>force_train=True</code>)</p> </li> <li>Clears cache directory</li> <li>Trains all models from scratch</li> <li>Useful for model updates or validation</li> </ol>"},{"location":"processing/model_persistence/#api-compatibility","title":"API Compatibility","text":"<p>\u2705 Backward Compatible - All new parameters have defaults \u2705 Drop-in Replacement - Works with existing code \u2705 No Breaking Changes - Safe to upgrade</p>"},{"location":"processing/model_persistence/#see-also","title":"See Also","text":"<ul> <li>API Reference - Forecasting</li> <li>API Reference - Data</li> <li>Preprocessing Guide</li> </ul>"}]}