---
title: "Introduction to Model Training"
---

The `train_new_model()` function in `spotforecast2.manager.trainer_full` serves as the primary gateway for bootstrapping a forecasting context. It is designed to cleanly separate the complex realities of data ingestion, training window creation (cutoffs), hyperparameter tuning, and cross-platform model persistence.

This guide explores the standard approach to training models with `train_new_model()`, evaluating argument utility, and demonstrating both basic and advanced scenarios.

## Core Arguments Overview

When interacting with `train_new_model()`, you supply necessary context parameters that define *what* model is built and *what* data it learns from.

| Argument | Type | Description |
| :--- | :--- | :--- |
| `model_class` | type | Reference to the python class representing the forecaster. It must accept `iteration`, `end_dev`, and `train_size`, and expose a `tune()` method. |
| `n_iteration` | int | Incremental version number distinguishing this training cycle from predecessors. Strongly recommended for lineage tagging. |
| `model_name` | str \| None | Base tracking tag. The final saved filename follows the format: `<model_name>_forecaster_<n_iteration>.joblib`. |
| `train_size` | pd.Timedelta \| None | Total duration of the time series window extracted backwards from the `end_dev` cutoff. |
| `save_to_file` | bool | Automatically compress and serialize the fully tuned model to a `.joblib` component on disk. |
| `model_dir` | str \| Path \| None | Output directory for the joblib. Defaults to the framework's canonical cache home if left unspecified. |
| `end_dev` | str \| pd.Timestamp \| None | Hard cutoff timestamp. Data strictly chronologically after this timestamp is masked from the training pipeline. If None, it automatically infers this as one day before the most recent data point. |
| `data_filename` | str \| None | Target csv path inside the dataset dir to load. Defers to `fetch_data()` logic if omitted. |
| `**kwargs` | Any | Key-value arguments streamed dynamically right into the `model_class` initialization lifecycle. |

: Available arguments for `train_new_model()`. {#tbl-args}

## Simple Training Example

Let's look at the most basic way to initialize and launch tuning for a new model pipeline. For the purpose of these examples, we will define a `MockForecaster` class representing our forecaster model, similar to how internal tracking elements act.

```{python}
import pandas as pd
from spotforecast2.manager.trainer_full import train_new_model

# 1. Define a Mock Model Class meeting the API requirements
class MockForecaster:
    def __init__(self, iteration, end_dev, train_size, **kwargs):
        self.iteration = iteration
        self.end_dev = end_dev
        self.train_size = train_size
        self.config = kwargs
    
    def tune(self):
        # In actual usage, this acts as the gateway to spotoptim_search
        print(f"Executing tune() for iteration {self.iteration}")
        print(f"Focus window cuts off at: {self.end_dev}")

    def get_params(self):
        return {"stub": "mock"}

# 2. Start a basic training run explicitly overriding the cutoff
# Note: we disable saving to prevent dumping a joblib locally during the example
model_basic = train_new_model(
    model_class=MockForecaster,
    n_iteration=1,
    model_name="baseline_mock",
    end_dev="2023-01-01 00:00+00:00",
    train_size=None, # Use the entire history
    save_to_file=False
)

print(f"Constructed class type: {type(model_basic).__name__}")
print(f"Model internal cutoff limit: {model_basic.end_dev}")
```

## Advanced Training Scenarios

In production systems, `train_new_model` handles rolling window progression safely via argument parameters. You will rarely want to default to `train_size=None` (complete history) as this risks severe memory allocation and concept drift over time. Instead, utilizing fixed continuous windows mapped against hard checkpoints handles edge cases effectively.

We can combine `train_size` constraint generation dynamically with extra parameter streaming (`**kwargs`):

```{python}
# 1. Start an advanced tuning workflow
model_advanced = train_new_model(
    model_class=MockForecaster,
    n_iteration=3,
    model_name="production_mock",
    train_size=pd.Timedelta(days=365), # Force exactly 1 year backward logic
    end_dev="2024-03-15 00:00+00:00",
    save_to_file=False,
    # Inject specific kwargs dynamically
    lags=48,
    advanced_regularization=True,
    surrogate_seed=1214
)

print(f"Validation bounded train_size setting: {model_advanced.train_size.days} days")
print(f"Injected **kwargs parameters -> lags: {model_advanced.config.get('lags')}")
print(f"Injected **kwargs parameters -> regularization: {model_advanced.config.get('advanced_regularization')}")
```

Because of its generalized class hook mechanism, any `ForecasterRecursive` wrap, complex pipeline, or hybrid system that matches the initialization signature and `tune()` command standard can be successfully optimized and routed through this framework entrypoint.

## Fully Functional End-to-End Example

To bridge theory into a real application, the following completely functional example demonstrates loading the packaged `demo01.csv` historical dataset. We construct a minimal implementation of the `model_class`, utilizing `fetch_data` to load the history inside the `tune` method and performing a genuine `ForecasterRecursive` fit. 

```{python}
import pandas as pd
from sklearn.linear_model import Ridge
from spotforecast2_safe.forecaster.recursive import ForecasterRecursive
from spotforecast2_safe.data.fetch_data import fetch_data, get_package_data_home
from spotforecast2.manager.trainer_full import train_new_model

class FunctionalForecaster:
    # Notice we capture `dataset_path` from dynamic **kwargs
    def __init__(self, iteration, end_dev, train_size, dataset_path=None, **kwargs):
        self.iteration = iteration
        self.end_dev = end_dev
        self.train_size = train_size
        self.dataset_path = dataset_path
        
        # A simple internal forecaster to be trained
        self.forecaster = ForecasterRecursive(estimator=Ridge(), lags=3)
        self.name = "demo01_model"

    def tune(self):
        # 1. Fetch the data inside the model
        df = fetch_data(filename=self.dataset_path)
        y = df["Actual Load"]
        
        # 2. Slice the historical data strictly up to end_dev according to train_size
        if self.train_size is not None:
            start_date = self.end_dev - self.train_size
            y_train = y.loc[start_date:self.end_dev]
        else:
            y_train = y.loc[:self.end_dev]

        # 3. Fit the model genuinely
        print(f"Fitting model strictly on data until {self.end_dev}")
        print(f"Training window length: {len(y_train)} hours")
        self.forecaster.fit(y=y_train)
        
    def get_params(self):
        return {}

# 1. Define path to the demo dataset packaged dynamically with spotforecast2_safe
demo_file = get_package_data_home() / "demo01.csv"

# 2. Execute the training pipeline
# By setting end_dev=None, train_new_model checks the CSV implicitly 
# to calculate the cutoff boundary to be exactly 1 day before the final record.
model_functional = train_new_model(
    model_class=FunctionalForecaster,
    n_iteration=1,
    train_size=pd.Timedelta(days=7), # Only use the last 7 days of data for training
    end_dev=None, 
    data_filename=str(demo_file), # Passed to train_new_model to compute cutoff
    save_to_file=False, # Disable file writes for the example
    dataset_path=str(demo_file) # Stored in kwargs and passed to __init__
)

assert model_functional.forecaster.is_fitted is True
print("Model pipeline successfully fitted!")
```

## Visualizing Prediction Quality

In safety-critical workflows, evaluating multi-step out-of-sample performance is critical. We can leverage the framework to explicitly constrain the `end_dev` boundary, allowing us to withhold future data. Once `train_new_model` completes training, we use the returned pipeline to project predictions across the held-out window and visualize the model's reliability in distinguishing structural patterns in the `demo02.csv` dataset.

```{python}
import pandas as pd
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_absolute_error
from spotforecast2_safe.forecaster.recursive import ForecasterRecursive
from spotforecast2_safe.data.fetch_data import fetch_data, get_package_data_home
from spotforecast2.manager.trainer_full import train_new_model
import plotly.graph_objects as go

class VisualizingForecaster:
    def __init__(self, iteration, end_dev, train_size, dataset_path=None, **kwargs):
        self.iteration = iteration
        self.end_dev = end_dev
        self.train_size = train_size
        self.dataset_path = dataset_path
        
        # Using a deeper lag window for more predictive capability
        self.forecaster = ForecasterRecursive(estimator=Ridge(), lags=24)
        self.name = "demo02_model"

    def tune(self):
        df = fetch_data(filename=self.dataset_path)
        y = df["A"].groupby(level=0).mean().asfreq("h").ffill() # Safely handle duplicates and NA gaps
        
        # Enforce hard upper cutoff
        y_train = y.loc[:self.end_dev]
        
        # Enforce lower boundary
        if self.train_size is not None:
            start_date = pd.to_datetime(self.end_dev, utc=True) - self.train_size
            y_train = y_train.loc[start_date:]
            
        print(f"Fitting model locally on {len(y_train)} points until {self.end_dev}")
        self.forecaster.fit(y=y_train)

    def get_params(self): return {}

# 1. Fetch the multi-variate continuous integration dataset "demo02.csv"
demo_file = get_package_data_home() / "demo02.csv"
df_full = fetch_data(filename=str(demo_file))
y_full = df_full["A"].groupby(level=0).mean().asfreq("h").ffill()

# 2. Establish chronological boundaries (e.g., test on the final 7 days)
test_duration = pd.Timedelta(days=7)
cutoff_date = y_full.index.max() - test_duration

# 3. Train isolated pipeline matching precise boundaries
model_vis = train_new_model(
    model_class=VisualizingForecaster,
    n_iteration=1,
    train_size=pd.Timedelta(days=60), # 60-day historical perspective
    end_dev=cutoff_date, 
    data_filename=str(demo_file), 
    save_to_file=False, 
    dataset_path=str(demo_file)
)

# 4. Extract ground truth testing window (exclusive of the cutoff)
y_test = y_full.loc[cutoff_date + pd.Timedelta(hours=1):]

# 5. Execute N-step recursive predictions
preds = model_vis.forecaster.predict(steps=len(y_test))
preds.index = y_test.index  # Align axes

# 6. Measure mathematical accuracy
mae = mean_absolute_error(y_test, preds)
print(f"Validation MAE: {mae:.3f}")

# 7. Generate interactive verification layer (plotly native view)
fig = go.Figure()
fig.add_trace(go.Scatter(x=y_test.index, y=y_test, mode="lines", name="Actual Truth"))
fig.add_trace(go.Scatter(
    x=preds.index, y=preds, mode="lines", 
    name="Forecaster Projection", 
    line=dict(dash="dash", color="orange")
))

fig.update_layout(
    title=f"demo02.csv Prediction Quality Appraisal (MAE: {mae:.3f})",
    xaxis_title="Time (UTC)",
    yaxis_title="Target Sensor: A",
    template="plotly_white",
    hovermode="x unified"
)
# fig.show() # Automatically evaluates inside Quarto output blocks
```

